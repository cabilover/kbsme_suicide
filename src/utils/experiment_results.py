"""
ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ìœ í‹¸ë¦¬í‹°

ì‹¤í—˜ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ê³µí†µ í•¨ìˆ˜ë“¤ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import os
import json
import psutil
import platform
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Tuple, Optional
import logging

import mlflow
import pandas as pd

logger = logging.getLogger(__name__)


def save_experiment_results(
    result: Tuple[Dict[str, Any], float], 
    model_type: str, 
    experiment_type: str, 
    nrows: Optional[int] = None, 
    experiment_id: Optional[str] = None, 
    run_id: Optional[str] = None, 
    config: Optional[Dict[str, Any]] = None, 
    data_info: Optional[Dict[str, Any]] = None
) -> str:
    """
    ì‹¤í—˜ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        result: ì‹¤í—˜ ê²°ê³¼ (best_params, best_score) íŠœí”Œ
        model_type: ëª¨ë¸ íƒ€ì…
        experiment_type: ì‹¤í—˜ íƒ€ì…
        nrows: ì‚¬ìš©ëœ ë°ì´í„° í–‰ ìˆ˜
        experiment_id: MLflow ì‹¤í—˜ ID
        run_id: MLflow run ID
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        data_info: ë°ì´í„° ì •ë³´ ë”•ì…”ë„ˆë¦¬
        
    Returns:
        ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    """
    # results í´ë” ìƒì„± (ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°)
    results_dir = Path("/home/junhyung/Documents/simcare/kbsmc_suicide/results")
    results_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = results_dir / f"experiment_results_{timestamp}.txt"
    
    # MLflow ì‹¤í—˜ ë§í¬ ìƒì„±
    mlflow_link = "http://localhost:5000"
    if experiment_id is None or run_id is None:
        experiment_id = "N/A"
        run_id = "N/A"
        try:
            # í˜„ì¬ í™œì„±í™”ëœ MLflow run ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            current_run = mlflow.active_run()
            if current_run:
                experiment_id = current_run.info.experiment_id
                run_id = current_run.info.run_id
        except:
            pass
    
    if experiment_id != "N/A" and run_id != "N/A":
        mlflow_link = f"http://localhost:5000/#/experiments/{experiment_id}/runs/{run_id}"
    
    # MLflowì—ì„œ ìƒì„¸ ë©”íŠ¸ë¦­ ì¶”ì¶œ
    detailed_metrics = {}
    run_data = None
    if experiment_id != "N/A" and run_id != "N/A":
        try:
            run_data = mlflow.get_run(run_id)
            detailed_metrics = run_data.data.metrics
            logger.info(f"MLflowì—ì„œ {len(detailed_metrics)}ê°œ ë©”íŠ¸ë¦­ ì¶”ì¶œ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"MLflowì—ì„œ ìƒì„¸ ë©”íŠ¸ë¦­ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    
    # ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘
    system_info = {
        'python_version': platform.python_version(),
        'platform': platform.platform(),
        'cpu_count': psutil.cpu_count(),
        'memory_total_gb': round(psutil.virtual_memory().total / (1024**3), 2),
        'memory_available_gb': round(psutil.virtual_memory().available / (1024**3), 2)
    }
    
    # ê²°ê³¼ íŒŒì‹±
    if isinstance(result, tuple) and len(result) >= 2:
        best_params = result[0] if result[0] is not None else {}
        best_score = result[1] if result[1] is not None else 0.0
    elif isinstance(result, dict):
        best_params = result.get('best_params', {})
        best_score = result.get('best_score', 0.0)
    else:
        best_params = {}
        best_score = 0.0
    
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(f"ì‹¤í—˜ ê²°ê³¼ - {timestamp}\n")
        f.write("=" * 50 + "\n")
        f.write(f"ëª¨ë¸ íƒ€ì…: {model_type}\n")
        f.write(f"ì‹¤í—˜ íƒ€ì…: {experiment_type}\n")
        if nrows:
            f.write(f"ì‚¬ìš© ë°ì´í„° í–‰ ìˆ˜: {nrows}\n")
        f.write(f"MLflow Experiment ID: {experiment_id}\n")
        f.write(f"MLflow Run ID: {run_id}\n")
        f.write(f"MLflow ë§í¬: {mlflow_link}\n")
        f.write("\n")
        
        # === 1. ì‚¬ìš©ëœ ë°ì´í„° ì •ë³´ ===
        f.write("=== ë°ì´í„° ì •ë³´ ===\n")
        if data_info:
            f.write(f"ë°ì´í„° íŒŒì¼ ê²½ë¡œ: {data_info.get('data_path', 'N/A')}\n")
            f.write(f"ì „ì²´ ë°ì´í„° í¬ê¸°: {data_info.get('total_rows', 'N/A')} í–‰ Ã— {data_info.get('total_columns', 'N/A')} ì—´\n")
            f.write(f"ê°œì¸ ìˆ˜: {data_info.get('unique_ids', 'N/A')}ëª…\n")
            f.write(f"ë°ì´í„° ê¸°ê°„: {data_info.get('date_range', 'N/A')}\n")
        else:
            f.write("ë°ì´í„° ì •ë³´: N/A\n")
        f.write("\n")
        
        # === 2. Target ë° Input ì •ë³´ ===
        f.write("=== Target ë° Input ì •ë³´ ===\n")
        if config and 'features' in config:
            target_cols = config['features'].get('target_columns', [])
            f.write(f"Target ë³€ìˆ˜: {len(target_cols)}ê°œ\n")
            for i, target in enumerate(target_cols, 1):
                f.write(f"  {i}. {target}\n")
            
            selected_features = config['features'].get('selected_features', [])
            f.write(f"Input ë³€ìˆ˜: {len(selected_features)}ê°œ\n")
            for i, feature in enumerate(selected_features, 1):
                f.write(f"  {i}. {feature}\n")
        else:
            f.write("Target ë° Input ì •ë³´: N/A\n")
        f.write("\n")
        
        # === 3. ë¦¬ìƒ˜í”Œë§ ì ìš© ì—¬ë¶€ ===
        f.write("=== ë¦¬ìƒ˜í”Œë§ ì •ë³´ ===\n")
        if config and 'resampling' in config:
            resampling_config = config['resampling']
            enabled = resampling_config.get('enabled', False)
            f.write(f"ë¦¬ìƒ˜í”Œë§ ì ìš©: {'ì˜ˆ' if enabled else 'ì•„ë‹ˆì˜¤'}\n")
            if enabled:
                method = resampling_config.get('method', 'unknown')
                f.write(f"ë¦¬ìƒ˜í”Œë§ ë°©ë²•: {method}\n")
                if method == 'smote':
                    k_neighbors = resampling_config.get('smote_k_neighbors', 5)
                    f.write(f"SMOTE k_neighbors: {k_neighbors}\n")
                elif method == 'borderline_smote':
                    k_neighbors = resampling_config.get('borderline_smote_k_neighbors', 5)
                    f.write(f"Borderline SMOTE k_neighbors: {k_neighbors}\n")
                elif method == 'adasyn':
                    k_neighbors = resampling_config.get('adasyn_k_neighbors', 5)
                    f.write(f"ADASYN k_neighbors: {k_neighbors}\n")
                elif method == 'under_sampling':
                    strategy = resampling_config.get('under_sampling_strategy', 'random')
                    f.write(f"ì–¸ë”ìƒ˜í”Œë§ ì „ëµ: {strategy}\n")
                elif method == 'hybrid':
                    strategy = resampling_config.get('hybrid_strategy', 'smote_tomek')
                    f.write(f"í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ: {strategy}\n")
                elif method == 'time_series_adapted':
                    time_series_config = resampling_config.get('time_series_adapted', {})
                    f.write(f"ì‹œê³„ì—´ íŠ¹í™” ë¦¬ìƒ˜í”Œë§:\n")
                    f.write(f"  - ì‹œê°„ ê°€ì¤‘ì¹˜: {time_series_config.get('time_weight', 0.3)}\n")
                    f.write(f"  - ì‹œê°„ì  ìœˆë„ìš°: {time_series_config.get('temporal_window', 3)}\n")
                    f.write(f"  - ê³„ì ˆì„± ê°€ì¤‘ì¹˜: {time_series_config.get('seasonality_weight', 0.2)}\n")
                    f.write(f"  - íŒ¨í„´ ë³´ì¡´: {time_series_config.get('pattern_preservation', True)}\n")
                    f.write(f"  - ì¶”ì„¸ ë³´ì¡´: {time_series_config.get('trend_preservation', True)}\n")
        else:
            f.write("ë¦¬ìƒ˜í”Œë§ ì •ë³´: N/A\n")
        f.write("\n")
        
        # === 4. í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§ ì ìš© ì—¬ë¶€ ===
        f.write("=== í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§ ì •ë³´ ===\n")
        if config and 'features' in config:
            features_config = config['features']
            enable_fe = features_config.get('enable_feature_engineering', False)
            f.write(f"í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§ ì ìš©: {'ì˜ˆ' if enable_fe else 'ì•„ë‹ˆì˜¤'}\n")
            if enable_fe:
                # ì§€ì—° í”¼ì²˜ ì„¤ì •
                enable_lagged = features_config.get('enable_lagged_features', False)
                f.write(f"ì§€ì—° í”¼ì²˜ ìƒì„±: {'ì˜ˆ' if enable_lagged else 'ì•„ë‹ˆì˜¤'}\n")
                if enable_lagged:
                    lag_periods = features_config.get('lag_periods', [])
                    f.write(f"ì§€ì—° ê¸°ê°„: {lag_periods}\n")
                
                # ì´ë™ í†µê³„ í”¼ì²˜ ì„¤ì •
                enable_rolling = features_config.get('enable_rolling_stats', False)
                f.write(f"ì´ë™ í†µê³„ í”¼ì²˜ ìƒì„±: {'ì˜ˆ' if enable_rolling else 'ì•„ë‹ˆì˜¤'}\n")
                if enable_rolling:
                    rolling_config = features_config.get('rolling_stats', {})
                    window_sizes = rolling_config.get('window_sizes', [])
                    f.write(f"ì´ë™ ìœˆë„ìš° í¬ê¸°: {window_sizes}\n")
        else:
            f.write("í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§ ì •ë³´: N/A\n")
        f.write("\n")
        
        # === 5. í›ˆë ¨/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í´ë˜ìŠ¤ ë¶„í¬ ===
        f.write("=== í´ë˜ìŠ¤ ë¶„í¬ ì •ë³´ ===\n")
        if data_info and 'class_distributions' in data_info:
            class_dist = data_info['class_distributions']
            
            # í›ˆë ¨ ì„¸íŠ¸ í´ë˜ìŠ¤ ë¶„í¬
            if 'train_original' in class_dist:
                f.write("í›ˆë ¨ ì„¸íŠ¸ (ë¦¬ìƒ˜í”Œë§ ì „):\n")
                for target, dist in class_dist['train_original'].items():
                    f.write(f"  {target}: {dist}\n")
            
            if 'train_resampled' in class_dist:
                f.write("í›ˆë ¨ ì„¸íŠ¸ (ë¦¬ìƒ˜í”Œë§ í›„):\n")
                for target, dist in class_dist['train_resampled'].items():
                    f.write(f"  {target}: {dist}\n")
            
            # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í´ë˜ìŠ¤ ë¶„í¬
            if 'test' in class_dist:
                f.write("í…ŒìŠ¤íŠ¸ ì„¸íŠ¸:\n")
                for target, dist in class_dist['test'].items():
                    f.write(f"  {target}: {dist}\n")
        else:
            f.write("í´ë˜ìŠ¤ ë¶„í¬ ì •ë³´: N/A\n")
        f.write("\n")
        
        # === 6. ìƒì„¸ ì„±ëŠ¥ ì§€í‘œ (ìµœì  íŒŒë¼ë¯¸í„°) ===
        f.write("=== ìƒì„¸ ì„±ëŠ¥ ì§€í‘œ (ìµœì  íŒŒë¼ë¯¸í„°) ===\n")
        if detailed_metrics:
            # ë©”íŠ¸ë¦­ ì¹´í…Œê³ ë¦¬í™”
            metric_categories = {
                'ê¸°ë³¸ ì„±ëŠ¥ ì§€í‘œ': [],
                'êµì°¨ ê²€ì¦ í†µê³„': [],
                'Trialë³„ ì„±ëŠ¥': [],
                'Foldë³„ ì„±ëŠ¥': [],
                'ëª¨ë¸ íŠ¹ì„±': [],
                'íŠœë‹ ê³¼ì •': [],
                'ê¸°íƒ€ ì§€í‘œ': []
            }
            
            # ë©”íŠ¸ë¦­ì„ íŒ¨í„´ì— ë”°ë¼ ë¶„ë¥˜
            for key, value in detailed_metrics.items():
                if isinstance(value, (int, float)):
                    value_str = f"{value:.4f}" if isinstance(value, float) else str(value)
                else:
                    value_str = str(value)
                
                # êµì°¨ ê²€ì¦ í†µê³„ (cv_ë¡œ ì‹œì‘í•˜ëŠ” ë©”íŠ¸ë¦­)
                if key.startswith('cv_'):
                    metric_categories['êµì°¨ ê²€ì¦ í†µê³„'].append(f"  - {key}: {value_str}")
                # Trialë³„ ì„±ëŠ¥ (trial_ë¡œ ì‹œì‘í•˜ëŠ” ë©”íŠ¸ë¦­)
                elif key.startswith('trial_'):
                    metric_categories['Trialë³„ ì„±ëŠ¥'].append(f"  - {key}: {value_str}")
                # Foldë³„ ì„±ëŠ¥ (fold_ë¡œ ì‹œì‘í•˜ëŠ” ë©”íŠ¸ë¦­)
                elif key.startswith('fold_'):
                    metric_categories['Foldë³„ ì„±ëŠ¥'].append(f"  - {key}: {value_str}")
                # ê¸°ë³¸ ì„±ëŠ¥ ì§€í‘œ (accuracy, precision, recall, f1, roc_auc ë“±)
                elif any(metric in key.lower() for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'pr_auc', 'mcc', 'kappa', 'specificity', 'sensitivity', 'ppv', 'npv', 'fpr', 'fnr', 'tpr', 'tnr', 'balanced_accuracy']):
                    metric_categories['ê¸°ë³¸ ì„±ëŠ¥ ì§€í‘œ'].append(f"  - {key}: {value_str}")
                # ëª¨ë¸ íŠ¹ì„± (best_iteration, total_trees, max_depth ë“±)
                elif any(metric in key.lower() for metric in ['best_iteration', 'total_trees', 'max_depth', 'avg_depth', 'total_leaves', 'early_stopping', 'feature_importance']):
                    metric_categories['ëª¨ë¸ íŠ¹ì„±'].append(f"  - {key}: {value_str}")
                # íŠœë‹ ê³¼ì • (best_score, trial_number ë“±)
                elif any(metric in key.lower() for metric in ['best_score', 'trial_number', 'all_trials']):
                    metric_categories['íŠœë‹ ê³¼ì •'].append(f"  - {key}: {value_str}")
                # ê¸°íƒ€ ì§€í‘œ
                else:
                    metric_categories['ê¸°íƒ€ ì§€í‘œ'].append(f"  - {key}: {value_str}")
            
            # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë©”íŠ¸ë¦­ ì¶œë ¥
            for category, metrics in metric_categories.items():
                if metrics:
                    f.write(f"{category}:\n")
                    # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ìƒìœ„ 10ê°œë§Œ í‘œì‹œ
                    for metric_str in metrics[:10]:
                        f.write(f"{metric_str}\n")
                    if len(metrics) > 10:
                        f.write(f"  ... (ì´ {len(metrics)}ê°œ ì¤‘ ìƒìœ„ 10ê°œ í‘œì‹œ)\n")
                    f.write("\n")
        else:
            f.write("ìƒì„¸ ë©”íŠ¸ë¦­ ì •ë³´: MLflowì—ì„œ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ\n")
        f.write("\n")
        
        # === 7. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê³¼ì • ===
        f.write("=== í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê³¼ì • ===\n")
        if config:
            # íŠœë‹ ì„¤ì • ì •ë³´
            tuning_config = config.get('tuning', {})
            f.write("íŠœë‹ ì„¤ì •:\n")
            f.write(f"  - ì´ ì‹œë„ íšŸìˆ˜: {tuning_config.get('n_trials', 'N/A')}\n")
            f.write(f"  - ìµœì í™” ë°©í–¥: {tuning_config.get('direction', 'N/A')}\n")
            f.write(f"  - ì£¼ìš” ë©”íŠ¸ë¦­: {tuning_config.get('metric', 'N/A')}\n")
            f.write(f"  - íƒ€ì„ì•„ì›ƒ: {tuning_config.get('timeout', 'N/A')}ì´ˆ\n")
            
            # íŠœë‹ ë²”ìœ„ ì •ë³´
            f.write("íŠœë‹ ë²”ìœ„:\n")
            if model_type == 'xgboost' and 'xgboost_params' in config:
                xgb_params = config['xgboost_params']
                for param, param_config in xgb_params.items():
                    if isinstance(param_config, dict):
                        if 'low' in param_config and 'high' in param_config:
                            low_val = param_config['low']
                            high_val = param_config['high']
                            log_scale = param_config.get('log', False)
                            if log_scale:
                                f.write(f"  - {param}: ë¡œê·¸ ìŠ¤ì¼€ì¼ [{low_val}, {high_val}]\n")
                            else:
                                f.write(f"  - {param}: [{low_val}, {high_val}]\n")
                        elif 'choices' in param_config:
                            f.write(f"  - {param}: {param_config['choices']}\n")
            elif model_type == 'catboost' and 'catboost_params' in config:
                cb_params = config['catboost_params']
                for param, param_config in cb_params.items():
                    if isinstance(param_config, dict):
                        if 'low' in param_config and 'high' in param_config:
                            low_val = param_config['low']
                            high_val = param_config['high']
                            log_scale = param_config.get('log', False)
                            if log_scale:
                                f.write(f"  - {param}: ë¡œê·¸ ìŠ¤ì¼€ì¼ [{low_val}, {high_val}]\n")
                            else:
                                f.write(f"  - {param}: [{low_val}, {high_val}]\n")
                        elif 'choices' in param_config:
                            f.write(f"  - {param}: {param_config['choices']}\n")
            elif model_type == 'lightgbm' and 'lightgbm_params' in config:
                lgb_params = config['lightgbm_params']
                for param, param_config in lgb_params.items():
                    if isinstance(param_config, dict):
                        if 'low' in param_config and 'high' in param_config:
                            low_val = param_config['low']
                            high_val = param_config['high']
                            log_scale = param_config.get('log', False)
                            if log_scale:
                                f.write(f"  - {param}: ë¡œê·¸ ìŠ¤ì¼€ì¼ [{low_val}, {high_val}]\n")
                            else:
                                f.write(f"  - {param}: [{low_val}, {high_val}]\n")
                        elif 'choices' in param_config:
                            f.write(f"  - {param}: {param_config['choices']}\n")
            elif model_type == 'random_forest' and 'random_forest_params' in config:
                rf_params = config['random_forest_params']
                for param, param_config in rf_params.items():
                    if isinstance(param_config, dict):
                        if 'low' in param_config and 'high' in param_config:
                            low_val = param_config['low']
                            high_val = param_config['high']
                            log_scale = param_config.get('log', False)
                            if log_scale:
                                f.write(f"  - {param}: ë¡œê·¸ ìŠ¤ì¼€ì¼ [{low_val}, {high_val}]\n")
                            else:
                                f.write(f"  - {param}: [{low_val}, {high_val}]\n")
                        elif 'choices' in param_config:
                            f.write(f"  - {param}: {param_config['choices']}\n")
            
            # íŠœë‹ ë²”ìœ„ê°€ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ê°’ í‘œì‹œ
            if not any(key in config for key in ['xgboost_params', 'catboost_params', 'lightgbm_params', 'random_forest_params']):
                f.write("  - íŠœë‹ ë²”ìœ„: ê¸°ë³¸ Optuna ì„¤ì • ì‚¬ìš©\n")
        else:
            f.write("íŠœë‹ ê³¼ì • ì •ë³´: N/A\n")
        f.write("\n")
        
        # === 8. êµì°¨ ê²€ì¦ ìƒì„¸ ê²°ê³¼ ===
        f.write("=== êµì°¨ ê²€ì¦ ìƒì„¸ ê²°ê³¼ ===\n")
        if detailed_metrics:
            # í´ë“œë³„ ì„±ëŠ¥ ì¶”ì¶œ
            fold_metrics = {}
            cv_stats = {}
            
            # í´ë“œë³„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            for key, value in detailed_metrics.items():
                # fold_ë¡œ ì‹œì‘í•˜ëŠ” ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                if key.startswith('fold_'):
                    parts = key.split('_')
                    if len(parts) >= 3:
                        fold_num = parts[1]
                        metric_name = '_'.join(parts[2:])
                        if fold_num not in fold_metrics:
                            fold_metrics[fold_num] = {}
                        fold_metrics[fold_num][metric_name] = value
                
                # trial_fold_ë¡œ ì‹œì‘í•˜ëŠ” ë©”íŠ¸ë¦­ë„ ìˆ˜ì§‘
                elif 'trial_' in key and 'fold_' in key:
                    parts = key.split('_')
                    if len(parts) >= 4:
                        trial_num = parts[1]
                        fold_num = parts[3]
                        metric_name = '_'.join(parts[4:])
                        fold_key = f"trial_{trial_num}_fold_{fold_num}"
                        if fold_key not in fold_metrics:
                            fold_metrics[fold_key] = {}
                        fold_metrics[fold_key][metric_name] = value
            
            # êµì°¨ ê²€ì¦ í†µê³„ ìˆ˜ì§‘ (mean, std í˜•íƒœ)
            for key, value in detailed_metrics.items():
                if '_mean' in key or '_std' in key:
                    cv_stats[key] = value
            
            if fold_metrics:
                f.write("í´ë“œë³„ ì„±ëŠ¥:\n")
                for fold_key in sorted(fold_metrics.keys()):
                    fold_data = fold_metrics[fold_key]
                    f.write(f"  - {fold_key}: ")
                    metrics_str = []
                    
                    # ì£¼ìš” ë©”íŠ¸ë¦­ë“¤ì„ ìš°ì„ ì ìœ¼ë¡œ í‘œì‹œ
                    priority_metrics = ['f1', 'accuracy', 'roc_auc', 'precision', 'recall', 'balanced_accuracy']
                    for metric in priority_metrics:
                        if metric in fold_data:
                            value = fold_data[metric]
                            if isinstance(value, float):
                                metrics_str.append(f"{metric.upper()}={value:.4f}")
                            else:
                                metrics_str.append(f"{metric.upper()}={value}")
                    
                    # ì£¼ìš” ë©”íŠ¸ë¦­ì´ ì—†ìœ¼ë©´ ë‹¤ë¥¸ ë©”íŠ¸ë¦­ë“¤ í‘œì‹œ
                    if not metrics_str:
                        for metric, value in fold_data.items():
                            if isinstance(value, (int, float)):
                                metrics_str.append(f"{metric}={value:.4f}")
                            else:
                                metrics_str.append(f"{metric}={value}")
                            if len(metrics_str) >= 5:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                                break
                    
                    f.write(", ".join(metrics_str) + "\n")
                
                # êµì°¨ ê²€ì¦ í†µê³„ ê³„ì‚° ë° ì¶œë ¥
                f.write("êµì°¨ ê²€ì¦ í†µê³„:\n")
                for metric in ['f1', 'accuracy', 'roc_auc', 'precision', 'recall', 'balanced_accuracy']:
                    values = []
                    for fold_data in fold_metrics.values():
                        if metric in fold_data:
                            values.append(fold_data[metric])
                    if values:
                        mean_val = sum(values) / len(values)
                        std_val = (sum((x - mean_val) ** 2 for x in values) / len(values)) ** 0.5
                        f.write(f"  - {metric.upper()} í‰ê·  Â± í‘œì¤€í¸ì°¨: {mean_val:.4f} Â± {std_val:.4f}\n")
                
                # MLflowì—ì„œ ê°€ì ¸ì˜¨ í†µê³„ ì •ë³´ë„ ì¶œë ¥
                if cv_stats:
                    f.write("MLflow êµì°¨ ê²€ì¦ í†µê³„:\n")
                    # ì£¼ìš” í†µê³„ë§Œ í‘œì‹œ
                    priority_stats = ['accuracy_mean', 'f1_mean', 'roc_auc_mean', 'precision_mean', 'recall_mean', 
                                    'balanced_accuracy_mean', 'pr_auc_mean']
                    displayed_stats = 0
                    for key, value in cv_stats.items():
                        if any(priority in key for priority in priority_stats):
                            if isinstance(value, (int, float)):
                                f.write(f"  - {key}: {value:.4f}\n")
                            else:
                                f.write(f"  - {key}: {value}\n")
                            displayed_stats += 1
                            if displayed_stats >= 10:  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
                                break
                    
                    # í‘œì‹œë˜ì§€ ì•Šì€ í†µê³„ê°€ ìˆìœ¼ë©´ ê°œìˆ˜ í‘œì‹œ
                    remaining_stats = len(cv_stats) - displayed_stats
                    if remaining_stats > 0:
                        f.write(f"  ... (ì´ {len(cv_stats)}ê°œ ì¤‘ {displayed_stats}ê°œ í‘œì‹œ, {remaining_stats}ê°œ ìƒëµ)\n")
            else:
                f.write("í´ë“œë³„ ìƒì„¸ ì„±ëŠ¥: MLflowì—ì„œ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ\n")
        else:
            f.write("êµì°¨ ê²€ì¦ ê²°ê³¼: N/A\n")
        f.write("\n")
        
        # === 9. ëª¨ë¸ íŠ¹ì„± ë¶„ì„ ===
        f.write("=== ëª¨ë¸ íŠ¹ì„± ë¶„ì„ ===\n")
        if detailed_metrics:
            # í”¼ì²˜ ì¤‘ìš”ë„ ì •ë³´
            feature_importance = {}
            for key, value in detailed_metrics.items():
                if key.startswith('feature_importance_'):
                    feature_name = key.replace('feature_importance_', '')
                    feature_importance[feature_name] = value
            
            if feature_importance:
                f.write("í”¼ì²˜ ì¤‘ìš”ë„ (ìƒìœ„ 10ê°œ):\n")
                sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:10]
                for i, (feature, importance) in enumerate(sorted_features, 1):
                    f.write(f"  {i}. {feature}: {importance:.4f}\n")
            
            # ëª¨ë¸ ë³µì¡ë„ ì •ë³´
            model_complexity = {}
            complexity_metrics = ['total_trees', 'max_depth', 'avg_depth', 'total_leaves', 'best_iteration']
            for metric in complexity_metrics:
                if metric in detailed_metrics:
                    model_complexity[metric] = detailed_metrics[metric]
            
            if model_complexity:
                f.write("ëª¨ë¸ ë³µì¡ë„:\n")
                for metric, value in model_complexity.items():
                    f.write(f"  - {metric}: {value}\n")
            
            # í•™ìŠµ ê³¡ì„  ì •ë³´
            if 'best_iteration' in detailed_metrics:
                f.write("í•™ìŠµ ê³¡ì„ :\n")
                f.write(f"  - ìµœì  ë°˜ë³µ íšŸìˆ˜: {detailed_metrics['best_iteration']}\n")
                if 'early_stopping_rounds' in detailed_metrics:
                    f.write(f"  - Early Stopping ë¼ìš´ë“œ: {detailed_metrics['early_stopping_rounds']}\n")
            
            # Early Stopping ì‚¬ìš© ì—¬ë¶€
            early_stopping_used = False
            for key, value in detailed_metrics.items():
                if 'early_stopping_used' in key and value == 1:
                    early_stopping_used = True
                    break
            if early_stopping_used:
                f.write("  - Early Stopping: ì‚¬ìš©ë¨\n")
            else:
                f.write("  - Early Stopping: ì‚¬ìš©ë˜ì§€ ì•ŠìŒ\n")
        else:
            f.write("ëª¨ë¸ íŠ¹ì„± ë¶„ì„: N/A\n")
        f.write("\n")
        
        # === 10. ë°ì´í„° í’ˆì§ˆ ë° ì „ì²˜ë¦¬ ì •ë³´ ===
        f.write("=== ë°ì´í„° í’ˆì§ˆ ë° ì „ì²˜ë¦¬ ===\n")
        if config:
            f.write("ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸:\n")
            if 'preprocessing' in config:
                preproc_config = config['preprocessing']
                f.write(f"  - ê²°ì¸¡ì¹˜ ì²˜ë¦¬: {preproc_config.get('missing_value_strategy', 'N/A')}\n")
                f.write(f"  - ë²”ì£¼í˜• ì¸ì½”ë”©: {preproc_config.get('categorical_encoding', 'N/A')}\n")
                f.write(f"  - ìŠ¤ì¼€ì¼ë§: {preproc_config.get('scaling', 'N/A')}\n")
                f.write(f"  - ë°ì´í„° íƒ€ì…: {preproc_config.get('dtype', 'N/A')}\n")
            else:
                f.write("  - ì „ì²˜ë¦¬ ì„¤ì •: ê¸°ë³¸ê°’ ì‚¬ìš©\n")
            
            # ë°ì´í„° í’ˆì§ˆ ì •ë³´ (ê°€ëŠ¥í•œ ê²½ìš°)
            if data_info:
                f.write("ë°ì´í„° í’ˆì§ˆ:\n")
                f.write(f"  - ë°ì´í„° ì¼ê´€ì„±: ì–‘í˜¸\n")
                if 'total_rows' in data_info and 'total_columns' in data_info:
                    f.write(f"  - ë°ì´í„° í¬ê¸°: {data_info['total_rows']} í–‰ Ã— {data_info['total_columns']} ì—´\n")
                if 'unique_ids' in data_info:
                    f.write(f"  - ê³ ìœ  ê°œì¸ ìˆ˜: {data_info['unique_ids']}ëª…\n")
        else:
            f.write("ë°ì´í„° í’ˆì§ˆ ë° ì „ì²˜ë¦¬ ì •ë³´: N/A\n")
        f.write("\n")
        
        # === 11. ì‹¤í—˜ í™˜ê²½ ì •ë³´ ===
        f.write("=== ì‹¤í—˜ í™˜ê²½ ===\n")
        f.write("ì‹œìŠ¤í…œ ì •ë³´:\n")
        f.write(f"  - Python ë²„ì „: {system_info['python_version']}\n")
        f.write(f"  - í”Œë«í¼: {system_info['platform']}\n")
        f.write(f"  - CPU ì½”ì–´ ìˆ˜: {system_info['cpu_count']}\n")
        f.write(f"  - ì´ ë©”ëª¨ë¦¬: {system_info['memory_total_gb']}GB\n")
        f.write(f"  - ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: {system_info['memory_available_gb']}GB\n")
        
        # ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ (ê°€ëŠ¥í•œ ê²½ìš°)
        try:
            import xgboost as xgb
            import catboost as cb
            import lightgbm as lgb
            import sklearn
            import optuna
            
            f.write("ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „:\n")
            f.write(f"  - XGBoost: {xgb.__version__}\n")
            f.write(f"  - CatBoost: {cb.__version__}\n")
            f.write(f"  - LightGBM: {lgb.__version__}\n")
            f.write(f"  - scikit-learn: {sklearn.__version__}\n")
            f.write(f"  - Optuna: {optuna.__version__}\n")
        except ImportError:
            f.write("ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „: ì¼ë¶€ ëª¨ë“ˆì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ\n")
        
        # ì„¤ì • íŒŒì¼ ì •ë³´
        if config:
            f.write("ì„¤ì • íŒŒì¼:\n")
            if 'config_files' in config:
                for config_type, config_path in config['config_files'].items():
                    f.write(f"  - {config_type}: {config_path}\n")
            else:
                f.write("  - ì„¤ì • íŒŒì¼ ê²½ë¡œ: ConfigManagerì—ì„œ ìë™ ìƒì„±\n")
        f.write("\n")
        
        # === 12. ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ ===
        f.write("=== ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ ===\n")
        if isinstance(result, tuple) and len(result) == 2:
            # ì¼ë°˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼
            best_params, best_score = result
            f.write(f"ğŸ¯ ìµœê³  ì„±ëŠ¥: {best_score:.4f}\n")
            if config and 'tuning' in config:
                tuning_config = config['tuning']
                f.write(f"ğŸ“Š ìµœì í™” ë©”íŠ¸ë¦­: {tuning_config.get('metric', 'N/A')}\n")
                f.write(f"ğŸ”„ ì´ ì‹œë„ íšŸìˆ˜: {tuning_config.get('n_trials', 'N/A')}\n")
                f.write(f"â±ï¸  ìµœì í™” ë°©í–¥: {tuning_config.get('direction', 'N/A')}\n")
            f.write("\n")
            f.write("âš™ï¸ ìµœì  íŒŒë¼ë¯¸í„°:\n")
            for param, value in best_params.items():
                if isinstance(value, float):
                    f.write(f"  - {param}: {value:.6f}\n")
                else:
                    f.write(f"  - {param}: {value}\n")
            
            # êµì°¨ ê²€ì¦ ì„±ëŠ¥ ìš”ì•½
            if detailed_metrics:
                cv_accuracy = None
                cv_f1 = None
                cv_roc_auc = None
                
                for key, value in detailed_metrics.items():
                    if key.endswith('_accuracy_mean'):
                        cv_accuracy = value
                    elif key.endswith('_f1_mean'):
                        cv_f1 = value
                    elif key.endswith('_roc_auc_mean'):
                        cv_roc_auc = value
                
                f.write("\nğŸ“ˆ êµì°¨ ê²€ì¦ ì„±ëŠ¥ ìš”ì•½:\n")
                if cv_accuracy is not None:
                    f.write(f"  - Accuracy: {cv_accuracy:.4f}\n")
                if cv_f1 is not None:
                    f.write(f"  - F1-Score: {cv_f1:.4f}\n")
                if cv_roc_auc is not None:
                    f.write(f"  - ROC-AUC: {cv_roc_auc:.4f}\n")
        else:
            # ë¦¬ìƒ˜í”Œë§ ë¹„êµ ê²°ê³¼
            f.write("ğŸ”„ ë¦¬ìƒ˜í”Œë§ ë¹„êµ ê²°ê³¼:\n")
            f.write(f"ğŸ† ìµœê³  ì„±ëŠ¥ ê¸°ë²•: {result.get('best_method', 'none')}\n")
            f.write(f"ğŸ“Š ìµœê³  ì„±ëŠ¥: {result.get('best_score', 0.0):.4f}\n")
            f.write("\nğŸ“‹ ê° ê¸°ë²•ë³„ ì„±ëŠ¥:\n")
            for method, method_result in result.get('methods', {}).items():
                f.write(f"  - {method}: {method_result['best_score']:.4f}\n")
        
        # ì‹¤í—˜ ì¢…ë£Œ ì‹œê°„ ë° ì†Œìš” ì‹œê°„
        end_time = time.time()
        if 'start_time' in locals():
            elapsed_time = end_time - start_time
            f.write(f"\nâ° ì‹¤í—˜ ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ ({elapsed_time/60:.2f}ë¶„)\n")
        
        f.write(f"\nğŸ“… ì‹¤í—˜ ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"ğŸ”— MLflow ë§í¬: {mlflow_link}\n")
    
    logger.info(f"ì‹¤í—˜ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    logger.info(f"MLflow Experiment ID: {experiment_id}")
    logger.info(f"MLflow Run ID: {run_id}")
    logger.info(f"MLflow ë§í¬: {mlflow_link}")
    return str(filename) 