# KBSMC μμ‚΄ μμΈ΅ ν”„λ΅μ νΈ

## ν”„λ΅μ νΈ κ°μ”
κ°μΈλ³„ μ—°κ°„ μ •μ‹  κ±΄κ°• μ§€ν‘ λ°μ΄ν„°λ¥Ό ν™μ©ν•μ—¬ λ‹¤μ ν•΄μ λ¶μ•/μ°μΈ/μλ©΄ μ μ λ° μμ‚΄ μ‚¬κ³ /μ‹λ„ μ—¬λ¶€λ¥Ό μμΈ΅ν•λ” λ¨Έμ‹ λ¬λ‹ ν”„λ΅μ νΈμ…λ‹λ‹¤.









## ν”„λ΅μ νΈ κµ¬μ΅°
```
kbsmc_suicide/
β”β”€β”€ data/
β”‚   β”β”€β”€ sourcedata/                    # μ›λ³Έ λ°μ΄ν„°
β”‚   β”‚   β””β”€β”€ data.csv
β”‚   β”β”€β”€ sourcedata_analysis/           # λ¶„μ„ κ²°κ³Ό
β”‚   β”‚   β”β”€β”€ figures/                   # λ¶„μ„ κ·Έλν”„
β”‚   β”‚   β””β”€β”€ reports/                   # λ¶„μ„ λ¦¬ν¬νΈ (.txt)
β”‚   β”β”€β”€ processed/                     # μ „μ²λ¦¬λ λ°μ΄ν„°
β”‚   β”‚   β””β”€β”€ processed_data_with_features.csv
β”‚   β”β”€β”€ tuning_results/                # ν•μ΄νΌνλΌλ―Έν„° νλ‹ κ²°κ³Ό
β”‚   β”β”€β”€ resampling_results/            # λ¦¬μƒν”λ§ μ‹¤ν— κ²°κ³Ό
β”‚   β””β”€β”€ resampling_tuning_results/     # λ¦¬μƒν”λ§ ν•μ΄νΌνλΌλ―Έν„° νλ‹ κ²°κ³Ό
β”β”€β”€ src/
β”‚   β”β”€β”€ data_analysis.py              # λ°μ΄ν„° λ¶„μ„ λ° μ „μ²λ¦¬ μ¤ν¬λ¦½νΈ
β”‚   β”β”€β”€ splits.py                     # λ°μ΄ν„° λ¶„ν•  μ „λµ
β”‚   β”β”€β”€ preprocessing.py              # μ „μ²λ¦¬ νμ΄ν”„λΌμΈ
β”‚   β”β”€β”€ feature_engineering.py        # ν”Όμ² μ—”μ§€λ‹μ–΄λ§
β”‚   β”β”€β”€ models/
β”‚   β”‚   β”β”€β”€ __init__.py               # λ¨λΈ ν¨ν‚¤μ§€ μ΄κΈ°ν™” (ModelFactory ν¬ν•¨)
β”‚   β”‚   β”β”€β”€ base_model.py             # BaseModel μ¶”μƒ ν΄λμ¤
β”‚   β”‚   β”β”€β”€ model_factory.py          # λ¨λΈ ν©ν† λ¦¬ ν΄λμ¤
β”‚   β”‚   β”β”€β”€ xgboost_model.py          # XGBoost λ¨λΈ ν΄λμ¤
β”‚   β”‚   β”β”€β”€ catboost_model.py         # CatBoost λ¨λΈ ν΄λμ¤
β”‚   β”‚   β”β”€β”€ lightgbm_model.py         # LightGBM λ¨λΈ ν΄λμ¤
β”‚   β”‚   β””β”€β”€ random_forest_model.py    # Random Forest λ¨λΈ ν΄λμ¤
β”‚   β”β”€β”€ utils/
β”‚   β”‚   β”β”€β”€ __init__.py               # μ ν‹Έλ¦¬ν‹° ν¨ν‚¤μ§€ μ΄κΈ°ν™”
β”‚   β”‚   β”β”€β”€ config_manager.py         # ConfigManager ν΄λμ¤
β”‚   β”‚   β”β”€β”€ mlflow_manager.py         # MLflow μ‹¤ν— κ΄€λ¦¬ ν΄λμ¤
β”‚   β”‚   β””β”€β”€ mlflow_logging.py         # MLflow λ΅κΉ… ν†µν•© λ¨λ“ (β… μµμ‹ )
β”‚   β”β”€β”€ training.py                   # ν›λ ¨ νμ΄ν”„λΌμΈ
β”‚   β”β”€β”€ evaluation.py                 # ν‰κ°€ λ¨λ“
β”‚   β”β”€β”€ hyperparameter_tuning.py      # ν•μ΄νΌνλΌλ―Έν„° νλ‹
β”‚   β”β”€β”€ utils.py                      # κ³µν†µ μ ν‹Έλ¦¬ν‹° ν•¨μ
β”‚   β””β”€β”€ reference/                    # μ°Έκ³  μλ£
β”β”€β”€ configs/
β”‚   β”β”€β”€ base/                         # κΈ°λ³Έ μ„¤μ •
β”‚   β”‚   β”β”€β”€ common.yaml               # κ³µν†µ μ„¤μ •
β”‚   β”‚   β”β”€β”€ evaluation.yaml           # ν‰κ°€ μ„¤μ •
β”‚   β”‚   β”β”€β”€ mlflow.yaml               # MLflow μ„¤μ •
β”‚   β”‚   β””β”€β”€ validation.yaml           # κ²€μ¦ μ„¤μ •
β”‚   β”β”€β”€ models/                       # λ¨λΈλ³„ μ„¤μ •
β”‚   β”‚   β”β”€β”€ xgboost.yaml              # XGBoost λ¨λΈ μ„¤μ •
β”‚   β”‚   β”β”€β”€ catboost.yaml             # CatBoost λ¨λΈ μ„¤μ •
β”‚   β”‚   β”β”€β”€ lightgbm.yaml             # LightGBM λ¨λΈ μ„¤μ •
β”‚   β”‚   β””β”€β”€ random_forest.yaml        # Random Forest λ¨λΈ μ„¤μ •
β”‚   β”β”€β”€ experiments/                  # μ‹¤ν—λ³„ μ„¤μ •
β”‚   β”‚   β”β”€β”€ focal_loss.yaml           # Focal Loss μ‹¤ν— μ„¤μ •
β”‚   β”‚   β”β”€β”€ resampling.yaml           # λ¦¬μƒν”λ§ μ‹¤ν— μ„¤μ •
β”‚   β”‚   β”β”€β”€ resampling_experiment.yaml # λ¦¬μƒν”λ§ λΉ„κµ μ‹¤ν— μ„¤μ •
β”‚   β”‚   β””β”€β”€ hyperparameter_tuning.yaml # ν•μ΄νΌνλΌλ―Έν„° νλ‹ μ„¤μ •
β”‚   β””β”€β”€ templates/                    # μ„¤μ • ν…ν”λ¦Ώ
β”‚       β”β”€β”€ default.yaml              # κΈ°λ³Έ ν…ν”λ¦Ώ
β”‚       β””β”€β”€ tuning.yaml               # νλ‹ ν…ν”λ¦Ώ
β”β”€β”€ scripts/
β”‚   β”β”€β”€ run_hyperparameter_tuning.py  # ν†µν•© μ‹¤ν— μ‹¤ν–‰ μ¤ν¬λ¦½νΈ
β”‚   β”β”€β”€ run_resampling_experiment.py  # λ¦¬μƒν”λ§ μ‹¤ν— μ‹¤ν–‰ μ¤ν¬λ¦½νΈ
β”‚   β”β”€β”€ cleanup_mlflow_experiments.py # MLflow μ‹¤ν— μ •λ¦¬ μ¤ν¬λ¦½νΈ
β”‚   β”β”€β”€ check_cpu_usage.py            # CPU μ‚¬μ©λ‰ μ²΄ν¬ μ¤ν¬λ¦½νΈ
β”‚   β”β”€β”€ template_experiment.sh        # μ‹¤ν— μ¤ν¬λ¦½νΈ ν…ν”λ¦Ώ
β”‚   β”β”€β”€ run_individual_models.sh      # κ°λ³„ λ¨λΈ μ‹¤ν–‰ μ¤ν¬λ¦½νΈ
β”‚   β”β”€β”€ quick_test.sh                 # λΉ λ¥Έ ν…μ¤νΈ μ¤ν¬λ¦½νΈ
β”‚   β””β”€β”€ test_input_resampling/        # π€ λ€κ·λ¨ μ‹¤ν— μ‹μ¤ν…
β”‚       β”β”€β”€ master_experiment_runner.sh    # λ§μ¤ν„° μ‹¤ν— λ¬λ„
β”‚       β”β”€β”€ phase1_baseline.sh             # Phase 1: κΈ°μ¤€μ„  μ„¤μ •
β”‚       β”β”€β”€ phase2_input_range.sh          # Phase 2: Input λ²”μ„ μ΅°μ •
β”‚       β”β”€β”€ phase3_resampling.sh           # Phase 3: λ¦¬μƒν”λ§ λΉ„κµ
β”‚       β”β”€β”€ phase4_deep_analysis.sh        # Phase 4: λ¨λΈ μ‹¬μΈµ λ¶„μ„
β”‚       β”β”€β”€ phase5_final_optimization.sh   # Phase 5: ν†µν•© μµμ ν™”
β”‚       β””β”€β”€ experiment_setup_guide.md      # μ‹¤ν— μ„¤μ • κ°€μ΄λ“
β”β”€β”€ results/                          # μ‹¤ν— κ²°κ³Ό μ €μ¥μ†
β”‚   β”β”€β”€ experiment_results_*.txt      # μƒμ„Έν• μ‹¤ν— κ²°κ³Ό νμΌ
β”‚   β”β”€β”€ tuning_log_*.txt              # νλ‹ κ³Όμ • λ΅κ·Έ
β”‚   β””β”€β”€ test_logs/                    # μλ™ν™” ν…μ¤νΈ λ΅κ·Έ
β”β”€β”€ models/                           # ν•™μµλ λ¨λΈ μ €μ¥μ†
β”β”€β”€ logs/                             # λ΅κ·Έ νμΌ μ €μ¥μ†
β”β”€β”€ mlruns/                           # MLflow μ‹¤ν— μ €μ¥μ†
β”β”€β”€ mlruns_backups/                   # MLflow μ‹¤ν— λ°±μ—…
β”β”€β”€ catboost_info/                    # CatBoost μ •λ³΄ νμΌ
β”β”€β”€ tests/                            # ν…μ¤νΈ μ½”λ“
β”β”€β”€ requirements.txt                  # ν•„μ”ν• ν¨ν‚¤μ§€ λ©λ΅
β”β”€β”€ projectplan                       # ν”„λ΅μ νΈ κ³„νμ„
β”β”€β”€ PROJECT_PROGRESS.md              # ν”„λ΅μ νΈ μ§„ν–‰ μƒν™© λ¬Έμ„
β”β”€β”€ refactoring_plan.md              # λ¦¬ν©ν† λ§ κ³„νμ„
β””β”€β”€ README.md                        # μ΄ νμΌ
```

## ν”„λ΅μ νΈ λ£°

### μ‘μ—… λ°©μ‹ μ›μΉ™
- **κ³„ν μ°μ„ **: μ½”λ“ μμ •μ„ μ§„ν–‰ν•κΈ° μ „μ— λ¨Όμ € μƒμ„Έν• κ³„νμ„ μ μ‹
- **μΉμΈ κΈ°λ° μ‹¤ν–‰**: μ‚¬μ©μμ λ…μ‹μ  μΉμΈ(accept)μ„ λ°›μ€ ν›„μ—λ§ μ‹¤μ  μ½”λ“ μμ • μ‹¤ν–‰
- **λ‹¨κ³„λ³„ μ§„ν–‰**: λ³µμ΅ν• μ‘μ—…μ κ²½μ° λ‹¨κ³„λ³„λ΅ κ³„νμ„ μ μ‹ν•κ³  μΉμΈμ„ λ°›μ•„ μ§„ν–‰
- **λ…ν™•ν• μ„¤λ…**: κ° λ‹¨κ³„μ—μ„ λ¬΄μ—‡μ„, μ™, μ–΄λ–»κ² λ³€κ²½ν• μ§€ λ…ν™•ν μ„¤λ…

### μ„¤μ • νμΌ κ΄€λ¦¬ μ›μΉ™
- **μ„¤μ • μ°μ„ **: λ¨λ“  ν•λ“μ½”λ”©λ κ°’μ€ μ„¤μ • νμΌλ΅ μ΄λ™ν•μ—¬ μ¤‘μ•™ κ΄€λ¦¬
- **μ„¤μ • κ²€μ¦**: μ„¤μ • νμΌμ΄ μ—†κ±°λ‚ λ¬Έμ κ°€ μλ” κ²½μ° ν•λ“μ½”λ”©μΌλ΅ μ§„ν–‰ν•μ§€ μ•κ³  μ§„ν–‰μ„ λ©μ¶¤
- **λ…ν™•ν• μ¤λ¥ λ©”μ‹μ§€**: μ„¤μ • νμΌ λ¬Έμ  μ‹ κµ¬μ²΄μ μΈ μμ • λ°©λ²•μ„ λ΅κ·Έλ΅ μ•λ‚΄
- **νƒ€κ² λ³€μλ… μ¤‘μ•™ν™”**: `configs/base/common.yaml`μ `target_variables` μ„Ήμ…μ—μ„ λ¨λ“  νƒ€κ² λ³€μλ… μ •μ
- **νƒ€κ² νƒ€μ… λ…μ‹**: `configs/base/common.yaml`μ `target_types` μ„Ήμ…μ—μ„ νκ·€/λ¶„λ¥ νƒ€μ… λ…μ‹μ  μ •μ

### μ½”λ“ μ»¨λ²¤μ…
- **PEP 8 μ¤€μ**: νμ΄μ¬ μ½”λ“ μ¤νƒ€μΌ κ°€μ΄λ“ μ¤€μ
- **μ„¤μ • κΈ°λ° λ΅μ§**: ν•λ“μ½”λ”© λ€μ‹  μ„¤μ • νμΌ κΈ°λ° λ™μ  μ²λ¦¬
- **μ¤λ¥ μ²λ¦¬**: μ„¤μ • νμΌ λ¬Έμ  μ‹ `SystemExit` λλ” `ValueError`λ΅ λ…ν™•ν• μ¤λ¥ λ°μƒ
- **λ΅κΉ…**: μ„¤μ • νμΌ λ¬Έμ  μ‹ κµ¬μ²΄μ μΈ μμ • λ°©λ²•μ„ `logger.error()`λ΅ μ•λ‚΄

### μ„¤μ • νμΌ κµ¬μ΅°
```yaml
# configs/base/common.yaml
features:
  target_variables:
    original_targets:
      score_targets: ["anxiety_score", "depress_score", "sleep_score", "comp"]
      binary_targets: ["suicide_t", "suicide_a"]
    next_year_targets:
      score_targets: ["anxiety_score_next_year", "depress_score_next_year", "sleep_score_next_year"]
      binary_targets: ["suicide_t_next_year", "suicide_a_next_year"]
  
  target_types:
    regression_targets: ["anxiety_score_next_year", "depress_score_next_year", "sleep_score_next_year"]
    classification_targets: ["suicide_t_next_year", "suicide_a_next_year"]
```

## ν™κ²½ μ„¤μ •

### 1. Conda ν™κ²½ ν™μ„±ν™”
```bash
conda activate simcare
```

### 2. ν•„μ”ν• ν¨ν‚¤μ§€ μ„¤μΉ
```bash
pip install -r requirements.txt
```

> β οΈ λ³Έ ν”„λ΅μ νΈλ” xgboost==1.7.6 λ²„μ „μ— μµμ ν™”λμ–΄ μμµλ‹λ‹¤. requirements.txtμ— λ…μ‹λ λ²„μ „μΌλ΅ μ„¤μΉν•΄μ•Ό μ‹¤ν—μ΄ μ •μƒ λ™μ‘ν•©λ‹λ‹¤.

## μ‹¤ν–‰ λ°©λ²•

### λ°μ΄ν„° λ¶„μ„ λ° μ „μ²λ¦¬ μ‹¤ν–‰
```bash
python src/data_analysis.py
```

μ΄ λ…λ Ήμ–΄λ” λ‹¤μ μ‘μ—…μ„ μν–‰ν•©λ‹λ‹¤:
- μ›λ³Έ λ°μ΄ν„° λ΅λ”© λ° κΈ°λ³Έ μ •λ³΄ λ¶„μ„
- κ²°μΈ΅μΉ λ° μ΄μƒμΉ λ¶„μ„
- μ‹κ³„μ—΄ κΈΈμ΄ λ¶„μ„
- νƒ€κ² λ³€μ λ¶„ν¬ λ¶„μ„
- λ°μ΄ν„° νƒ€μ… λ³€ν™ λ° μ „μ²λ¦¬
- νƒ€κ² λ³€μ μƒμ„± (λ‹¤μ ν•΄ μμΈ΅κ°’)
- ν”Όμ² μ—”μ§€λ‹μ–΄λ§
- κ²°κ³Ό μ €μ¥ λ° MLflow λ΅κΉ…

### π€ μ•μ „ν• μ‹¤ν— μ¤ν¬λ¦½νΈ μ‚¬μ©λ²• (κ¶μ¥)

#### 1. μ¤ν¬λ¦½νΈ ν…ν”λ¦Ώ μ‚¬μ© (κ°€μ¥ μ•μ „)
```bash
# ν…ν”λ¦Ώμ„ λ³µμ‚¬ν•μ—¬ μƒλ΅μ΄ μ‹¤ν— μ¤ν¬λ¦½νΈ μƒμ„±
cp scripts/template_experiment.sh scripts/my_experiment.sh

# μ‹¤ν— μ΄λ¦„κ³Ό μ„¤μ •μ„ μμ •ν• ν›„ μ‹¤ν–‰
chmod +x scripts/my_experiment.sh
./scripts/my_experiment.sh
```

**ν…ν”λ¦Ώμ μ£Όμ” νΉμ§•:**
- **λ©”λ¨λ¦¬ μ•μ „**: `n_jobs=4`λ΅ μ„¤μ •ν•μ—¬ λ©”λ¨λ¦¬ μ‚¬μ©λ‰ μµμ†ν™”
- **μλ™ λ©”λ¨λ¦¬ μ •λ¦¬**: κ° λ¨λΈ μ‹¤ν–‰ ν›„ κ°•ν™”λ λ©”λ¨λ¦¬ μ •λ¦¬
- **μ¤λ¥ ν—μ©**: ν• λ¨λΈμ΄ μ‹¤ν¨ν•΄λ„ λ‹¤μ λ¨λΈλ΅ κ³„μ† μ§„ν–‰
- **μƒμ„Έν• λ΅κΉ…**: κ° λ‹¨κ³„λ³„ λ©”λ¨λ¦¬ μƒνƒ λ° μ§„ν–‰ μƒν™© κΈ°λ΅
- **λ¨λ“ν™”λ κµ¬μ΅°**: `run_model()` ν•¨μλ΅ μ¬μ‚¬μ© κ°€λ¥ν• κµ¬μ΅°
- **ν™•μ¥ κ°€λ¥**: Phase 3 μ£Όμ„ ν•΄μ λ΅ μ¶”κ°€ μ‹¤ν— μ‰½κ² μ¶”κ°€
- **ν„μ‹¤μ μΈ κΈ°λ³Έκ°’**: n_trials=100, μ „μ²΄ λ°μ΄ν„°μ…‹ μ‚¬μ©

#### 2. κΈ°μ΅΄ κ²€μ¦λ μ¤ν¬λ¦½νΈ μ‚¬μ©
```bash
# κ°λ³„ λ¨λΈ μ‹¤ν–‰ (λ©”λ¨λ¦¬ μµμ ν™”)
./scripts/run_individual_models.sh

# νΉμ • n_trialsλ΅ μ‹¤ν–‰ (κΈ°λ³Έκ°’: 100)
./scripts/run_individual_models.sh 50
```

**μ¤ν¬λ¦½νΈ νΉμ§•:**
- **Phase 1**: κΈ°λ³Έ λ¨λΈλ“¤ (catboost, random_forest, xgboost, lightgbm)
- **Phase 2**: SMOTE λ¦¬μƒν”λ§ λ¨λΈλ“¤
- **λ©”λ¨λ¦¬ μµμ ν™”**: n_jobs=4, κ°•ν™”λ λ©”λ¨λ¦¬ μ •λ¦¬, 15λ¶„ λ€κΈ°
- **μ „μ²΄ λ°μ΄ν„°μ…‹**: nrows μµμ… λ―Έμ§€μ •μΌλ΅ μ „μ²΄ λ°μ΄ν„° μ‚¬μ©

#### 3. π€ test_input_resampling μ‹μ¤ν… μ‚¬μ© (λ€κ·λ¨ μ‹¤ν— κ¶μ¥)
```bash
# λ§μ¤ν„° μ¤ν¬λ¦½νΈλ΅ μ „μ²΄ μ‹¤ν— κ΄€λ¦¬
cd scripts/test_input_resampling/
chmod +x *.sh
./master_experiment_runner.sh
```

**test_input_resampling μ‹μ¤ν… νΉμ§•:**
- **5λ‹¨κ³„ μ²΄κ³„μ  μ‹¤ν—**: Phase 1-5λ΅ λ‹¨κ³„λ³„ μ‹¤ν— μ§„ν–‰
- **κ°•ν™”λ λ©”λ¨λ¦¬ κ΄€λ¦¬**: μ‹¤μ‹κ°„ λ¨λ‹ν„°λ§ λ° μλ™ μ •λ¦¬
- **μ•μ „ν• μ‹¤ν— ν™κ²½**: μ‹¤ν— μ¤‘λ‹¨/μ¬μ‹μ‘ κΈ°λ¥
- **ν†µν•© μ‹¤ν— κ΄€λ¦¬**: λ§μ¤ν„° μ¤ν¬λ¦½νΈλ΅ λ¨λ“  μ‹¤ν— ν†µν•© κ΄€λ¦¬
- **μƒμ„Έν• κ°€μ΄λ“**: experiment_setup_guide.mdλ΅ μ‚¬μ©λ²• μ•λ‚΄

**5λ‹¨κ³„ μ‹¤ν— κµ¬μ„±:**
- **Phase 1**: κΈ°μ¤€μ„  μ„¤μ • (4κ° λ¨λΈ, 50 trials, 4-6μ‹κ°„)
- **Phase 2**: Input λ²”μ„ μ΅°μ • (λ°μ΄ν„° ν¬κΈ°, ν”Όμ² μ„ νƒ, 6-8μ‹κ°„)
- **Phase 3**: λ¦¬μƒν”λ§ λΉ„κµ (35κ° μ‹¤ν—, 8-12μ‹κ°„)
- **Phase 4**: λ¨λΈ μ‹¬μΈµ λ¶„μ„ (16κ° μ‹¤ν—, 12-16μ‹κ°„)
- **Phase 5**: ν†µν•© μµμ ν™” (14κ° μ‹¤ν—, 4-6μ‹κ°„)

**μ‹μ¤ν… μ”κµ¬μ‚¬ν•­:**
- **CPU**: μµμ† 8μ½”μ–΄ (κ¶μ¥ 16μ½”μ–΄ μ΄μƒ)
- **λ©”λ¨λ¦¬**: μµμ† 16GB (κ¶μ¥ 32GB μ΄μƒ)
- **μ €μ¥κ³µκ°„**: μµμ† 50GB μ—¬μ κ³µκ°„
- **μ‹κ°„**: μ „μ²΄ μ‹¤ν— 2-4μΌ μ†μ”

#### 4. λΉ λ¥Έ ν…μ¤νΈ μ¤ν¬λ¦½νΈ μ‚¬μ©
```bash
# λΉ λ¥Έ ν…μ¤νΈ μ‹¤ν–‰ (μ†κ·λ¨ λ°μ΄ν„°, μ μ€ n_trials)
./scripts/quick_test.sh
```

**λΉ λ¥Έ ν…μ¤νΈ νΉμ§•:**
- **μ†κ·λ¨ λ°μ΄ν„°**: nrows=1000 (μ „μ²΄ λ°μ΄ν„°μ μΌλ¶€)
- **μ μ€ μ‹λ„ νμ**: n_trials=10 (λΉ λ¥Έ κ²°κ³Ό ν™•μΈ)
- **λ‚®μ€ λ³‘λ ¬ μ²λ¦¬**: n_jobs=2 (λ©”λ¨λ¦¬ μ μ•½)
- **μƒμ„Έ λ΅κΉ…**: verbose=2 (λ””λ²„κΉ… μ©μ΄)
- **λ¨λ“  λ¨λΈ ν…μ¤νΈ**: 4κ° λ¨λΈ λ¨λ‘ λΉ λ¥Έ ν…μ¤νΈ

**μ°Έκ³ **: ν”Όμ² μ„ νƒμ€ `configs/base/common.yaml`μ `selected_features`μ—μ„ μ¤‘μ•™ κ΄€λ¦¬λ©λ‹λ‹¤. 
ν”Όμ² μ΅°ν•©μ„ λ³€κ²½ν•λ ¤λ©΄ μ„¤μ • νμΌμ„ μμ •ν•μ„Έμ”.

#### 3. μ¤ν¬λ¦½νΈ ν…ν”λ¦Ώ μ»¤μ¤ν„°λ§μ΄μ§• κ°€μ΄λ“

**κΈ°λ³Έ μ„¤μ • μμ •:**
```bash
# λ³‘λ ¬ μ²λ¦¬ μ„¤μ • (μ‹μ¤ν…μ— λ§κ² μ΅°μ •)
N_JOBS=4  # μ•μ „ν• κ°’: 4-8, κ³ μ„±λ¥: 16-28

# λ©”λ¨λ¦¬ μ ν• μ„¤μ •
export MEMORY_LIMIT=50  # GB λ‹¨μ„

# μ‹¤ν— μ΄λ¦„ μμ •
echo "μ‹¤ν— μ‹μ‘: [μ‹¤ν— μ΄λ¦„μ„ μ—¬κΈ°μ— μ…λ ¥ν•μ„Έμ”]"  # μ›ν•λ” μ‹¤ν— μ΄λ¦„μΌλ΅ λ³€κ²½

# n_trials μ„¤μ • (template_experiment.shμ run_model ν•¨μ λ‚΄)
--n-trials 100  # κΈ°λ³Έκ°’, ν•„μ”μ‹ 50-200μΌλ΅ μ΅°μ •
```

**λ¨λΈ μ¶”κ°€/μ κ±°:**
```bash
# Phase 1μ— λ¨λΈ μ¶”κ°€
run_model "xgboost" "xgboost_basic" ""

# Phase 2μ— μƒλ΅μ΄ λ¦¬μƒν”λ§ λ°©λ²• μ¶”κ°€
run_model "lightgbm" "lightgbm_adasyn" "--resampling-enabled --resampling-method adasyn --resampling-ratio 0.5"

# Phase 3 μ£Όμ„ ν•΄μ ν•μ—¬ μ¶”κ°€ μ‹¤ν— μ‹¤ν–‰
# run_model "xgboost" "xgboost_adasyn" "--resampling-enabled --resampling-method adasyn --resampling-ratio 0.5"
# run_model "catboost" "catboost_feature_selection" "--feature-selection --feature-selection-method mutual_info --feature-selection-k 10"
```

**μ¶”κ°€ νλΌλ―Έν„° μ„¤μ •:**
```bash
# ν•μ΄νΌνλΌλ―Έν„° νλ‹ μ‹λ„ νμ μ΅°μ •
--n-trials 100  # κΈ°λ³Έκ°’, ν•„μ”μ‹ 50-200μΌλ΅ μ΅°μ •

# νƒ€μ„μ•„μ›ƒ μ„¤μ • μ¶”κ°€
--timeout 3600  # 1μ‹κ°„ νƒ€μ„μ•„μ›ƒ

# Early Stopping μ¶”κ°€
--early-stopping --early-stopping-rounds 50

# λ΅κ·Έ λ λ²¨ μ„¤μ •
--verbose 1  # κΈ°λ³Έ λ΅κ·Έ λ λ²¨ (0: μµμ†, 1: κΈ°λ³Έ, 2: μƒμ„Έ)
```

### ConfigManager κΈ°λ° ν•μ΄νΌνλΌλ―Έν„° νλ‹ μ‹¤ν–‰
```bash
# XGBoost λ¨λΈ νλ‹
python scripts/run_hyperparameter_tuning.py --model-type xgboost --experiment-type hyperparameter_tuning --nrows 10000

# CatBoost λ¨λΈ νλ‹
python scripts/run_hyperparameter_tuning.py --model-type catboost --experiment-type hyperparameter_tuning --nrows 10000

# LightGBM λ¨λΈ νλ‹
python scripts/run_hyperparameter_tuning.py --model-type lightgbm --experiment-type hyperparameter_tuning --nrows 10000

# Random Forest λ¨λΈ νλ‹
python scripts/run_hyperparameter_tuning.py --model-type random_forest --experiment-type hyperparameter_tuning --nrows 10000
```

### π€ μλ™ν™”λ μ‹¤ν—μ„ μ„ν• κ³ κΈ‰ λ…λ Ήν–‰ μΈμ μ‚¬μ©λ²•

#### κΈ°λ³Έ μΈμ
```bash
# ν•„μ μΈμ
--model-type {xgboost,lightgbm,random_forest,catboost}  # μ‚¬μ©ν•  λ¨λΈ νƒ€μ…
--experiment-type {hyperparameter_tuning,focal_loss,resampling}  # μ‹¤ν— νƒ€μ…

# λ°μ΄ν„° κ΄€λ ¨ μΈμ
--data_path PATH                    # λ°μ΄ν„° νμΌ κ²½λ΅
--nrows INT                         # μ‚¬μ©ν•  λ°μ΄ν„° ν–‰ μ (ν…μ¤νΈμ©)
```

#### κ²€μ¦ λ° λ¶„ν•  κ΄€λ ¨ μΈμ
```bash
# λ°μ΄ν„° λ¶„ν•  μ „λµ
--split-strategy {group_kfold,time_series_walk_forward,time_series_group_kfold}
--cv-folds INT                      # κµμ°¨ κ²€μ¦ ν΄λ“ μ (κΈ°λ³Έ: 5)
--test-size FLOAT                   # ν…μ¤νΈ μ„ΈνΈ λΉ„μ¨ (0.0-1.0, κΈ°λ³Έ: 0.15)
--random-state INT                  # λλ¤ μ‹λ“ (κΈ°λ³Έ: 42)
```

#### ν•μ΄νΌνλΌλ―Έν„° νλ‹ κ΄€λ ¨ μΈμ
```bash
# νλ‹ μ„¤μ •
--n-trials INT                      # νλ‹ μ‹λ„ νμ (κΈ°λ³Έ: 100)
--tuning-direction {maximize,minimize}  # νλ‹ λ°©ν–¥
--primary-metric STR                # μ£Όμ” ν‰κ°€ μ§€ν‘ (f1, precision, recall, mcc, roc_auc, pr_auc λ“±)
--n-jobs INT                        # λ³‘λ ¬ μ²λ¦¬ μ‘μ—… μ
--timeout INT                       # νλ‹ νƒ€μ„μ•„μ›ƒ (μ΄)
```

#### Early Stopping κ΄€λ ¨ μΈμ
```bash
--early-stopping                    # Early stopping ν™μ„±ν™”
--early-stopping-rounds INT         # Early stopping λΌμ΄λ“ μ
```

#### ν”Όμ² μ„ νƒ κ΄€λ ¨ μΈμ
```bash
--feature-selection                 # ν”Όμ² μ„ νƒ ν™μ„±ν™”
--feature-selection-method {mutual_info,chi2,f_classif,recursive}  # ν”Όμ² μ„ νƒ λ°©λ²•
--feature-selection-k INT           # μ„ νƒν•  ν”Όμ² μ
```

#### λ¦¬μƒν”λ§ κ΄€λ ¨ μΈμ
```bash
--resampling-enabled                # λ¦¬μƒν”λ§ ν™μ„±ν™”
--resampling-method {smote,borderline_smote,adasyn,under_sampling,hybrid}  # λ¦¬μƒν”λ§ λ°©λ²•
--resampling-ratio FLOAT            # λ¦¬μƒν”λ§ ν›„ μ–‘μ„± ν΄λμ¤ λΉ„μ¨
```

#### MLflow λ° κ²°κ³Ό μ €μ¥ κ΄€λ ¨ μΈμ
```bash
--experiment-name STR               # MLflow μ‹¤ν— μ΄λ¦„
--save-model                        # μµμ  λ¨λΈ μ €μ¥
--save-predictions                  # μμΈ΅ κ²°κ³Ό μ €μ¥
--mlflow_ui                         # νλ‹ μ™„λ£ ν›„ MLflow UI μ‹¤ν–‰
--verbose {0,1,2}                   # λ΅κ·Έ λ λ²¨ (0: μµμ†, 1: κΈ°λ³Έ, 2: μƒμ„Έ)
```

#### μ‹¤μ©μ μΈ μ‹¤ν— μμ‹

**1. λΉ λ¥Έ ν…μ¤νΈ (μ†κ·λ¨ λ°μ΄ν„°)**
```bash
python scripts/run_hyperparameter_tuning.py \
  --model-type xgboost \
  --experiment-type hyperparameter_tuning \
  --nrows 1000 \
  --n-trials 10 \
  --cv-folds 3 \
  --verbose 2
```

**2. μ‹κ°„ κΈ°λ° λ¶„ν•  μ‹¤ν—**
```bash
python scripts/run_hyperparameter_tuning.py \
  --model-type catboost \
  --experiment-type hyperparameter_tuning \
  --split-strategy time_series_walk_forward \
  --cv-folds 5 \
  --n-trials 50 \
  --primary-metric pr_auc
```

**3. λ¦¬μƒν”λ§ λΉ„κµ μ‹¤ν—**
```bash
python scripts/run_hyperparameter_tuning.py \
  --model-type lightgbm \
  --experiment-type resampling \
  --resampling-comparison \
  --resampling-methods smote adasyn \
  --n-trials 30 \
  --cv-folds 5
```

**4. ν”Όμ² μ„ νƒ μ‹¤ν—**
```bash
python scripts/run_hyperparameter_tuning.py \
  --model-type random_forest \
  --experiment-type hyperparameter_tuning \
  --feature-selection \
  --feature-selection-method mutual_info \
  --feature-selection-k 10 \
  --n-trials 50 \
  --save-model \
  --save-predictions
```

**5. κ³ μ„±λ¥ μ‹¤ν— (μ „μ²΄ λ°μ΄ν„°)**
```bash
python scripts/run_hyperparameter_tuning.py \
  --model-type xgboost \
  --experiment-type hyperparameter_tuning \
  --n-trials 200 \
  --cv-folds 5 \
  --primary-metric f1 \
  --early-stopping \
  --early-stopping-rounds 50 \
  --save-model \
  --save-predictions \
  --mlflow_ui
```

### π” λ¦¬μƒν”λ§ μ‹¤ν— λ™μ‘ λ°©μ‹ μƒμ„Έ μ„¤λ…

#### κΈ°λ³Έ λ™μ‘ μ›λ¦¬
λ¦¬μƒν”λ§ μ‹¤ν—μ€ **ν•μ΄νΌνλΌλ―Έν„° νλ‹ + λ¦¬μƒν”λ§ νλ‹μ„ λ™μ‹μ— μν–‰**ν•©λ‹λ‹¤:

1. **Optunaκ°€ λ§¤ trialμ—μ„ λ‹¤μ μ¤‘ ν•λ‚ μ„ νƒ**:
   ```python
   ['none', 'smote', 'borderline_smote', 'adasyn', 'under_sampling', 'hybrid']
   ```

2. **μ„ νƒλ λ°©λ²•μ— λ”°λΌ**:
   - **λ¨λΈ ν•μ΄νΌνλΌλ―Έν„°**: ν•­μƒ νλ‹ (n_estimators, max_depth λ“±)
   - **λ¦¬μƒν”λ§ νλΌλ―Έν„°**: μ„ νƒλ λ°©λ²•μ— λ”°λΌ νλ‹ (k_neighbors, sampling_strategy λ“±)

#### μ‹¤ν— κ²°κ³Ό ν•΄μ„
- **`λ¦¬μƒν”λ§ μ μ©: μ•„λ‹μ¤`**: Optunaκ°€ `'none'`μ„ μµμ  λ°©λ²•μΌλ΅ μ„ νƒ
- **`λ¦¬μƒν”λ§ μ μ©: μ`**: νΉμ • λ¦¬μƒν”λ§ λ°©λ²•μ΄ μµμ μΌλ΅ μ„ νƒλ¨

#### μ‹¤ν–‰ μμ‹
```bash
# μλ™ νλ‹ (Optunaκ°€ μµμ  λ°©λ²• μλ™ μ„ νƒ)
python scripts/run_resampling_experiment.py --model-type xgboost --n-trials 100

# λΉ„κµ μ‹¤ν— (κ° λ°©λ²•λ³„ κ°λ³„ νλ‹)
python scripts/run_resampling_experiment.py --model-type xgboost --resampling-comparison --resampling-methods smote adasyn --n-trials 50
```

### ConfigManager κΈ°λ° λ¦¬μƒν”λ§ λΉ„κµ μ‹¤ν— μ‹¤ν–‰ (β… μµμ‹  κΈ°λ¥)
```bash
# λ¨λ“  λ¨λΈμ— λ€ν•΄ λ¦¬μƒν”λ§ λΉ„κµ μ‹¤ν—
python scripts/run_hyperparameter_tuning.py --model-type xgboost --experiment-type resampling --resampling-comparison
python scripts/run_hyperparameter_tuning.py --model-type catboost --experiment-type resampling --resampling-comparison
python scripts/run_hyperparameter_tuning.py --model-type lightgbm --experiment-type resampling --resampling-comparison
python scripts/run_hyperparameter_tuning.py --model-type random_forest --experiment-type resampling --resampling-comparison

# νΉμ • λ¦¬μƒν”λ§ κΈ°λ²•λ§ λΉ„κµ
python scripts/run_hyperparameter_tuning.py --model-type xgboost --experiment-type resampling --resampling-comparison --resampling-methods smote adasyn

# μ‹κ³„μ—΄ νΉν™” λ¦¬μƒν”λ§ μ‹¤ν— (β… 2025-07-16 μ‹ κ· κΈ°λ¥)
python scripts/run_hyperparameter_tuning.py --model-type xgboost --experiment-type resampling --resampling-method time_series_adapted
```

### μ „μ© λ¦¬μƒν”λ§ μ‹¤ν— μ¤ν¬λ¦½νΈ μ‚¬μ©λ²•
```bash
# λ¦¬μƒν”λ§ λΉ„κµ μ‹¤ν— (μ „μ© μ¤ν¬λ¦½νΈ)
python scripts/run_resampling_experiment.py --model-type xgboost --resampling-methods smote adasyn borderline_smote

# λ¦¬μƒν”λ§ ν•μ΄νΌνλΌλ―Έν„° νλ‹
python scripts/run_resampling_experiment.py --model-type catboost --resampling-method smote --tune-parameters
```

### λ¦¬μƒν”λ§ ν•μ΄νΌνλΌλ―Έν„° νλ‹ μ‹¤ν–‰ (β… 2025-07-16 μ‹ κ· κΈ°λ¥)
```bash
# SMOTE k_neighbors λ° sampling_strategy νλ‹
python scripts/run_hyperparameter_tuning.py --model-type xgboost --experiment-type resampling --resampling-method smote --n-trials 50

# Borderline SMOTE νλΌλ―Έν„° νλ‹
python scripts/run_hyperparameter_tuning.py --model-type catboost --experiment-type resampling --resampling-method borderline_smote --n-trials 50

# ADASYN νλΌλ―Έν„° νλ‹
python scripts/run_hyperparameter_tuning.py --model-type lightgbm --experiment-type resampling --resampling-method adasyn --n-trials 50

# μ‹κ³„μ—΄ νΉν™” λ¦¬μƒν”λ§ νλΌλ―Έν„° νλ‹
python scripts/run_hyperparameter_tuning.py --model-type random_forest --experiment-type resampling --resampling-method time_series_adapted --n-trials 50
```

### μ‹κ°ν™” νμΌ κ΄€λ¦¬ μ‹μ¤ν… μ‚¬μ©λ²• (β… 2025-08-04 μµμ‹  κΈ°λ¥)

#### π“ μƒμ„±λλ” ν΄λ” κµ¬μ΅°
```
results/visualizations/
β”β”€β”€ hyperparameter_tuning_experiment_xgboost_20250804_100253/
β”‚   β””β”€β”€ optimization_visualization.png
β”β”€β”€ hyperparameter_tuning_experiment_xgboost_20250804_100254/
β”‚   β”β”€β”€ cv_score_distribution.png
β”‚   β””β”€β”€ learning_curves.png
β”β”€β”€ resampling_experiment_xgboost_20250804_100933/
β”‚   β””β”€β”€ optimization_visualization.png
β””β”€β”€ resampling_experiment_xgboost_20250804_100934/
    β”β”€β”€ cv_score_distribution.png
    β””β”€β”€ learning_curves.png
```

#### π― μ‹κ°ν™” νμΌ μΆ…λ¥
- **`optimization_visualization.png`**: μµμ ν™” κ³Όμ • μΆ…ν•© μ‹κ°ν™” (6κ° μ„λΈν”λ΅―)
  - μµμ ν™” νμ¤ν† λ¦¬ ν”λ΅―
  - νλΌλ―Έν„° μ¤‘μ”λ„ ν”λ΅―
  - λ³‘λ ¬ μΆν‘ ν”λ΅―
  - μ¬λΌμ΄μ¤ ν”λ΅―
  - μ»¨ν¬μ–΄ ν”λ΅―
  - μλ™ νλΌλ―Έν„° μ¤‘μ”λ„ ν”λ΅―
- **`cv_score_distribution.png`**: κµμ°¨ κ²€μ¦ μ μ λ¶„ν¬ (λ°•μ¤ν”λ΅―)
- **`learning_curves.png`**: ν΄λ“λ³„ ν•™μµ/κ²€μ¦ κ³΅μ„ 

#### π”§ ν΄λ”λ… κ·μΉ™
- **ν•μ‹**: `{experiment_type}_{model_type}_{timestamp}`
- **μμ‹**: `hyperparameter_tuning_experiment_xgboost_20250804_100253`
- **κµ¬μ„± μ”μ†**:
  - `experiment_type`: μ‹¤ν— νƒ€μ… (hyperparameter_tuning, resampling_experiment λ“±)
  - `model_type`: λ¨λΈ νƒ€μ… (xgboost, lightgbm, catboost, random_forest)
  - `timestamp`: μ‹¤ν–‰ μ‹κ°„ (YYYYMMDD_HHMMSS)

#### π“ MLflow μ•„ν‹°ν©νΈ μ—°λ™
- λ¨λ“  μ‹κ°ν™” νμΌμ΄ MLflow μ•„ν‹°ν©νΈλ΅ μλ™ λ΅κΉ…
- μ‹¤ν— μ¶”μ  λ° λ²„μ „ κ΄€λ¦¬ κ°€λ¥
- MLflow UIμ—μ„ μ‹κ°ν™” νμΌ μ§μ ‘ ν™•μΈ κ°€λ¥

### λ κ±°μ‹ λ‹¨μΌ νμΌ config μ‚¬μ© (λ°±μ›λ“ νΈν™μ„±)
```bash
python scripts/run_hyperparameter_tuning.py --tuning_config configs/hyperparameter_tuning.yaml --base_config configs/default_config.yaml
```

## μƒμ„±λλ” νμΌλ“¤

### λ¶„μ„ κ²°κ³Ό (data/sourcedata_analysis/)
- **figures/**: 7κ°μ λ¶„μ„ κ·Έλν”„ (PNG)
- **reports/**: 6κ°μ λ¶„μ„ λ¦¬ν¬νΈ (TXT)

### μ „μ²λ¦¬ λ°μ΄ν„° (data/processed/)
- **processed_data_with_features.csv**: ν”Όμ² μ—”μ§€λ‹μ–΄λ§μ΄ μ™„λ£λ λ°μ΄ν„°

### μ‹¤ν— κ²°κ³Ό (MLflow)
- **μ‹¤ν— νλΌλ―Έν„°**: μ„¤μ • νμΌμ λ¨λ“  νλΌλ―Έν„°
- **ν΄λ“λ³„ λ©”νΈλ¦­**: κ° κµμ°¨ κ²€μ¦ ν΄λ“μ μ„±λ¥ μ§€ν‘
- **κ³ κΈ‰ ν‰κ°€ μ§€ν‘**: Balanced Accuracy, Precision-Recall Curve, ROC-AUC vs PR-AUC λΉ„κµ
- **λ¨λΈ μ•„ν‹°ν©νΈ**: ν•™μµλ λ¨λΈ λ° κ²°κ³Ό μ”μ•½
- **λ©”νΈλ¦­ μλ™ λ¶„λ¥**: 6κ° μΉ΄ν…κ³ λ¦¬(fold_metrics, trial_metrics, cv_metrics, feature_importance, model_complexity, basic_metrics)λ΅ μλ™ λ¶„λ¥
- **Trialλ³„ μ„±λ¥ λ¶„μ„**: μµκ³ /ν‰κ· /μµμ € Trial μ„±λ¥ λ° μ„±κ³µν• Trial μ ν†µκ³„

### νλ‹ κ²°κ³Ό (models/)
- **best_tuned_model.joblib**: μµμ  νλΌλ―Έν„°λ΅ ν•™μµλ λ¨λΈ
- **optuna_study.pkl**: Optuna study κ°μ²΄
- **optimization_plots.png**: νλ‹ κ³Όμ • μ‹κ°ν™”

### λ¦¬μƒν”λ§ μ‹¤ν— κ²°κ³Ό (data/resampling_results/)
- **resampling_comparison_*.csv**: λ¦¬μƒν”λ§ κΈ°λ²•λ³„ μ„±λ¥ λΉ„κµ
- **resampling_parameters_*.csv**: λ¦¬μƒν”λ§ νλΌλ―Έν„° νλ‹ κ²°κ³Ό

### ν•μ΄νΌνλΌλ―Έν„° νλ‹ κ²°κ³Ό (data/tuning_results/)
- **tuning_history_*.csv**: νλ‹ κ³Όμ • νμ¤ν† λ¦¬
- **best_parameters_*.json**: μµμ  νλΌλ―Έν„° μ €μ¥

## μ£Όμ” νΉμ§•

### λ°μ΄ν„° νΉμ„±
- **κ·λ¨**: 1,569,071 ν–‰ Γ— 15 μ—΄
- **κ°μΈ μ**: 269,339λ…
- **κΈ°κ°„**: 2015-2024 (10λ…„)
- **μ‹κ³„μ—΄ κΈΈμ΄**: ν‰κ·  5.79λ…„/κ°μΈ

### μμΈ΅ λ©ν‘
- **μ—°μ†ν•**: anxiety_score, depress_score, sleep_score
- **μ΄μ§„ν•**: suicide_t (μμ‚΄ μ‚¬κ³ ), suicide_a (μμ‚΄ μ‹λ„)

### μ£Όμ” λ„μ „κ³Όμ 
- **κ·Ήλ„ λ¶κ· ν•**: μμ‚΄ μ‹λ„ 0.12% (849:1)
- **μ‹κ³„μ—΄ λ‹¤μ–‘μ„±**: κ°μΈλ³„ μ‹κ³„μ—΄ κΈΈμ΄ 1-10λ…„
- **λ°μ΄ν„° ν’μ§**: μΌλ¶€ μ΄μƒμΉ λ° κ²°μΈ΅μΉ μ΅΄μ¬

### κµ¬ν„λ λ¨λΈ (β… 4κ° λ¨λΈ μ™„λ£)
- **XGBoost λ¨λΈ**: κ·Ήλ‹¨μ  λ¶κ· ν• λ°μ΄ν„° μ²λ¦¬ (μ •ν™•λ„ 99.87%)
- **CatBoost λ¨λΈ**: λ²”μ£Όν• λ³€μ μ²λ¦¬ κ°•μ , κ· ν•μ΅ν μ„±λ¥ (μ •ν™•λ„ 85%, AUC-ROC 0.91)
- **LightGBM λ¨λΈ**: λΉ λ¥Έ ν•™μµ μ†λ„μ™€ λ†’μ€ μ„±λ¥ (μ •ν™•λ„ 84%, AUC-ROC 0.90)
- **Random Forest λ¨λΈ**: ν•΄μ„ κ°€λ¥μ„±κ³Ό μ•μ •μ„± (μ •ν™•λ„ 83%, AUC-ROC 0.89)
- **λ¨λΈ μ•„ν‚¤ν…μ² ν‘μ¤€ν™”**: BaseModel μ¶”μƒ ν΄λμ¤μ™€ ModelFactoryλ¥Ό ν†µν• μΌκ΄€λ μΈν„°νμ΄μ¤
- **λ¨λΈλ³„ λ°μ΄ν„° κ²€μ¦ μµμ ν™”**: κ° λ¨λΈμ νΉμ„±μ— λ§λ” `_validate_input_data` λ©”μ„λ“ μ¤λ²„λΌμ΄λ“
  - XGBoost: λ²”μ£Όν• λ³€μλ¥Ό μ«μλ΅ λ³€ν™ (XGBoost νΈν™μ„±)
  - CatBoost/LightGBM/Random Forest: λ²”μ£Όν• λ³€μ λ³΄μ΅΄ (λ¨λΈ μμ²΄ μ²λ¦¬)
- **ModelFactory ν¨ν„΄**: λ™μ  λ¨λΈ μƒμ„± λ° λ“±λ΅ μ‹μ¤ν…μΌλ΅ ν™•μ¥μ„± ν™•λ³΄

### ν‰κ°€ μ‹μ¤ν… κ°μ„  (β… μµμ‹  μ—…λ°μ΄νΈ)
- **μ¤‘μ•™ν™”λ ν‰κ°€ λ΅μ§**: `evaluation.py`μ `calculate_all_metrics`κ°€ λ¨λ“  ν‰κ°€μ λ‹¨μΌ μ§„μ…μ 
- **λ³µμ΅ν• μ»¬λΌ λ§¤μΉ­**: μ „μ²λ¦¬ ν›„ μ»¬λΌλ… λ³€κ²½(`remainder__`, `pass__`, `num__`, `cat__` μ ‘λ‘μ‚¬)μ—λ„ μ•μ •μ  λ§¤μΉ­
- **μ„¤μ • κΈ°λ° νƒ€κ² νƒ€μ… μ •μ**: ν•λ“μ½”λ”©λ λ΅μ§ λ€μ‹  `configs/base/common.yaml`μ `target_types` μ„Ήμ…μ—μ„ λ…μ‹μ  μ •μ
- **λ‹¤μ¤‘ νƒ€κ² μ§€μ›**: κ° νƒ€κ²λ³„λ΅ κ°λ³„ ν‰κ°€ μν–‰ ν›„ ν†µν•© κ²°κ³Ό λ°ν™
- **νƒ€κ²λ³„ λ©”νΈλ¦­ κµ¬μ΅°**: ν‰λ©΄ν™”λ λ©”νΈλ¦­μ—μ„ νƒ€κ²λ³„ κµ¬μ΅°ν™”λ λ©”νΈλ¦­μΌλ΅ κ°μ„ 
- **ν™•μ¥λ ν‰κ°€ μ§€ν‘**: 10κ° μ¶”κ°€ λ©”νΈλ¦­ (MCC, Kappa, Specificity, NPV, FPR, FNR λ“±)

### π€ μλ™ν™”λ μ‹¤ν— μ‹μ¤ν… (β… μµμ‹  κΈ°λ¥)
- **λ…λ Ήν–‰ μΈμ κΈ°λ° μ‹¤ν— μ μ–΄**: 20κ° μ΄μƒμ μΈμλ΅ μ‹¤ν— μ™„μ „ μλ™ν™”
- **ConfigManager κΈ°λ° μ„¤μ • κ΄€λ¦¬**: κ³„μΈµμ  μ„¤μ • νμΌ μλ™ λ³‘ν•© λ° κ²€μ¦
- **μ μ—°ν• λ°μ΄ν„° λ¶„ν• **: 3κ°€μ§€ λ¶„ν•  μ „λµ (group_kfold, time_series_walk_forward, time_series_group_kfold)
- **κ³ κΈ‰ ν•μ΄νΌνλΌλ―Έν„° νλ‹**: Optuna κΈ°λ° μµμ ν™”, Early Stopping, νƒ€μ„μ•„μ›ƒ μ§€μ›
- **ν”Όμ² μ„ νƒ μλ™ν™”**: Mutual Info, Chi2, F-test, Recursive Feature Elimination μ§€μ›
- **λ¦¬μƒν”λ§ μ‹¤ν— ν†µν•©**: SMOTE, ADASYN, Borderline SMOTE λ“± λΉ„κµ μ‹¤ν—
- **MLflow μ‹¤ν— μ¶”μ **: λ¨λ“  μ‹¤ν— κ²°κ³Ό μλ™ λ΅κΉ… λ° μ‹κ°ν™”
- **κ²°κ³Ό μ €μ¥ μλ™ν™”**: λ¨λΈ, μμΈ΅ κ²°κ³Ό, μ‹κ°ν™” μλ™ μ €μ¥
- **μ‹¤ν— κ΄€λ¦¬ μ‹μ¤ν…**: MLflowExperimentManagerλ¥Ό ν†µν• μ‹¤ν— μ •λ¦¬ λ° λ°±μ—… μλ™ν™”

### π”„ λ¦¬μƒν”λ§ μ‹μ¤ν… (β… 2025-07-16 λ€ν­ κ°μ„ )
- **7κ°€μ§€ λ¦¬μƒν”λ§ κΈ°λ²• μ§€μ›**: none, smote, borderline_smote, adasyn, under_sampling, hybrid, time_series_adapted
- **ν•μ΄νΌνλΌλ―Έν„° νλ‹ ν†µν•©**: k_neighbors, sampling_strategy λ“± λ¦¬μƒν”λ§ νλΌλ―Έν„°λ„ Optunaλ΅ μλ™ νλ‹
- **μ‹κ³„μ—΄ νΉν™” λ¦¬μƒν”λ§**: time_weight, temporal_window, seasonality_weight λ“± 5κ° νλΌλ―Έν„°λ΅ μ‹κ°„μ  μΆ…μ†μ„± κ³ λ ¤
- **κ·Ήλ„ λ¶κ· ν• λ°μ΄ν„° λ€μ‘**: 849:1 λ¶κ· ν• λΉ„μ¨μ— μµμ ν™”λ νλΌλ―Έν„° λ²”μ„ μ„¤μ •
- **MLflow μλ™ λ΅κΉ…**: λ¨λ“  λ¦¬μƒν”λ§ νλΌλ―Έν„°μ™€ κ²°κ³Όκ°€ MLflowμ— μλ™ κΈ°λ΅
- **ConfigManager μ—°λ™**: λ¦¬μƒν”λ§ μ„¤μ •μ΄ κ³„μΈµμ  config μ‹μ¤ν…κ³Ό μ™„μ „ ν†µν•©

### λ¶κ· ν• λ°μ΄ν„° μ²λ¦¬
- **ν΄λμ¤ κ°€μ¤‘μΉ, scale_pos_weight**: XGBoost λ“±μ—μ„ λ¶κ· ν• λ°μ΄ν„° μ²λ¦¬λ¥Ό μ„ν• κ°€μ¤‘μΉ μµμ… μ§€μ›

### π› οΈ κ³µν†µ μ ν‹Έλ¦¬ν‹° μ‹μ¤ν… (`src/utils.py`) (β… μµμ‹  μ¶”κ°€)

#### π“ μ‹¤ν— λ΅κΉ… μ‹μ¤ν…
- **ν†µν•© λ΅κΉ… μ»¨ν…μ¤νΈ**: `experiment_logging_context()` ν•¨μλ΅ μ‹¤ν— μ „μ²΄ μƒλ…μ£ΌκΈ° κ΄€λ¦¬
- **μλ™ λ΅κ·Έ νμΌ μƒμ„±**: μ‹¤ν— νƒ€μ…κ³Ό λ¨λΈ νƒ€μ…λ³„λ΅ κµ¬μ΅°ν™”λ λ΅κ·Έ νμΌ μλ™ μƒμ„±
- **μ½μ†” μ¶λ ¥ μΊ΅μ²**: `ConsoleCapture` ν΄λμ¤λ΅ λ¨λ“  ν„°λ―Έλ„ μ¶λ ¥ μλ™ μΊ΅μ² λ° μ €μ¥
- **μμ™Έ μ²λ¦¬ ν†µν•©**: μ‹¤ν— μ¤‘ λ°μƒν•λ” λ¨λ“  μμ™Έ μ •λ³΄ μλ™ κΈ°λ΅
- **μ‹¤ν— μ”μ•½ λ΅κΉ…**: `log_experiment_summary()` ν•¨μλ΅ μ‹¤ν— κ²°κ³Ό κµ¬μ΅°ν™” μ €μ¥

#### π”Ά μ«μ μ²λ¦¬ λ° κ²€μ¦ μ‹μ¤ν…
- **μ•μ „ν• μ«μ λ³€ν™**: `safe_float_conversion()`, `safe_float()` ν•¨μλ΅ MLflow λ©”νΈλ¦­ λ΅κΉ… μ•μ „μ„± λ³΄μ¥
- **μ ν¨μ„± κ²€μ¦**: `is_valid_number()` ν•¨μλ΅ NaN/Inf κ°’ μλ™ κ°μ§€ λ° μ²λ¦¬
- **λ°μ΄ν„° ν’μ§ λ³΄μ¥**: ν•μ΄νΌνλΌλ―Έν„° νλ‹ κ³Όμ •μ—μ„ μ•μ „ν• μ«μ μ²λ¦¬

#### π”§ λ°μ΄ν„° μ²λ¦¬ μ ν‹Έλ¦¬ν‹°
- **μ»¬λΌλ… λ§¤ν•‘**: `find_column_with_remainder()` ν•¨μλ΅ scikit-learn νμ΄ν”„λΌμΈ ν›„ μ»¬λΌλ… μλ™ λ§¤ν•‘
- **μ•μ „ν• ν”Όμ²λ…**: `safe_feature_name()` ν•¨μλ΅ MLflow λ΅κΉ…μ— μ•μ „ν• ν”Όμ²λ… μƒμ„±
- **μ¬ν„μ„± λ³΄μ¥**: `set_random_seed()` ν•¨μλ΅ numpy, random, xgboost μ‹λ“ ν†µν•© κ΄€λ¦¬

#### π“ μ‹¤ν— κ΄€λ¦¬ μ‹μ¤ν…
- **ConfigManager**: κ³„μΈµμ  μ„¤μ • νμΌ κ΄€λ¦¬ λ° μλ™ λ³‘ν•© μ‹μ¤ν… (`src/utils/config_manager.py`)
- **MLflowExperimentManager**: MLflow μ‹¤ν— κ΄€λ¦¬ λ° μ •λ¦¬ μλ™ν™” (`src/utils/mlflow_manager.py`)
- **κ²°κ³Ό μ €μ¥ μ‹μ¤ν…**: `save_experiment_results()` ν•¨μλ΅ μ‹¤ν— κ²°κ³Όλ¥Ό 12κ° μ„Ήμ…μΌλ΅ κµ¬μ΅°ν™” μ €μ¥
  - **MLflow λ©”νΈλ¦­ μ¶”μ¶ κ°•ν™”**: 6κ° μΉ΄ν…κ³ λ¦¬(fold_metrics, trial_metrics, cv_metrics, feature_importance, model_complexity, basic_metrics)λ΅ μλ™ λ¶„λ¥
  - **λ…ν™•ν• μ¤λ¥ λ©”μ‹μ§€**: λ„λ½λ μ •λ³΄μ— λ€ν• κµ¬μ²΄μ μΈ μ›μΈ μ„¤λ… μ κ³µ
  - **Trialλ³„ μ„±λ¥ μ”μ•½**: μµκ³ /ν‰κ· /μµμ € Trial μ„±λ¥ λ° μ„±κ³µν• Trial μ ν‘μ‹
  - **μ‹¤μ‹κ°„ λ©”νΈλ¦­ λ¶„μ„**: κ° μΉ΄ν…κ³ λ¦¬λ³„ λ©”νΈλ¦­ μμ™€ μμ‹λ¥Ό λ΅κ·Έλ΅ μ¶λ ¥

#### π€ μ‚¬μ© μμ‹
```python
from src.utils import experiment_logging_context, log_experiment_summary

# μ‹¤ν— λ΅κΉ… μ»¨ν…μ¤νΈ μ‚¬μ©
with experiment_logging_context(
    experiment_type="hyperparameter_tuning",
    model_type="xgboost",
    log_level="INFO",
    capture_console=True
) as log_file_path:
    # μ‹¤ν— μ½”λ“ μ‹¤ν–‰
    result = run_hyperparameter_tuning()
    
    # μ‹¤ν— μ”μ•½ λ΅κΉ…
    log_experiment_summary(
        experiment_type="hyperparameter_tuning",
        model_type="xgboost",
        best_score=0.85,
        best_params={"max_depth": 6},
        execution_time=3600.5,
        n_trials=100,
        data_info={"total_rows": 10000},
        log_file_path=log_file_path
    )
```

### π€ MLflow λ΅κΉ… μ‹μ¤ν… (β… 2025-08-04 μµμ‹  κΈ°λ¥)
- **ν†µν•© λ΅κΉ… λ¨λ“**: `src/utils/mlflow_logging.py`λ΅ λ¨λ“  MLflow λ΅κΉ… κΈ°λ¥ ν†µν•©
- **ν”Όμ² μ¤‘μ”λ„ λ΅κΉ…**: μƒμ„ 20κ° ν”Όμ², μ‹κ°ν™” μ°¨νΈ, CSV νμΌ μ €μ¥
- **λ¨λΈ μ•„ν‹°ν©νΈ μ €μ¥**: joblib λ¨λΈ, JSON νλΌλ―Έν„°, MLflow λ¨λΈ μ €μ¥
- **μ‹κ°ν™” λ΅κΉ…**: μµμ ν™” μ§„ν–‰λ„, μ„±λ¥ λ¶„ν¬, νλΌλ―Έν„° μ¤‘μ”λ„ μ°¨νΈ
- **λ©”λ¨λ¦¬ μ‚¬μ©λ‰ μ¶”μ **: ν”„λ΅μ„Έμ¤ λ° μ‹μ¤ν… λ©”λ¨λ¦¬ μ •λ³΄
- **ν•™μµ κ³΅μ„  λ΅κΉ…**: ν΄λ“λ³„ ν•™μµ/κ²€μ¦ μ†μ‹¤ λ° μ •ν™•λ„
- **λ¦¬μƒν”λ§ νΉν™” λ¶„μ„**: ν΄λμ¤ λ¶„ν¬, λ¶κ· ν• λΉ„μ¨, λ¦¬μƒν”λ§ ν¨κ³Ό
- **ν†µν•© μΈν„°νμ΄μ¤**: `log_all_advanced_metrics()` ν•¨μλ΅ λ¨λ“  λ΅κΉ… κΈ°λ¥ ν• λ²μ— μ‹¤ν–‰
- **μ½”λ“ μ¤‘λ³µ μ κ±°**: 800μ¤„ μ΄μƒμ μ¤‘λ³µ μ½”λ“ μ κ±°λ΅ μ μ§€λ³΄μμ„± λ€ν­ ν–¥μƒ
- **μ¬μ‚¬μ©μ„± ν–¥μƒ**: μƒλ΅μ΄ μ‹¤ν—μ—μ„λ„ λ™μΌν• λ΅κΉ… κΈ°λ¥ μ‰½κ² μ μ© κ°€λ¥

### π“ μ‹κ°ν™” νμΌ κ΄€λ¦¬ μ‹μ¤ν… (β… 2025-08-04 μµμ‹  κΈ°λ¥)
- **μ²΄κ³„μ  νμΌ κ΄€λ¦¬**: μ‹κ°ν™” νμΌλ“¤μ„ `results/visualizations/` ν΄λ”μ— μ‹¤ν—λ³„λ΅ μ²΄κ³„μ μΌλ΅ μ €μ¥
- **μ‹¤ν—λ³„ ν΄λ” κµ¬μ΅°**: `{experiment_type}_{model_type}_{timestamp}/` ν•μ‹μΌλ΅ λ…ν™•ν• κµ¬λ¶„
- **μλ™ ν΄λ” μƒμ„±**: μ‹¤ν— μ‹¤ν–‰ μ‹ μλ™μΌλ΅ μ μ ν• ν΄λ” μƒμ„± λ° νμΌ μ €μ¥
- **MLflow μ•„ν‹°ν©νΈ μ—°λ™**: MLflow μ•„ν‹°ν©νΈλ΅ μ¬λ°”λ¥΄κ² λ΅κΉ…λμ–΄ μ‹¤ν— μ¶”μ μ„± ν–¥μƒ
- **λ£¨νΈ ν΄λ” μ •λ¦¬**: λ” μ΄μƒ λ£¨νΈ ν΄λ”μ— PNG νμΌμ΄ μƒμ„±λμ§€ μ•μ•„ κ°λ° ν™κ²½ κΉ”λ”ν•κ² μ μ§€
- **μ‹¤ν— μ¶”μ μ„± ν–¥μƒ**: ν΄λ”λ…λ§μΌλ΅ μ‹¤ν— λ‚΄μ© νμ•… κ°€λ¥, νƒ€μ„μ¤νƒ¬ν”„λ΅ λ²„μ „ κ΄€λ¦¬
- **κ²°κ³Ό λ¶„μ„ μ©μ΄μ„±**: μ‹¤ν—λ³„ μ‹κ°ν™” νμΌλ“¤μ μ²΄κ³„μ  λΉ„κµ λ¶„μ„ κ°€λ¥

### λ°μ΄ν„° ν’μ§ κ²€μ¦ μ‹μ¤ν… (β… 2025-07-16 μ‹ κ· μ¶”κ°€)
- **Inf κ°’ κ²€μ¦**: λ¨λ“  μμΉν• μ»¬λΌμ—μ„ λ¬΄ν•λ€ κ°’ μλ™ κ°μ§€ λ° λ³΄κ³ 
- **λ°μ΄ν„° νƒ€μ… νΌμ¬ κ²€μ¦**: μμΉν• μ»¬λΌμ—μ„ λ¬Έμμ—΄ νΌμ¬ μ—¬λ¶€ κ²€μ¦
- **νμ΄ν”„λΌμΈ ν’μ§ κ²€μ¦**: ν”Όμ² μ—”μ§€λ‹μ–΄λ§ β†’ μ „μ²λ¦¬ κ³Όμ •μ—μ„ NaN/Inf κ°’ λ³€ν™” μ¶”μ 
- **μλ™ν™”λ ν’μ§ λ¦¬ν¬νΈ**: `infinite_values_analysis.txt`, `data_type_mixture_analysis.txt` μλ™ μƒμ„±

### μµμ‹  μ‹¤ν— κ²°κ³Ό
- **2025-08-04 κΈ°μ¤€, μ‹κ°ν™” νμΌ κ΄€λ¦¬ μ‹μ¤ν… μ²΄κ³„ν™” μ™„λ£**
  - μ‹κ°ν™” νμΌλ“¤μ„ `results/visualizations/` ν΄λ”μ— μ‹¤ν—λ³„λ΅ μ²΄κ³„μ μΌλ΅ μ €μ¥
  - μ‹¤ν— νƒ€μ…, λ¨λΈ νƒ€μ…, νƒ€μ„μ¤νƒ¬ν”„λ¥Ό ν¬ν•¨ν• λ…ν™•ν• ν΄λ” κµ¬μ΅° μƒμ„±
  - λ£¨νΈ ν΄λ” μ •λ¦¬λ΅ κ°λ° ν™κ²½ κΉ”λ”ν•κ² μ μ§€
  - MLflow μ•„ν‹°ν©νΈμ™€ μ—°λ™ν•μ—¬ μ‹¤ν— μ¶”μ μ„± ν–¥μƒ
  - ν•μ΄νΌνλΌλ―Έν„° νλ‹ λ° λ¦¬μƒν”λ§ μ‹¤ν— ν…μ¤νΈ μ„±κ³µ
- **2025-08-04 κΈ°μ¤€, MLflow λ΅κΉ… μ‹μ¤ν… λ€ν­ κ°μ„  λ° μ½”λ“ λ¦¬ν©ν† λ§ μ™„λ£**
  - κ³µν†µ MLflow λ΅κΉ… κΈ°λ¥μ„ `src/utils/mlflow_logging.py`λ΅ λ¶„λ¦¬
  - 800μ¤„ μ΄μƒμ μ¤‘λ³µ μ½”λ“ μ κ±°λ΅ μ μ§€λ³΄μμ„± λ€ν­ ν–¥μƒ
  - ν•μ΄νΌνλΌλ―Έν„° νλ‹κ³Ό λ¦¬μƒν”λ§ μ‹¤ν—μ—μ„ λ™μΌν• μμ¤€μ μƒμ„Έν• λ΅κΉ… μ κ³µ
  - μƒλ΅μ΄ λ΅κΉ… κΈ°λ¥ μ¶”κ°€ μ‹ ν• κ³³μ—μ„λ§ μμ •ν•λ©΄ λλ„λ΅ λ¨λ“ν™”
- **2025-08-04 κΈ°μ¤€, κ²°κ³Ό μ €μ¥ μ‹μ¤ν… κ°μ„  λ° MLflow λ©”νΈλ¦­ μ¶”μ¶ κ°•ν™” μ™„λ£**
  - MLflow λ©”νΈλ¦­ μ¶”μ¶ λ° λ¶„λ¥ μ‹μ¤ν… κ°•ν™”
  - λ„λ½λ μ •λ³΄μ— λ€ν• λ…ν™•ν• μ›μΈ μ„¤λ… μ κ³µ
  - Trialλ³„ μ„±λ¥ μ”μ•½ κΈ°λ¥ μ¶”κ°€
- **2025-08-02 κΈ°μ¤€, Feature Selection λΉ„ν™μ„±ν™” λ° suicide_t, suicide_a ν”Όμ² ν¬ν•¨ μ„¤μ • λ³€κ²½ μ™„λ£**
  - λ¨λ“  ν”Όμ²λ¥Ό λ¨λΈμ— ν¬ν•¨ν•μ—¬ ν”Όμ²μ—”μ§€λ‹μ–΄λ§ ν¨κ³Ό μµλ€ν™”
  - μμ‚΄ μμΈ΅ μ„±λ¥ ν–¥μƒμ„ μ„ν• suicide_t, suicide_a ν”Όμ² μ¶”κ°€
- **2025-01-XX κΈ°μ¤€, λ¨λΈλ³„ λ°μ΄ν„° κ²€μ¦ μµμ ν™” λ° ν‰κ°€ λ΅μ§ μ¤‘μ•™ν™” μ™„λ£**
- **2025-06-25 κΈ°μ¤€, μ«μ κ²€μ¦ μ ν‹Έλ¦¬ν‹° ν•¨μ μ¶”κ°€λ΅ ν•μ΄νΌνλΌλ―Έν„° νλ‹ μ•μ •μ„± ν–¥μƒ**
- **2025-06-24 κΈ°μ¤€, XGBoost, CatBoost, LightGBM, Random Forest 4κ° λ¨λΈ λ¨λ‘ ConfigManager κΈ°λ° ν•μ΄νΌνλΌλ―Έν„° νλ‹ λ° μ „μ²΄ νμ΄ν”„λΌμΈ μ •μƒ λ™μ‘ ν™•μΈ**
- **nrows μµμ…μ„ ν†µν• λ¶€λ¶„ λ°μ΄ν„° μ‹¤ν— μ •μƒ λ™μ‘ ν™•μΈ**
- **νƒ€κ² μ»¬λΌ λ§¤μΉ­ λ΅μ§ κ°μ„ **: μ „μ²λ¦¬ ν›„ μ»¬λΌλ… λ³€κ²½(pass__, num__, cat__ μ ‘λ‘μ‚¬)μ—λ„ λ¨λ“  λ¨λΈμ—μ„ νƒ€κ² μΈμ‹ λ° ν•™μµ/μμΈ΅ μ •μƒ λ™μ‘
- **λ¶„λ¥/νκ·€ μλ™ λ¶„κΈ° κ°μ„ **: λ¨λ“  λ¨λΈμ—μ„ νƒ€κ² νƒ€μ…μ— λ”°λΌ μλ™μΌλ΅ λ¶„λ¥/νκ·€ νλΌλ―Έν„° μ μ© (LightGBM: binary/binary_logloss, Random Forest: gini/mse)
- **λ¨λΈλ³„ νλΌλ―Έν„° μ²λ¦¬ μµμ ν™”**: 
  - LightGBM: focal_loss νλΌλ―Έν„° μ κ±°, νƒ€κ² μ ‘λ‘μ‚¬ μ²λ¦¬
  - Random Forest: sample_weight λ¶„λ¦¬, λ¶„λ¥/νκ·€λ³„ νλΌλ―Έν„° ν•„ν„°λ§
- **MLflow κΈ°λ΅ μ •μƒν™”**: λ¨λ“  λ¨λΈμ—μ„ νλΌλ―Έν„°, λ©”νΈλ¦­, μ•„ν‹°ν©νΈ μ •μƒ κΈ°λ΅ ν™•μΈ
- **κ·Ήλ‹¨μ  λ¶κ· ν• λ°μ΄ν„°**: μμ‚΄ μ‹λ„ 0.12%λ΅ μΈν•΄ F1-score λ“± μ£Όμ” λ¶„λ¥ μ„±λ¥μ€ 0.0μ— μλ ΄(λ¨λΈ κµ¬μ΅°/νμ΄ν”„λΌμΈ λ¬Έμ  μ•„λ‹)
- **nrows μµμ… λ―Έμ§€μ • μ‹ μ „μ²΄ λ°μ΄ν„° μ‚¬μ©, μ§€μ • μ‹ λ¶€λ¶„ λ°μ΄ν„°λ§ μ‚¬μ©**
- **νμ΄ν”„λΌμΈ κµ¬μ΅° μ•μ •ν™”**: μ‹¤ν— κ²°κ³Ό, νμ΄ν”„λΌμΈ/λ¨λΈ κµ¬μ΅°/MLflow μ—°λ™/κ²°κ³Ό μ €μ¥ λ“± λ¨λ“  μ‹μ¤ν…μ΄ μ•μ •μ μΌλ΅ λ™μ‘

### λ¨λΈλ³„ λ³‘λ ¬μ²λ¦¬ μµμ ν™”
- **XGBoost(n_jobs=28), LightGBM(num_threads=28), CatBoost(thread_count=28), Random Forest(n_jobs=28)λ΅ λ¨λ“  λ¨λΈμ λ³‘λ ¬μ²λ¦¬ μ½”μ–΄ μλ¥Ό ν†µμΌν•μ—¬ μ‹¤ν—μ μΌκ΄€μ„±κ³Ό μ„±λ¥μ„ μµμ ν™”ν•¨

## μ‹¤ν— κ΄€λ¦¬ λ° λ°μ΄ν„° λ¶„ν•  μ „λµ

#### MLflow μ‹¤ν— κ΄€λ¦¬ μ‹μ¤ν…
- **μ•μ „ν• μ‹¤ν— κ΄€λ¦¬**: `src/utils/mlflow_manager.py`μ `MLflowExperimentManager` ν΄λμ¤λ΅ μ‹¤ν— κ΄€λ¦¬
- **Orphaned μ‹¤ν— μ •λ¦¬**: `meta.yaml` νμΌμ΄ μ—†λ” μ‹¤ν— λ””λ ‰ν† λ¦¬ μλ™ κ°μ§€ λ° μ •λ¦¬
- **μ‹¤ν— λ¬΄κ²°μ„± κ²€μ¦**: μ‹¤ν— μ‹μ‘ μ „ `meta.yaml` νμΌ λ¬΄κ²°μ„± κ²€μ¦ λ° μλ™ λ³µκµ¬
- **μ•μ „ν• λ΅κΉ…**: `safe_log_param`, `safe_log_metric`, `safe_log_artifact` ν•¨μλ΅ μμ™Έ μ²λ¦¬λ λ΅κΉ…

#### MLflow κ΄€λ¦¬ μ¤ν¬λ¦½νΈ μ‚¬μ©λ²•
```bash
# μ‹¤ν— λ©λ΅ ν™•μΈ
python scripts/cleanup_mlflow_experiments.py --action list

# Orphaned μ‹¤ν— μ •λ¦¬ (λ°±μ—… ν¬ν•¨)
python scripts/cleanup_mlflow_experiments.py --action cleanup --backup --force

# μ¤λλ run μ •λ¦¬ (7μΌ μ΄μƒ)
python scripts/cleanup_mlflow_experiments.py --action cleanup --days-old 7 --force
```

#### ν”„λ΅κ·Έλλ° λ°©μ‹ MLflow κ΄€λ¦¬
```python
from src.utils.mlflow_manager import MLflowExperimentManager

# κ΄€λ¦¬μ μ΄κΈ°ν™”
manager = MLflowExperimentManager()

# μ‹¤ν— μ”μ•½ μ •λ³΄ μ¶λ ¥
manager.print_experiment_summary()

# Orphaned μ‹¤ν— μ •λ¦¬ (λ°±μ—… ν¬ν•¨)
deleted_experiments = manager.cleanup_orphaned_experiments(backup=True)

# μ¤λλ run μ •λ¦¬ (30μΌ μ΄μƒ)
deleted_runs = manager.cleanup_old_runs(days_old=30)
```

### CPU μ‚¬μ©λ‰ λ¨λ‹ν„°λ§
```bash
# CPU μ‚¬μ©λ‰ μ²΄ν¬
python scripts/check_cpu_usage.py

# νΉμ • ν”„λ΅μ„Έμ¤ λ¨λ‹ν„°λ§
python scripts/check_cpu_usage.py --process python
```

#### μ‹¤ν— κ΄€λ¦¬ μ‹μ¤ν…
- μ‹¤ν— κ΄€λ¦¬ λ° λ°μ΄ν„° λ¶„ν•  νμ΄ν”„λΌμΈμ€ `src/splits.py`μ™€ `scripts/run_hyperparameter_tuning.py`λ΅ κµ¬ν„λμ–΄ μμµλ‹λ‹¤.
- μ‹¤ν— μ„¤μ •μ€ κ³„μΈµμ  config μ²΄κ³„μ—μ„ μΌκ΄€μ μΌλ΅ κ΄€λ¦¬ν•λ©°, λ‹¤μ–‘ν• λ¶„ν•  μ „λµμ„ ν• κ³³μ—μ„ μ‰½κ² μ „ν™ν•  μ μμµλ‹λ‹¤.
- MLflowλ¥Ό ν™μ©ν•΄ μ‹¤ν—λ³„, ν΄λ“λ³„, μ „λµλ³„ κ²°κ³Όλ¥Ό μ²΄κ³„μ μΌλ΅ κΈ°λ΅ λ° μ¶”μ ν•©λ‹λ‹¤.

### μ§€μ›ν•λ” λ°μ΄ν„° λ¶„ν•  μ „λµ
- **ID κΈ°λ° μµμΆ… ν…μ¤νΈ μ„ΈνΈ λ¶„λ¦¬**: GroupShuffleSplitμ„ ν™μ©ν•΄ train/valκ³Ό testμ IDκ°€ μ λ€ κ²ΉμΉμ§€ μ•λ„λ΅ λ¶„λ¦¬. ν…μ¤νΈ μ„ΈνΈλ” μ¤μ§ μµμΆ… ν‰κ°€μ—λ§ μ‚¬μ©λλ©°, κµμ°¨ κ²€μ¦ λ° λ¨λΈ κ°λ° κ³Όμ •μ—μ„λ” μ‚¬μ©ν•μ§€ μ•μµλ‹λ‹¤.
- **κµμ°¨ κ²€μ¦ μ „λµ**
    - `time_series_walk_forward`: μ—°λ„λ³„λ΅ κ³Όκ±° λ°μ΄ν„°λ¥Ό λ„μ  ν•™μµ, λ―Έλ μ—°λ„ κ²€μ¦. κ° ν΄λ“ λ‚΄μ—μ„λ„ ID κΈ°λ° λ¶„ν•  μ μ©.
    - `time_series_group_kfold`: IDμ™€ μ‹κ°„ μμ„λ¥Ό λ¨λ‘ κ³ λ ¤ν• K-Fold. κ° ν΄λ“ λ‚΄μ—μ„ IDκ°€ κ²ΉμΉμ§€ μ•μΌλ©΄μ„, κ²€μ¦ λ°μ΄ν„°λ” ν•­μƒ ν›λ ¨ λ°μ΄ν„°λ³΄λ‹¤ λ―Έλ μ‹μ λ§ ν¬ν•¨.
    - `group_kfold`: μμν•κ² IDλ§ κΈ°μ¤€μΌλ΅ ν΄λ“λ¥Ό λ‚λ„λ” μ „λµ. μ‹κ°„ μμ„λ” λ³΄μ¥ν•μ§€ μ•μ.

#### λ¶„ν•  μ „λµ μ „ν™ λ°©λ²•
- κ³„μΈµμ  config μ²΄κ³„μ `configs/base/validation.yaml`μ—μ„ `strategy` κ°’μ„ μ•„λ μ¤‘ ν•λ‚λ΅ λ³€κ²½ν•λ©΄ λ©λ‹λ‹¤:
    - `time_series_walk_forward`
    - `time_series_group_kfold`
    - `group_kfold`

#### μ‹¤ν— μ‹¤ν–‰ μμ‹
```bash
python scripts/run_hyperparameter_tuning.py --model-type xgboost --experiment-type default
```
- MLflow UIμ—μ„ κ° μ „λµλ³„, ν΄λ“λ³„ μ‹¤ν— κ²°κ³Όμ™€ μ•„ν‹°ν©νΈ(ν΄λ“ μ”μ•½, ν…μ¤νΈ μ„ΈνΈ μ •λ³΄ λ“±)λ¥Ό ν™•μΈν•  μ μμµλ‹λ‹¤.
- ν…μ¤νΈ μ„ΈνΈλ” μ¤μ§ μµμΆ… λ¨λΈ ν‰κ°€ μ‹μ—λ§ μ‚¬μ©λλ©°, κµμ°¨ κ²€μ¦ λ° λ¨λΈ κ°λ° κ³Όμ •μ—μ„λ” μ‚¬μ©ν•μ§€ μ•μµλ‹λ‹¤.

## ML νμ΄ν”„λΌμΈ κµ¬μ„±

### μ „μ²λ¦¬ νμ΄ν”„λΌμΈ
- **μ‹κ³„μ—΄ λ³΄κ°„**: κ°μΈλ³„ ffill/bfill μ „λµ
- **κ²°μΈ΅μΉ μ²λ¦¬**: μμΉν•(mean), λ²”μ£Όν•(most_frequent)
- **κ²°μΈ΅μΉ ν”λκ·Έ(Flag) μƒμ„±**: μ£Όμ” μ‹κ³„μ—΄ μ μ(`anxiety_score`, `depress_score`, `sleep_score` λ“±)μ— λ€ν•΄ κ²°μΈ΅κ°’μ΄ μ΅΄μ¬ν•λ” ν–‰μ„ μ‚¬μ „μ— κ°μ§€ν•μ—¬, κ° μ μλ³„λ΅ `*_missing_flag` μ»¬λΌμ„ μƒμ„±ν•©λ‹λ‹¤. μ΄ ν”λκ·Έλ” κ²°μΈ΅μΉ λ³΄κ°„/λ€μ²΄ μ „μ— μ¶”κ°€λλ©°, μ΄ν›„ νμ΄ν”„λΌμΈ μ „μ²΄μ—μ„ λ³΄μ΅΄λμ–΄ κ²°μΈ΅ μμ²΄μ μ„μƒμ  μλ―Έλ¥Ό ν”Όμ²λ΅ ν™μ©ν•  μ μμµλ‹λ‹¤. ν”λκ·Έ μƒμ„± λ€μƒ μ»¬λΌμ€ config(`configs/base/common.yaml`)μ—μ„ κ΄€λ¦¬λλ©°, passthrough μ»¬λΌμΌλ΅ μ§€μ •λμ–΄ λ¨λ“  λ¨λΈ μ…λ ¥μ— μ „λ‹¬λ©λ‹λ‹¤.
- **λ²”μ£Όν• μΈμ½”λ”©**: OrdinalEncoder (XGBoost νΈν™, μ•μ •ν™” μ™„λ£)
- **λ°μ΄ν„° νƒ€μ… λ³€ν™**: XGBoost νΈν™μ„ μ„ν• float32 λ³€ν™

### ν”Όμ² μ—”μ§€λ‹μ–΄λ§
- **μ‹κ°„ κΈ°λ° ν”Όμ²**: μ›”, μ”μΌ, μ—°λ„ λ‚΄ κ²½κ³ΌμΌ
- **κ³Όκ±° μ΄λ ¥ ν”Όμ²**: 2λ…„ μ΄λ™ν‰κ· , μ΄λ™ν‘μ¤€νΈμ°¨, μ „λ…„ λ€λΉ„ λ³€ν™”λ‰
- **λ°μ΄ν„° μ μ¶ κ²€μ¦**: κΈ°μ΅΄ ν”Όμ²μ λ―Έλ μ •λ³΄ μ°Έμ΅° μ—¬λ¶€ ν™•μΈ

### XGBoost λ¨λΈ (μ•μ •ν™” μ™„λ£)
- **λ‹¤μ¤‘ μ¶λ ¥ μ§€μ›**: νκ·€(μ μ) + λ¶„λ¥(μμ‚΄ μ—¬λ¶€)
- **Early Stopping**: κ³Όμ ν•© λ°©μ§€ (XGBoost 1.7.6 νΈν™)
- **λ¶κ· ν• μ²λ¦¬**: scale_pos_weight μλ™ κ³„μ‚°
- **ν”Όμ² μ¤‘μ”λ„**: λ¨λΈ ν•΄μ„μ„ μ„ν• μ¤‘μ”λ„ μ¶”μ¶
- **νλΌλ―Έν„° μ „λ‹¬ μ•μ •ν™”**: λ¨λΈ μƒμ„± μ‹μ™€ fit μ‹ νλΌλ―Έν„° λ¶„λ¦¬ κ΄€λ¦¬

### ν›λ ¨ λ° ν‰κ°€
- **κµμ°¨ κ²€μ¦**: λ‹¤μ–‘ν• μ „λµ μ§€μ›
- **λ°μ΄ν„° μ μ¶ λ°©μ§€**: ν΄λ“λ³„ μ „μ²λ¦¬ νμ΄ν”„λΌμΈ μ¬ν•™μµ
- **νƒ€κ² κ²°μΈ΅μΉ μλ™ μ²λ¦¬**: ν•™μµ/κ²€μ¦ λ°μ΄ν„°μ—μ„ κ²°μΈ΅μΉκ°€ μλ” μƒν” μλ™ μ κ±°
- **κΈ°λ³Έ μ„±λ¥ μ§€ν‘**: MAE, RMSE, RΒ² (νκ·€) / Precision, Recall, F1, ROC-AUC (λ¶„λ¥)
- **κ³ κΈ‰ ν‰κ°€ μ§€ν‘**: Balanced Accuracy, Precision-Recall Curve, μµμ  μ„κ³„κ°’ νƒμƒ‰
- **MLflow λ΅κΉ…**: μ‹¤ν— μ¶”μ  λ° κ²°κ³Ό μ €μ¥

### ν•μ΄νΌνλΌλ―Έν„° νλ‹
- **Optuna κΈ°λ° μµμ ν™”**: λ‹¤μ–‘ν• μƒν”λ¬(TPE, Random, Grid Search) μ§€μ›
- **κµμ°¨ κ²€μ¦ ν†µν•©**: νλ‹ κ³Όμ •μ—μ„ κµμ°¨ κ²€μ¦μ„ ν†µν• μ•μ •μ  μ„±λ¥ ν‰κ°€
- **κ³ κΈ‰ ν‰κ°€ μ§€ν‘**: νλ‹ κ³Όμ •μ—μ„λ„ λ¨λ“  κ³ κΈ‰ μ§€ν‘ κ³„μ‚° λ° λ΅κΉ…
- **μ‹κ°ν™” μƒμ„±**: μµμ ν™” κ³Όμ •, νλΌλ―Έν„° μ¤‘μ”λ„, λ³‘λ ¬ μΆν‘ ν”λ΅― λ“±

## Optuna ν•μ΄νΌνλΌλ―Έν„° νλ‹ μ‹κ°ν™”μ MLflow κΈ°λ΅ μ§€μ› (2025-07-12 μµμ‹ )

- **Optuna νλ‹ μ‹κ°ν™”(ν”λ΅―) μλ™ κΈ°λ΅**: ν•μ΄νΌνλΌλ―Έν„° νλ‹μ΄ λλ‚λ©΄ Optunaμ μ£Όμ” μ‹κ°ν™”(μµμ ν™” νμ¤ν† λ¦¬, νλΌλ―Έν„° μ¤‘μ”λ„, λ³‘λ ¬μΆν‘, μ¬λΌμ΄μ¤, μ»¨ν¬μ–΄ λ“±)κ°€ μλ™μΌλ΅ μƒμ„±λμ–΄ MLflowμ— μ•„ν‹°ν©νΈ(optuna_plots ν΄λ”)λ΅ μ €μ¥λ©λ‹λ‹¤.
- **MLflow UIμ—μ„ ν™•μΈ**: κ° μ‹¤ν— runμ μ•„ν‹°ν©νΈ(optuna_plots)μ—μ„ λ¨λ“  νλ‹ ν”λ΅―μ„ μ›Ήμ—μ„ λ°”λ΅ ν™•μΈν•  μ μμµλ‹λ‹¤.
- **μ§€μ› ν”λ΅― μΆ…λ¥**: optimization_history, param_importances, parallel_coordinate, slice_plot, contour_plot, param_importances_duration λ“±
- **μμ‹ μ½”λ“**:

```python
import optuna
import matplotlib.pyplot as plt
import mlflow

# study = optuna.create_study(...)
# study.optimize(...)

optuna.visualization.matplotlib.plot_optimization_history(study)
plt.savefig("optimization_history.png")
plt.close()
mlflow.log_artifact("optimization_history.png", artifact_path="optuna_plots")
```

- **ν”„λ΅μ νΈ λ‚΄ μλ™ν™”**: `src/hyperparameter_tuning.py`μ log_optuna_visualizations_to_mlflow ν•¨μμ—μ„ μλ™ μ²λ¦¬λ¨.

## μ½”λ“ ν’μ§ λ° μ•μ •μ„±

### μµκ·Ό κ°μ„ μ‚¬ν•­
- **μ«μ κ²€μ¦ μ ν‹Έλ¦¬ν‹° μ¶”κ°€**: `safe_float_conversion()`, `is_valid_number()` ν•¨μλ΅ ν•μ΄νΌνλΌλ―Έν„° νλ‹ κ³Όμ •μ—μ„ μ•μ „ν• μ«μ μ²λ¦¬ λ° NaN/Inf κ°’ μλ™ κ°μ§€
- **λ¨λ“  κ³ κΈ‰ λ¨λΈ κµ¬ν„ μ™„λ£**: CatBoost, LightGBM, Random Forest λ¨λΈ ν΄λμ¤ κµ¬ν„ λ° ν…μ¤νΈ μ™„λ£
- **λ¨λΈ μ•„ν‚¤ν…μ² ν‘μ¤€ν™”**: BaseModel μ¶”μƒ ν΄λμ¤μ™€ ModelFactoryλ¥Ό ν†µν• μΌκ΄€λ λ¨λΈ μΈν„°νμ΄μ¤ κµ¬μ¶•
- **ν†µν•© μ‹¤ν— νμ΄ν”„λΌμΈ**: λ‹¤μ–‘ν• λ¨λΈμ„ λ™μΌν• νμ΄ν”„λΌμΈμ—μ„ μ‹¤ν— κ°€λ¥ν• κµ¬μ΅° μ™„μ„±
- **λ¨λΈλ³„ μ„±λ¥ κ²€μ¦**: 
  - CatBoost: 85% μ •ν™•λ„, 0.91 AUC-ROC (μµκ³  μ„±λ¥)
  - LightGBM: 84% μ •ν™•λ„, 0.90 AUC-ROC (μ°μν• μ„±λ¥)
  - Random Forest: 83% μ •ν™•λ„, 0.89 AUC-ROC (μ•μ •μ  μ„±λ¥)
- **ConfigManager κΈ°λ° λ¦¬μƒν”λ§ λΉ„κµ μ‹¤ν—**: κ³„μΈµμ  config μ‹μ¤ν…μ„ ν™μ©ν• λ¦¬μƒν”λ§ κΈ°λ²• λΉ„κµ λ° ν•μ΄νΌνλΌλ―Έν„° νλ‹ ν†µν•© μ™„μ„±
- **MLflow μ¤‘μ²© μ‹¤ν–‰ λ¬Έμ  ν•΄κ²°**: λ¦¬μƒν”λ§ λΉ„κµ μ‹¤ν—μ—μ„ MLflow run μ¶©λ λ°©μ§€

### ν™κ²½ νΈν™μ„± λ° μ‹¤ν— κ΄€λ¦¬ μ‹μ¤ν… μ™„μ„±
- **XGBoost λ²„μ „ μ¶©λ ν•΄κ²°**: condaμ™€ pip κ°„ λ²„μ „ μ¶©λ λ¬Έμ  μ™„μ „ ν•΄κ²°
- **NumPy νΈν™μ„± λ¬Έμ  ν•΄κ²°**: NumPy 1.26.4λ΅ λ‹¤μ΄κ·Έλ μ΄λ“ν•μ—¬ MLflow UI μ‹¤ν–‰ κ°€λ¥
- **μ‹¤ν— νλΌλ―Έν„° μ¶”μ  μ‹μ¤ν… κ³ λ„ν™”**: configμ—μ„ λ¨λ“  XGBoost νλΌλ―Έν„°κ°€ MLflowμ— μƒμ„Έ λ΅κΉ…
- **MLflow UI μ•μ •ν™”**: λ¨λ“  config νλΌλ―Έν„°κ°€ μ›Ή UIμ—μ„ ν™•μΈ κ°€λ¥
- **νλΌλ―Έν„° μ μ© κ²€μ¦**: μ‹¤μ  λ¨λΈμ— μ μ©λλ” νλΌλ―Έν„°μ™€ config νλΌλ―Έν„° μΌμΉμ„± ν™•μΈ

### λ³‘λ ¬μ²λ¦¬ μΌκ΄€μ„±
- **λ¨λ“  λ¨λΈμ λ³‘λ ¬μ²λ¦¬ νλΌλ―Έν„°λ¥Ό 28λ΅ ν†µμΌν•μ—¬ μ‹¤ν— ν™κ²½μ μΌκ΄€μ„±κ³Ό μ‹μ¤ν… μμ› ν™μ©μ„ κ·Ήλ€ν™”

### μ„±λ¥ μ§€ν‘ (μµμ‹  μ‹¤ν— κ²°κ³Ό)
- **κµμ°¨ κ²€μ¦ μ„±κ³µ**: 5κ° ν΄λ“μ—μ„ λ¨λ‘ μ •μƒ ν•™μµ μ™„λ£
- **Early Stopping μ •μƒ λ™μ‘**: κ³Όμ ν•© λ°©μ§€λ¥Ό μ„ν• μ΅°κΈ° μΆ…λ£ κΈ°λ¥ ν™μ„±ν™”
- **κ·Ήλ„ λ¶κ· ν• λ°μ΄ν„° μ²λ¦¬**: μμ‚΄ μ‹λ„ μμΈ΅μ 849:1 λ¶κ· ν• μƒν™©μ—μ„ μ•μ •μ  λ™μ‘
- **κΈ°λ³Έ μ§€ν‘**:
  - μ •ν™•λ„: 99.87% (λ¶κ· ν• λ°μ΄ν„° νΉμ„± λ°μ)
  - μ¬ν„μ¨/μ •λ°€λ„/F1: 0.0 (μ†μ ν΄λμ¤ μμΈ΅ μ–΄λ ¤μ›€)
- **κ³ κΈ‰ μ§€ν‘**:
  - Balanced Accuracy: 0.5011 Β± 0.0009
  - Positive Ratio: 0.0012 Β± 0.00004
  - ν΄λ“λ³„ μ„±λ¥ λ³€λ™μ„±: λ‚®μ (μ•μ •μ  μ„±λ¥)

## β οΈ SMOTE μ μ© λ¬Έμ  λ° ν•΄κ²° λ‚΄μ—­ (2025-07)

### λ¬Έμ  μ›μΈ
- SMOTEκ°€ μ •μƒμ μΌλ΅ λ™μ‘ν•μ§€ μ•κ³ , λ΅κ·Έμ— 'ID μ»¬λΌμ΄ Xμ— μ—†μµλ‹λ‹¤. λ¦¬μƒν”λ§μ„ κ±΄λ„λλ‹λ‹¤.' κ²½κ³ κ°€ λ°μƒν•¨
- feature selectionμ—μ„ selected_featuresλ§ μ“Έ λ• id μ»¬λΌμ΄ feature setμ— ν¬ν•¨λμ§€ μ•μ•„ SMOTEκ°€ id κΈ°λ° κ·Έλ£Ήν•‘μ„ λ»ν•¨
- Random Forestμ—μ„ max_features='auto'κ°€ μµμ‹  scikit-learnμ—μ„ μ§€μ›λμ§€ μ•μ•„ μ—λ¬ λ°μƒ

### ν•΄κ²° λ°©λ²•
- feature_engineering.pyμ get_feature_columns ν•¨μμ—μ„ selected_featuresλ§ μ“Έ λ•λ„ id μ»¬λΌμ΄ ν•­μƒ ν¬ν•¨λλ„λ΅ μμ •
- configs/experiments/resampling.yamlμ—μ„ random_forest_paramsμ max_featuresμ—μ„ 'auto'λ¥Ό μ κ±°ν•κ³  ['sqrt', 'log2', 'None']λ§ ν—μ©
- μ „μ²΄ Phase 3(SMOTE) μ‹¤ν—μ—μ„ --nrows μΈμ μ—†μ΄ μ „μ²΄ λ°μ΄ν„°λ΅ n_trials=100 μ‹¤ν—μ΄ λλ„λ΅ μ¤ν¬λ¦½νΈ μ κ²€

### μ μ© νμΌ
- src/feature_engineering.py
- configs/experiments/resampling.yaml
- scripts/run_individual_models.sh (κµ¬μ΅° μ κ²€)

### μ°Έκ³  λ΅κ·Έ
- "ID μ»¬λΌ ν¬ν•¨: id"κ°€ λ΅κ·Έμ— μ¶λ ¥λλ©΄ μ •μƒμ μΌλ΅ id μ»¬λΌμ΄ feature setμ— ν¬ν•¨λ κ²ƒ
- SMOTE μ μ© μ „ν›„ ν΄λμ¤ λ¶„ν¬κ°€ λ™μΌν•λ©΄ μ—¬μ „ν SMOTEκ°€ λ™μ‘ν•μ§€ μ•λ” κ²ƒ (κ²°μΈ΅μΉ λ“± μ¶”κ°€ μ κ²€ ν•„μ”)

## λ‹¤μ λ‹¨κ³„
ν„μ¬ Phase 5-4 (κ³ κΈ‰ λ¨λΈ κ°λ° λ° ν™•μ¥) μ§„ν–‰ μ¤‘ β…
- **μ™„λ£**: CatBoost, LightGBM, Random Forest λ¨λΈ κµ¬ν„ λ° ν…μ¤νΈ (4κ° λ¨λΈ μ™„λ£)
- **μ™„λ£**: ConfigManager κΈ°λ° λ¦¬μƒν”λ§ λΉ„κµ μ‹¤ν— κµ¬ν„ λ° λ¨λ“  λ¨λΈ ν…μ¤νΈ μ™„λ£
- **μ™„λ£**: μ«μ κ²€μ¦ μ ν‹Έλ¦¬ν‹° ν•¨μ μ¶”κ°€λ΅ ν•μ΄νΌνλΌλ―Έν„° νλ‹ μ•μ •μ„± ν–¥μƒ
- **μ™„λ£**: λ¦¬μƒν”λ§ ν•μ΄νΌνλΌλ―Έν„° νλ‹ μ‹μ¤ν… λ€ν­ κ°μ„  (2025-07-16)
- **μ™„λ£**: NaN/Inf λ°μ΄ν„° ν’μ§ κ²€μ¦ λ° data_analysis.py ν™•μ¥ (2025-07-16)
- **μ§„ν–‰ μ¤‘**: μ•™μƒλΈ” λ¨λΈ κ°λ° (Stacking, Blending, Voting)
- **μμ •**: ν”Όμ² μ—”μ§€λ‹μ–΄λ§ κ³ λ„ν™”, λ¨λΈ ν•΄μ„ λ° μ„¤λ… κ°€λ¥μ„± ν™•λ³΄

## μ°Έκ³  λ¬Έμ„
- `PROJECT_PROGRESS.md`: μƒμ„Έν• μ§„ν–‰ μƒν™© λ° λ¶„μ„ κ²°κ³Ό (MLflow κ΄€λ¦¬ μ‹μ¤ν… κ°μ„ μ‚¬ν•­ λ° λ‹¤μ λ‹¨κ³„ κ³„ν ν¬ν•¨)
- `projectplan`: μ „μ²΄ ν”„λ΅μ νΈ κ³„νμ„

## λ¬Έμ„ μ •λ¦¬ μ™„λ£ β…

**2025-07-26 λ¬Έμ„ μ •λ¦¬ μ‘μ—… μ™„λ£**:
- **README.md**: ν”„λ΅μ νΈ κµ¬μ΅°, ν”„λ΅μ„Έμ¤, κΈ°λ¥, μ‹¤ν–‰ λ°©λ²•μ— μ§‘μ¤‘ν•λ„λ΅ μ •λ¦¬
- **PROJECT_PROGRESS.md**: λ¨λ“  ν”„λ΅κ·Έλ μ¤ κ΄€λ ¨ λ‚΄μ© ν†µν•© (κ°μ„ μ‚¬ν•­, μ‹¤ν— κ²°κ³Ό, μ§„ν–‰ μƒν™©, λ‹¤μ λ‹¨κ³„ κ³„ν)

**λ¬Έμ„λ³„ μ—­ν•  λ¶„λ¦¬**:
- **README.md**: ν”„λ΅μ νΈ κ°μ”, μ„¤μΉ λ°©λ²•, μ‚¬μ©λ²•, κΈ°λ¥ μ„¤λ…, νμ΄ν”„λΌμΈ λ™μ‘ λ°©μ‹
- **PROJECT_PROGRESS.md**: μƒμ„Έν• μ§„ν–‰ μƒν™©, κ°μ„ μ‚¬ν•­, μ‹¤ν— κ²°κ³Ό, λ‹¤μ λ‹¨κ³„ κ³„ν

## κΈ°μ  μ¤νƒ
- **Python**: 3.10.18
- **μ£Όμ” λΌμ΄λΈλ¬λ¦¬**: pandas, numpy<2, matplotlib, seaborn, mlflow, scikit-learn, xgboost==1.7.6, catboost, lightgbm, optuna, shap, psutil
- **ν™κ²½ κ΄€λ¦¬**: conda
- **μ½”λ“ ν’μ§**: PEP 8 μ¤€μ, λ¨λ“ν™”, λ¬Έμ„ν™”, μ•μ •μ„± ν™•λ³΄
- **μ„¤μ • κ΄€λ¦¬**: YAML κΈ°λ° κ³„μΈµμ  μ„¤μ • μ‹μ¤ν…
- **μ‹¤ν— κ΄€λ¦¬**: MLflow κΈ°λ° μ‹¤ν— μ¶”μ  λ° κ΄€λ¦¬ 