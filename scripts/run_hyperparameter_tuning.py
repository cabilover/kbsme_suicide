#!/usr/bin/env python3
"""
í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

Optunaë¥¼ í™œìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
MLflowì™€ ì—°ë™ë˜ì–´ ì‹¤í—˜ ì¶”ì ì´ ê°€ëŠ¥í•˜ë©°, ë‹¤ì–‘í•œ íŠœë‹ ì „ëµì„ ì§€ì›í•©ë‹ˆë‹¤.
ë¦¬ìƒ˜í”Œë§ ê¸°ë²• ë¹„êµ ì‹¤í—˜ ê¸°ëŠ¥ë„ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
"""

import argparse
import logging
import sys
from pathlib import Path
import mlflow
import yaml
import pandas as pd
import numpy as np
from typing import Dict, Any, List

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from src.hyperparameter_tuning import HyperparameterTuner
from src.splits import (
    load_config, 
    split_test_set, 
    validate_splits, 
    log_splits_info
)
from src.utils.config_manager import ConfigManager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def setup_mlflow_experiment(experiment_name: str):
    """
    MLflow ì‹¤í—˜ì„ ì„¤ì •í•©ë‹ˆë‹¤.
    
    Args:
        experiment_name: ì‹¤í—˜ ì´ë¦„
        
    Returns:
        experiment_id: ì‹¤í—˜ ID
    """
    mlflow.set_experiment(experiment_name)
    experiment = mlflow.get_experiment_by_name(experiment_name)
    if experiment is None:
        experiment_id = mlflow.create_experiment(experiment_name)
    else:
        experiment_id = experiment.experiment_id
    
    logger.info(f"MLflow ì‹¤í—˜ ì„¤ì •: {experiment_name} (ID: {experiment_id})")
    return experiment_id


def log_tuning_params(tuning_config: Dict[str, Any], base_config: Dict[str, Any]):
    """
    íŠœë‹ íŒŒë¼ë¯¸í„°ë¥¼ MLflowì— ë¡œê¹…í•©ë‹ˆë‹¤.
    
    Args:
        tuning_config: íŠœë‹ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        base_config: ê¸°ë³¸ ì„¤ì • ë”•ì…”ë„ˆë¦¬
    """
    # íŠœë‹ íŒŒë¼ë¯¸í„°
    tuning_params = {
        "n_trials": tuning_config['tuning']['n_trials'],
        "timeout": tuning_config['tuning'].get('timeout', None),
        "sampler": tuning_config['sampler']['type'],
        "optimization_direction": tuning_config['evaluation']['optimization_direction'],
        "primary_metric": tuning_config['evaluation']['primary_metric']
    }
    
    # XGBoost íŒŒë¼ë¯¸í„° ë²”ìœ„
    xgboost_params = tuning_config['xgboost_params']
    for param, param_config in xgboost_params.items():
        if 'range' in param_config:
            tuning_params[f"tune_{param}_min"] = param_config['range'][0]
            tuning_params[f"tune_{param}_max"] = param_config['range'][1]
        elif 'choices' in param_config:
            tuning_params[f"tune_{param}_choices"] = str(param_config['choices'])
    
    # ê¸°ë³¸ ì„¤ì •ì—ì„œ ë¦¬ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
    resampling_config = base_config.get('resampling', {})
    resampling_params = {
        "resampling_enabled": resampling_config.get('enabled', False),
        "resampling_method": resampling_config.get('method', 'none'),
        "resampling_random_state": resampling_config.get('random_state', 42)
    }
    
    # ëª¨ë“  íŒŒë¼ë¯¸í„° ë¡œê¹…
    all_params = {**tuning_params, **resampling_params}
    mlflow.log_params(all_params)
    logger.info("íŠœë‹ íŒŒë¼ë¯¸í„° ë¡œê¹… ì™„ë£Œ")


def run_resampling_tuning_comparison(tuning_config_path: str, base_config_path: str,
                                   data_path: str, resampling_methods: List[str],
                                   nrows: int = None) -> Dict[str, Any]:
    """
    ë‹¤ì–‘í•œ ë¦¬ìƒ˜í”Œë§ ê¸°ë²•ì— ëŒ€í•´ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ë¹„êµí•˜ëŠ” ì‹¤í—˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    
    Args:
        tuning_config_path: íŠœë‹ ì„¤ì • íŒŒì¼ ê²½ë¡œ
        base_config_path: ê¸°ë³¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ
        data_path: ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        resampling_methods: í…ŒìŠ¤íŠ¸í•  ë¦¬ìƒ˜í”Œë§ ê¸°ë²• ë¦¬ìŠ¤íŠ¸
        nrows: ì‚¬ìš©í•  ë°ì´í„° í–‰ ìˆ˜ (Noneì´ë©´ ì „ì²´ ì‚¬ìš©)
        
    Returns:
        ë¹„êµ ì‹¤í—˜ ê²°ê³¼
    """
    logger.info("=== ë¦¬ìƒ˜í”Œë§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë¹„êµ ì‹¤í—˜ ì‹œì‘ ===")
    
    # ê¸°ë³¸ ì„¤ì • ë¡œë“œ
    base_config = load_config(base_config_path)
    
    # ë°ì´í„° ë¡œë“œ
    logger.info(f"ë°ì´í„° ë¡œë“œ ì¤‘: {data_path}")
    if nrows:
        df = pd.read_csv(data_path, nrows=nrows)
        logger.info(f"í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë¡œë“œ: {len(df):,} í–‰")
    else:
        df = pd.read_csv(data_path)
        logger.info(f"ì „ì²´ ë°ì´í„° ë¡œë“œ: {len(df):,} í–‰")
    
    # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„ë¦¬
    logger.info("=== í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„ë¦¬ ===")
    train_val_df, test_df, test_ids = split_test_set(df, base_config)
    
    # ë¶„í•  ê²€ì¦
    if not validate_splits(train_val_df, test_df, base_config):
        logger.error("ë¶„í•  ê²€ì¦ ì‹¤íŒ¨!")
        return {}
    
    comparison_results = {
        'methods': {},
        'best_method': None,
        'best_score': -np.inf,
        'summary': {}
    }
    
    for method in resampling_methods:
        logger.info(f"\n--- {method.upper()} í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘ ---")
        
        # ì„¤ì • ë³µì‚¬ ë° ë¦¬ìƒ˜í”Œë§ ì„¤ì • ì—…ë°ì´íŠ¸
        method_base_config = base_config.copy()
        method_base_config['resampling'] = {
            'enabled': method != 'none',
            'method': method if method != 'none' else 'smote',  # noneì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì„¤ì •
            'random_state': 42
        }
        
        # ë¦¬ìƒ˜í”Œë§ ê¸°ë²•ë³„ íŒŒë¼ë¯¸í„° ì„¤ì •
        if method == 'smote':
            method_base_config['resampling']['smote_k_neighbors'] = 5
        elif method == 'borderline_smote':
            method_base_config['resampling']['borderline_smote_k_neighbors'] = 5
        elif method == 'adasyn':
            method_base_config['resampling']['adasyn_k_neighbors'] = 5
        elif method == 'under_sampling':
            method_base_config['resampling']['under_sampling_strategy'] = 'random'
        elif method == 'hybrid':
            method_base_config['resampling']['hybrid_strategy'] = 'smote_tomek'
        
        # MLflow ì¤‘ì²© ì‹¤í–‰
        with mlflow.start_run(run_name=f"resampling_tuning_{method}", nested=True):
            # íŠœë‹ ì„¤ì • ë¡œë“œ
            with open(tuning_config_path, 'r', encoding='utf-8') as f:
                method_tuning_config = yaml.safe_load(f)
            
            # ì‹¤í—˜ íŒŒë¼ë¯¸í„° ë¡œê¹…
            log_tuning_params(method_tuning_config, method_base_config)
            
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë„ˆ ìƒì„± (ì„ì‹œ ì„¤ì • íŒŒì¼ ì‚¬ìš©)
            import tempfile
            import os
            
            # ì„ì‹œ ê¸°ë³¸ ì„¤ì • íŒŒì¼ ìƒì„±
            with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
                yaml.dump(method_base_config, f, default_flow_style=False)
                temp_base_config_path = f.name
            
            try:
                # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë„ˆ ìƒì„±
                tuner = HyperparameterTuner(temp_base_config_path, temp_base_config_path)
                
                # ìµœì í™” ì‹¤í–‰
                logger.info(f"{method} í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹¤í–‰ ì¤‘...")
                best_params, best_score = tuner.optimize(start_mlflow_run=False)
                
                # ê²°ê³¼ ì €ì¥
                logger.info(f"{method} íŠœë‹ ê²°ê³¼ ì €ì¥ ì¤‘...")
                tuner.save_results()
                
                # ê²°ê³¼ ìˆ˜ì§‘
                method_results = {
                    'best_params': best_params,
                    'best_score': best_score,
                    'tuning_config': method_tuning_config,
                    'base_config': method_base_config
                }
                
                comparison_results['methods'][method] = method_results
                
                # ìµœê³  ì„±ëŠ¥ ì—…ë°ì´íŠ¸
                if best_score > comparison_results['best_score']:
                    comparison_results['best_score'] = best_score
                    comparison_results['best_method'] = method
                
                logger.info(f"{method} í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ: {best_score:.4f}")
                
            finally:
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                if os.path.exists(temp_base_config_path):
                    os.unlink(temp_base_config_path)
    
    # ë¹„êµ ê²°ê³¼ ìš”ì•½
    logger.info("\n=== ë¦¬ìƒ˜í”Œë§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë¹„êµ ê²°ê³¼ ìš”ì•½ ===")
    for method, results in comparison_results['methods'].items():
        best_score = results['best_score']
        logger.info(f"{method}: ìµœê³  ì„±ëŠ¥ = {best_score:.4f}")
    
    logger.info(f"ìµœê³  ì„±ëŠ¥ ê¸°ë²•: {comparison_results['best_method']} (ì„±ëŠ¥: {comparison_results['best_score']:.4f})")
    
    return comparison_results


def validate_config_files(tuning_config_path: str, base_config_path: str):
    """
    ì„¤ì • íŒŒì¼ë“¤ì˜ ìœ íš¨ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.
    
    Args:
        tuning_config_path: íŠœë‹ ì„¤ì • íŒŒì¼ ê²½ë¡œ
        base_config_path: ê¸°ë³¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ
    """
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not Path(tuning_config_path).exists():
        raise FileNotFoundError(f"íŠœë‹ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {tuning_config_path}")
    
    if not Path(base_config_path).exists():
        raise FileNotFoundError(f"ê¸°ë³¸ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {base_config_path}")
    
    # YAML íŒŒì‹± í…ŒìŠ¤íŠ¸
    try:
        with open(tuning_config_path, 'r', encoding='utf-8') as f:
            tuning_config = yaml.safe_load(f)
        logger.info("íŠœë‹ ì„¤ì • íŒŒì¼ íŒŒì‹± ì„±ê³µ")
    except Exception as e:
        raise ValueError(f"íŠœë‹ ì„¤ì • íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
    
    try:
        with open(base_config_path, 'r', encoding='utf-8') as f:
            base_config = yaml.safe_load(f)
        logger.info("ê¸°ë³¸ ì„¤ì • íŒŒì¼ íŒŒì‹± ì„±ê³µ")
    except Exception as e:
        raise ValueError(f"ê¸°ë³¸ ì„¤ì • íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
    
    # í•„ìˆ˜ ì„¤ì • í™•ì¸
    required_tuning_keys = ['tuning', 'sampler', 'xgboost_params', 'evaluation', 'results', 'mlflow']
    for key in required_tuning_keys:
        if key not in tuning_config:
            raise ValueError(f"íŠœë‹ ì„¤ì •ì— í•„ìˆ˜ í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤: {key}")
    
    logger.info("ì„¤ì • íŒŒì¼ ê²€ì¦ ì™„ë£Œ")


def run_resampling_tuning_comparison_with_configmanager(
    model_type: str,
    experiment_type: str,
    data_path: str,
    resampling_methods: List[str],
    nrows: int = None
) -> Dict[str, Any]:
    """
    ConfigManager ê¸°ë°˜ ë‹¤ì–‘í•œ ë¦¬ìƒ˜í”Œë§ ê¸°ë²•ì— ëŒ€í•´ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ë¹„êµí•˜ëŠ” ì‹¤í—˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    logger.info("=== [ConfigManager] ë¦¬ìƒ˜í”Œë§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë¹„êµ ì‹¤í—˜ ì‹œì‘ ===")
    config_manager = ConfigManager()

    # ë°ì´í„° ë¡œë“œ
    logger.info(f"ë°ì´í„° ë¡œë“œ ì¤‘: {data_path}")
    if nrows:
        df = pd.read_csv(data_path, nrows=nrows)
        logger.info(f"í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë¡œë“œ: {len(df):,} í–‰")
    else:
        df = pd.read_csv(data_path)
        logger.info(f"ì „ì²´ ë°ì´í„° ë¡œë“œ: {len(df):,} í–‰")

    # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„ë¦¬
    logger.info("=== í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„ë¦¬ ===")
    # configëŠ” ë¦¬ìƒ˜í”Œë§ê³¼ ë¬´ê´€í•˜ê²Œ ë¶„í• ì—ë§Œ ì‚¬ìš©
    base_config = config_manager.create_experiment_config(model_type, experiment_type)
    train_val_df, test_df, test_ids = split_test_set(df, base_config)

    # ë¶„í•  ê²€ì¦
    if not validate_splits(train_val_df, test_df, base_config):
        logger.error("ë¶„í•  ê²€ì¦ ì‹¤íŒ¨!")
        return {}

    comparison_results = {
        'methods': {},
        'best_method': None,
        'best_score': -np.inf,
        'summary': {}
    }

    # ìƒìœ„ MLflow run ì‹œì‘
    with mlflow.start_run(run_name=f"resampling_comparison_{model_type}"):
        for method in resampling_methods:
            logger.info(f"\n--- {method.upper()} í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘ ---")
            # config ìƒì„± ë° ë¦¬ìƒ˜í”Œë§ ì„¤ì • ìˆ˜ì •
            config = config_manager.create_experiment_config(model_type, experiment_type)
            config['resampling'] = {
                'enabled': method != 'none',
                'method': method if method != 'none' else 'smote',
                'random_state': 42
            }
            # ë¦¬ìƒ˜í”Œë§ ê¸°ë²•ë³„ íŒŒë¼ë¯¸í„° ì„¤ì •
            if method == 'smote':
                config['resampling']['smote_k_neighbors'] = 5
            elif method == 'borderline_smote':
                config['resampling']['borderline_smote_k_neighbors'] = 5
            elif method == 'adasyn':
                config['resampling']['adasyn_k_neighbors'] = 5
            elif method == 'under_sampling':
                config['resampling']['under_sampling_strategy'] = 'random'
            elif method == 'hybrid':
                config['resampling']['hybrid_strategy'] = 'smote_tomek'
            # ë°ì´í„° ê²½ë¡œ ë°˜ì˜
            config['data']['data_path'] = data_path
            # MLflow ì¤‘ì²© ì‹¤í–‰
            with mlflow.start_run(run_name=f"resampling_tuning_{method}", nested=True):
                # íŠœë‹ íŒŒë¼ë¯¸í„° ë¡œê¹… (tuning_configëŠ” configì—ì„œ ì¶”ì¶œ)
                log_tuning_params(config, config)
                # ì„ì‹œ config íŒŒì¼ ìƒì„± ë° íŠœë„ˆ ì‹¤í–‰
                import tempfile, yaml, os
                with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
                    yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
                    temp_config_path = f.name
                try:
                    tuner = HyperparameterTuner(temp_config_path, temp_config_path)
                    logger.info(f"{method} í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹¤í–‰ ì¤‘...")
                    best_params, best_score = tuner.optimize(start_mlflow_run=False)
                    logger.info(f"{method} íŠœë‹ ê²°ê³¼ ì €ì¥ ì¤‘...")
                    tuner.save_results()
                    method_results = {
                        'best_params': best_params,
                        'best_score': best_score,
                        'config': config
                    }
                    comparison_results['methods'][method] = method_results
                    if best_score > comparison_results['best_score']:
                        comparison_results['best_score'] = best_score
                        comparison_results['best_method'] = method
                    logger.info(f"{method} í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ: {best_score:.4f}")
                finally:
                    if os.path.exists(temp_config_path):
                        os.unlink(temp_config_path)
    
    # ë¹„êµ ê²°ê³¼ ìš”ì•½
    logger.info("\n=== ë¦¬ìƒ˜í”Œë§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë¹„êµ ê²°ê³¼ ìš”ì•½ ===")
    for method, results in comparison_results['methods'].items():
        best_score = results['best_score']
        logger.info(f"{method}: ìµœê³  ì„±ëŠ¥ = {best_score:.4f}")
    logger.info(f"ìµœê³  ì„±ëŠ¥ ê¸°ë²•: {comparison_results['best_method']} (ì„±ëŠ¥: {comparison_results['best_score']:.4f})")
    return comparison_results


def run_hyperparameter_tuning_with_config(config: Dict[str, Any], data_path: str = None, nrows: int = None, resampling_comparison: bool = False, resampling_methods: List[str] = None):
    """
    ë³‘í•©ëœ config ê°ì²´ë¥¼ ë°›ì•„ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    # ë°ì´í„° ê²½ë¡œ í™•ì¸
    if data_path is None:
        data_path = config.get('data', {}).get('data_path', "data/processed/processed_data_with_features.csv")
    if not Path(data_path).exists():
        raise FileNotFoundError(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_path}")
    logger.info(f"ë°ì´í„° íŒŒì¼: {data_path}")
    if nrows:
        logger.info(f"ì‚¬ìš©í•  ë°ì´í„° í–‰ ìˆ˜: {nrows}")
    
    # MLflow ì‹¤í—˜ ì„¤ì •
    experiment_name = config.get('mlflow', {}).get('experiment_name', 'hyperparameter_tuning')
    setup_mlflow_experiment(experiment_name)
    
    # ë¦¬ìƒ˜í”Œë§ ë¹„êµ ì‹¤í—˜
    if resampling_comparison:
        logger.info("=== ë¦¬ìƒ˜í”Œë§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë¹„êµ ì‹¤í—˜ ===")
        if resampling_methods is None:
            resampling_methods = ['none', 'smote', 'borderline_smote', 'adasyn', 'under_sampling', 'hybrid']
        # ConfigManager ê¸°ë°˜ ë¹„êµ í•¨ìˆ˜ í˜¸ì¶œ
        model_type = config['model']['model_type'] if 'model' in config and 'model_type' in config['model'] else None
        experiment_type = config.get('experiment_type', 'resampling')
        return run_resampling_tuning_comparison_with_configmanager(
            model_type=model_type,
            experiment_type=experiment_type,
            data_path=data_path,
            resampling_methods=resampling_methods,
            nrows=nrows
        )
    else:
        # ì¼ë°˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (MLflow run ì‹œì‘)
        with mlflow.start_run():
            # ê¸°ì¡´ ì½”ë“œ ê·¸ëŒ€ë¡œ ìœ ì§€
            import tempfile, yaml
            with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
                yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
                merged_config_path = f.name
            try:
                tuner = HyperparameterTuner(merged_config_path, merged_config_path)
                logger.info("í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹¤í–‰ ì¤‘...")
                best_params, best_score = tuner.optimize()
                logger.info("íŠœë‹ ê²°ê³¼ ì €ì¥ ì¤‘...")
                tuner.save_results()
                logger.info("=== í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ ===")
                logger.info(f"ìµœê³  ì„±ëŠ¥: {best_score:.4f}")
                logger.info("ìµœì  íŒŒë¼ë¯¸í„°:")
                for param, value in best_params.items():
                    logger.info(f"  {param}: {value}")
                return best_params, best_score
            finally:
                Path(merged_config_path).unlink(missing_ok=True)


def main():
    """ë©”ì¸ í•¨ìˆ˜ (ConfigManager ê¸°ë°˜ ê³„ì¸µì  config ë³‘í•© ì§€ì›)"""
    parser = argparse.ArgumentParser(
        description="í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í–‰ (ConfigManager ì§€ì›)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ê³„ì¸µì  configë¡œ íŠœë‹ ì‹¤í–‰
  python scripts/run_hyperparameter_tuning.py --model-type xgboost --experiment-type hyperparameter_tuning
  # legacy ë‹¨ì¼ íŒŒì¼ configë¡œ íŠœë‹ ì‹¤í–‰
  python scripts/run_hyperparameter_tuning.py --tuning_config configs/hyperparameter_tuning.yaml --base_config configs/default_config.yaml
        """
    )
    parser.add_argument("--model-type", type=str, choices=['xgboost', 'lightgbm', 'random_forest', 'catboost'], default=None, help="ì‚¬ìš©í•  ëª¨ë¸ íƒ€ì… (ê³„ì¸µì  config ë³‘í•©)")
    parser.add_argument("--experiment-type", type=str, default=None, help="ì‹¤í—˜ íƒ€ì… (focal_loss, resampling, hyperparameter_tuning ë“±)")
    parser.add_argument("--tuning_config", type=str, default=None, help="(legacy) íŠœë‹ ì„¤ì • íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--base_config", type=str, default=None, help="(legacy) ê¸°ë³¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--data_path", type=str, default=None, help="ë°ì´í„° íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--nrows", type=int, default=None, help="ì‚¬ìš©í•  ë°ì´í„° í–‰ ìˆ˜")
    parser.add_argument("--resampling-comparison", action="store_true", help="ë¦¬ìƒ˜í”Œë§ ê¸°ë²• ë¹„êµ ì‹¤í—˜ ì‹¤í–‰")
    parser.add_argument("--resampling-methods", nargs="+", choices=['none', 'smote', 'borderline_smote', 'adasyn', 'under_sampling', 'hybrid'], default=None, help="ë¹„êµí•  ë¦¬ìƒ˜í”Œë§ ê¸°ë²•ë“¤")
    parser.add_argument("--mlflow_ui", action="store_true", help="íŠœë‹ ì™„ë£Œ í›„ MLflow UI ì‹¤í–‰")
    args = parser.parse_args()
    try:
        if args.model_type:
            # ConfigManager ê¸°ë°˜ ê³„ì¸µì  config ë³‘í•©
            config_manager = ConfigManager()
            config = config_manager.create_experiment_config(args.model_type, args.experiment_type)
            if args.data_path:
                config['data']['data_path'] = args.data_path
            result = run_hyperparameter_tuning_with_config(
                config,
                data_path=args.data_path,
                nrows=args.nrows,
                resampling_comparison=args.resampling_comparison,
                resampling_methods=args.resampling_methods
            )
        elif args.tuning_config and args.base_config:
            # legacy: ë‹¨ì¼ íŒŒì¼ ê¸°ë°˜
            result = run_resampling_tuning_comparison(
                tuning_config_path=args.tuning_config,
                base_config_path=args.base_config,
                data_path=args.data_path,
                resampling_methods=args.resampling_methods,
                nrows=args.nrows
            )
        else:
            raise ValueError("model-type ë˜ëŠ” tuning_config+base_config ì¤‘ í•˜ë‚˜ëŠ” ë°˜ë“œì‹œ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
        if args.resampling_comparison:
            print(f"\nğŸ‰ ë¦¬ìƒ˜í”Œë§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë¹„êµ ì‹¤í—˜ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            print(f"ğŸ“Š ìµœê³  ì„±ëŠ¥ ê¸°ë²•: {result.get('best_method', 'none')}")
            print(f"âš™ï¸  ìµœê³  ì„±ëŠ¥: {result.get('best_score', 0.0):.4f}")
            print(f"\nğŸ“ˆ ê° ê¸°ë²•ë³„ ì„±ëŠ¥:")
            for method, method_result in result.get('methods', {}).items():
                print(f"  {method}: {method_result['best_score']:.4f}")
        else:
            best_params, best_score = result
            print(f"\nğŸ‰ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            print(f"ğŸ“Š ìµœê³  ì„±ëŠ¥: {best_score:.4f}")
            print(f"âš™ï¸  ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")
        if args.mlflow_ui:
            print(f"\nğŸŒ MLflow UIë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤...")
            print(f"   ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:5000 ìœ¼ë¡œ ì ‘ì†í•˜ì„¸ìš”.")
            print(f"   ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”.")
            import subprocess
            try:
                subprocess.run(["mlflow", "ui"], check=True)
            except KeyboardInterrupt:
                print(f"\nğŸ‘‹ MLflow UIë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            except Exception as e:
                print(f"âŒ MLflow UI ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
    except Exception as e:
        logger.error(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤íŒ¨: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main() 