#!/usr/bin/env python3
"""
í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

Optunaë¥¼ í™œìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
MLflowì™€ ì—°ë™ë˜ì–´ ì‹¤í—˜ ì¶”ì ì´ ê°€ëŠ¥í•˜ë©°, ë‹¤ì–‘í•œ íŠœë‹ ì „ëµì„ ì§€ì›í•©ë‹ˆë‹¤.
ë¦¬ìƒ˜í”Œë§ ê¸°ë²• ë¹„êµ ì‹¤í—˜ ê¸°ëŠ¥ë„ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
"""

import argparse
import logging
import sys
import os
from datetime import datetime
from pathlib import Path
import mlflow
import yaml
import pandas as pd
import numpy as np
import optuna
from typing import Dict, Any, List

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from src.hyperparameter_tuning import HyperparameterTuner
from src.splits import (
    load_config, 
    split_test_set, 
    validate_splits, 
    log_splits_info
)
from src.utils.config_manager import ConfigManager
from src.utils import setup_logging, setup_experiment_logging, log_experiment_summary, experiment_logging_context
from src.utils.experiment_results import save_experiment_results

# ë¡œê¹… ì„¤ì •
setup_logging(level="INFO")
logger = logging.getLogger(__name__)


def setup_mlflow_experiment(experiment_name: str):
    """
    MLflow ì‹¤í—˜ì„ ì„¤ì •í•©ë‹ˆë‹¤.
    
    Args:
        experiment_name: ì‹¤í—˜ ì´ë¦„
        
    Returns:
        experiment_id: ì‹¤í—˜ ID
    """
    from src.utils.mlflow_manager import setup_mlflow_experiment_safely
    
    experiment_id = setup_mlflow_experiment_safely(experiment_name)
    logger.info(f"MLflow ì‹¤í—˜ ì„¤ì •: {experiment_name} (ID: {experiment_id})")
    return experiment_id


def log_tuning_params(tuning_config: Dict[str, Any], base_config: Dict[str, Any]):
    """
    íŠœë‹ íŒŒë¼ë¯¸í„°ë¥¼ MLflowì— ë¡œê¹…í•©ë‹ˆë‹¤.
    
    Args:
        tuning_config: íŠœë‹ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        base_config: ê¸°ë³¸ ì„¤ì • ë”•ì…”ë„ˆë¦¬
    """
    try:
        # íŠœë‹ íŒŒë¼ë¯¸í„° (ì•ˆì „í•œ ì ‘ê·¼)
        tuning_params = {}
        
        # n_trials
        n_trials = tuning_config.get('tuning', {}).get('n_trials')
        if n_trials is not None:
            tuning_params["n_trials"] = n_trials
        
        # timeout
        timeout = tuning_config.get('tuning', {}).get('timeout')
        if timeout is not None:
            tuning_params["timeout"] = timeout
        
        # sampler
        sampler_type = tuning_config.get('sampler', {}).get('type')
        if sampler_type is not None:
            tuning_params["sampler"] = sampler_type
        
        # optimization_direction
        optimization_direction = tuning_config.get('evaluation', {}).get('optimization_direction')
        if optimization_direction is not None:
            tuning_params["optimization_direction"] = optimization_direction
        
        # primary_metric
        primary_metric = tuning_config.get('evaluation', {}).get('primary_metric')
        if primary_metric is not None:
            tuning_params["primary_metric"] = primary_metric
        
        # XGBoost íŒŒë¼ë¯¸í„° ë²”ìœ„
        xgboost_params = tuning_config.get('xgboost_params', {})
        for param, param_config in xgboost_params.items():
            if isinstance(param_config, dict):
                if 'range' in param_config and len(param_config['range']) >= 2:
                    tuning_params[f"tune_{param}_min"] = param_config['range'][0]
                    tuning_params[f"tune_{param}_max"] = param_config['range'][1]
                elif 'choices' in param_config:
                    tuning_params[f"tune_{param}_choices"] = str(param_config['choices'])
        
        # ê¸°ë³¸ ì„¤ì •ì—ì„œ ë¦¬ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° ì¶”ì¶œ (ê°œì„ ë¨)
        resampling_config = base_config.get('resampling', {})
        resampling_params = {
            "resampling_enabled": resampling_config.get('enabled', False),
            "resampling_method": resampling_config.get('method', 'none'),
            "resampling_random_state": resampling_config.get('random_state', 42)
        }
        
        # ë¦¬ìƒ˜í”Œë§ ê¸°ë²•ë³„ ìƒì„¸ íŒŒë¼ë¯¸í„° ì¶”ê°€
        method = resampling_config.get('method', 'none')
        if method == 'smote':
            resampling_params.update({
                "smote_k_neighbors": resampling_config.get('smote_k_neighbors', 5),
                "smote_sampling_strategy": resampling_config.get('smote_sampling_strategy', 0.1)
            })
        elif method == 'borderline_smote':
            resampling_params.update({
                "borderline_smote_k_neighbors": resampling_config.get('borderline_smote_k_neighbors', 5),
                "borderline_smote_sampling_strategy": resampling_config.get('borderline_smote_sampling_strategy', 0.1),
                "borderline_smote_m_neighbors": resampling_config.get('borderline_smote_m_neighbors', 10)
            })
        elif method == 'adasyn':
            resampling_params.update({
                "adasyn_k_neighbors": resampling_config.get('adasyn_k_neighbors', 5),
                "adasyn_sampling_strategy": resampling_config.get('adasyn_sampling_strategy', 0.1)
            })
        elif method == 'time_series_adapted':
            time_series_config = resampling_config.get('time_series_adapted', {})
            resampling_params.update({
                "time_weight": time_series_config.get('time_weight', 0.3),
                "temporal_window": time_series_config.get('temporal_window', 3),
                "seasonality_weight": time_series_config.get('seasonality_weight', 0.2),
                "pattern_preservation": time_series_config.get('pattern_preservation', True),
                "trend_preservation": time_series_config.get('trend_preservation', True)
            })
        
        # ëª¨ë“  íŒŒë¼ë¯¸í„° ë¡œê¹…
        all_params = {**tuning_params, **resampling_params}
        
        # MLflowì— íŒŒë¼ë¯¸í„° ë¡œê¹… (ê° íŒŒë¼ë¯¸í„°ë³„ë¡œ ê°œë³„ ì²˜ë¦¬)
        for param_name, param_value in all_params.items():
            if param_value is not None:
                try:
                    mlflow.log_param(param_name, param_value)
                except Exception as e:
                    logger.warning(f"MLflow íŒŒë¼ë¯¸í„° ë¡œê¹… ì‹¤íŒ¨ ({param_name}): {e}")
        
        logger.info(f"íŠœë‹ íŒŒë¼ë¯¸í„° ë¡œê¹… ì™„ë£Œ: {len(all_params)}ê°œ íŒŒë¼ë¯¸í„°")
        
    except Exception as e:
        logger.warning(f"íŠœë‹ íŒŒë¼ë¯¸í„° ë¡œê¹… ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        # ì˜ˆì™¸ê°€ ë°œìƒí•´ë„ ì‹¤í—˜ì€ ê³„ì† ì§„í–‰








def validate_config_files(tuning_config_path: str, base_config_path: str):
    """
    ì„¤ì • íŒŒì¼ë“¤ì˜ ìœ íš¨ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.
    
    Args:
        tuning_config_path: íŠœë‹ ì„¤ì • íŒŒì¼ ê²½ë¡œ
        base_config_path: ê¸°ë³¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ
    """
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not Path(tuning_config_path).exists():
        raise FileNotFoundError(f"íŠœë‹ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {tuning_config_path}")
    
    if not Path(base_config_path).exists():
        raise FileNotFoundError(f"ê¸°ë³¸ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {base_config_path}")
    
    # YAML íŒŒì‹± í…ŒìŠ¤íŠ¸
    try:
        with open(tuning_config_path, 'r', encoding='utf-8') as f:
            tuning_config = yaml.safe_load(f)
        logger.info("íŠœë‹ ì„¤ì • íŒŒì¼ íŒŒì‹± ì„±ê³µ")
    except Exception as e:
        raise ValueError(f"íŠœë‹ ì„¤ì • íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
    
    try:
        with open(base_config_path, 'r', encoding='utf-8') as f:
            base_config = yaml.safe_load(f)
        logger.info("ê¸°ë³¸ ì„¤ì • íŒŒì¼ íŒŒì‹± ì„±ê³µ")
    except Exception as e:
        raise ValueError(f"ê¸°ë³¸ ì„¤ì • íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
    
    # í•„ìˆ˜ ì„¤ì • í™•ì¸
    required_tuning_keys = ['tuning', 'sampler', 'xgboost_params', 'evaluation', 'results', 'mlflow']
    for key in required_tuning_keys:
        if key not in tuning_config:
            raise ValueError(f"íŠœë‹ ì„¤ì •ì— í•„ìˆ˜ í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤: {key}")
    
    logger.info("ì„¤ì • íŒŒì¼ ê²€ì¦ ì™„ë£Œ")





def run_hyperparameter_tuning_with_config(config: Dict[str, Any], data_path: str = None, nrows: int = None):
    """
    ì„¤ì • ê¸°ë°˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    
    Args:
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        data_path: ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        nrows: ì‚¬ìš©í•  ë°ì´í„° í–‰ ìˆ˜
        resampling_comparison: ë¦¬ìƒ˜í”Œë§ ë¹„êµ ì‹¤í—˜ ì—¬ë¶€ (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
        resampling_methods: ë¦¬ìƒ˜í”Œë§ ë°©ë²•ë“¤ (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
        
    Returns:
        íŠœë‹ ê²°ê³¼ íŠœí”Œ (best_params, best_score, study, tuner)
    """
    import time
    start_time = time.time()
    
    # ì‹¤í—˜ ì •ë³´ ì¶”ì¶œ
    model_type = config.get('model', {}).get('model_type', 'unknown')
    experiment_type = config.get('experiment_type', 'hyperparameter_tuning')
    
    # ìƒˆë¡œìš´ ë¡œê¹… ì‹œìŠ¤í…œ ì ìš©
    with experiment_logging_context(
        experiment_type=experiment_type,
        model_type=model_type,
        log_level="INFO",
        capture_console=True
    ) as log_file_path:
        
        logger.info(f"=== í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘ ===")
        logger.info(f"ëª¨ë¸ íƒ€ì…: {model_type}")
        logger.info(f"ì‹¤í—˜ íƒ€ì…: {experiment_type}")
        logger.info(f"ë¡œê·¸ íŒŒì¼: {log_file_path}")
        
        try:
            # MLflow ì‹¤í—˜ ë¬´ê²°ì„± ê²€ì¦ ë° ë³µêµ¬
            from src.utils.mlflow_manager import MLflowExperimentManager
            manager = MLflowExperimentManager()
            
            # í˜„ì¬ ì‹¤í—˜ ìƒíƒœ í™•ì¸
            logger.info("MLflow ì‹¤í—˜ ìƒíƒœ í™•ì¸ ì¤‘...")
            manager.print_experiment_summary()
            
            # Orphaned ì‹¤í—˜ ì •ë¦¬
            logger.info("Orphaned ì‹¤í—˜ ì •ë¦¬ ì¤‘...")
            deleted_experiments = manager.cleanup_orphaned_experiments(backup=True)
            if deleted_experiments:
                logger.info(f"ì •ë¦¬ëœ orphaned ì‹¤í—˜: {deleted_experiments}")
            
            # ë°ì´í„° ê²½ë¡œ ì„¤ì •
            if data_path is None:
                data_path = config['data'].get('file_path', 'data/processed/processed_data_with_features.csv')
            
            # ë°ì´í„° ë¡œë“œ
            logger.info(f"ë°ì´í„° ë¡œë“œ ì¤‘: {data_path}")
            if nrows:
                df = pd.read_csv(data_path, nrows=nrows)
                logger.info(f"í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë¡œë“œ: {len(df):,} í–‰")
            else:
                df = pd.read_csv(data_path)
                logger.info(f"ì „ì²´ ë°ì´í„° ë¡œë“œ: {len(df):,} í–‰")
            
            # ë°ì´í„° ì •ë³´ ìˆ˜ì§‘
            data_info = collect_data_info(df, config)
            logger.info(f"ë°ì´í„° ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {data_info['total_rows']} í–‰, {data_info['total_columns']} ì—´")
            
            # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„ë¦¬
            logger.info("=== í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„ë¦¬ ===")
            train_val_df, test_df, test_ids = split_test_set(df, config)
            
            # ë°ì´í„° ì •ë³´ ì—…ë°ì´íŠ¸ (í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì •ë³´ ì¶”ê°€)
            data_info = collect_data_info(df, config, train_val_df, test_df)
            
            # ë¶„í•  ê²€ì¦
            if not validate_splits(train_val_df, test_df, config):
                logger.error("ë¶„í•  ê²€ì¦ ì‹¤íŒ¨!")
                return None
            
            # MLflow ì‹¤í—˜ ì„¤ì •
            experiment_name = config['mlflow']['experiment_name']
            experiment_id = setup_mlflow_experiment(experiment_name)
            
            # ì‹¤í—˜ ë¬´ê²°ì„± ê²€ì¦
            if not manager.validate_experiment_integrity(experiment_id):
                logger.warning(f"ì‹¤í—˜ {experiment_id} ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨, ë³µêµ¬ ì‹œë„ ì¤‘...")
                if manager.repair_experiment(experiment_id):
                    logger.info(f"ì‹¤í—˜ {experiment_id} ë³µêµ¬ ì™„ë£Œ")
                else:
                    logger.error(f"ì‹¤í—˜ {experiment_id} ë³µêµ¬ ì‹¤íŒ¨")
            
            # íŠœë„ˆ ìƒì„±
            tuner = HyperparameterTuner(config, train_val_df, nrows=nrows)
            
            # íŠœë‹ ì‹¤í–‰ (ì•ˆì „í•œ MLflow run ê´€ë¦¬)
            from src.utils.mlflow_manager import safe_mlflow_run
            
            with safe_mlflow_run(experiment_id=experiment_id) as run:
                logger.info(f"MLflow Run ì‹œì‘: {run.info.run_id}")
                
                # ì„¤ì • ë¡œê¹…
                log_tuning_params(config, config)  # configë¥¼ base_configë¡œë„ ì‚¬ìš©
                
                # íŠœë‹ ì‹¤í–‰
                result = tuner.optimize(start_mlflow_run=False)  # ì´ë¯¸ runì´ ì‹œì‘ë˜ì–´ ìˆìœ¼ë¯€ë¡œ False
                
                # ê²°ê³¼ ì €ì¥
                try:
                    tuner.save_results()
                    logger.info("=== í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ ===")
                except Exception as e:
                    logger.error(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
                    # ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                
                # === ê³ ê¸‰ ë¡œê¹… ê¸°ëŠ¥ ì¶”ê°€ ===
                from src.utils.mlflow_logging import log_all_advanced_metrics
                
                # ëª¨ë“  ê³ ê¸‰ ë¡œê¹… ê¸°ëŠ¥ ì‹¤í–‰
                logging_results = log_all_advanced_metrics(tuner, config, run)
                
                # ë¡œê¹… ê²°ê³¼ ìš”ì•½
                successful_features = [k for k, v in logging_results.items() if v]
                failed_features = [k for k, v in logging_results.items() if not v]
                
                if successful_features:
                    logger.info(f"ê³ ê¸‰ ë¡œê¹… ì„±ê³µ: {successful_features}")
                if failed_features:
                    logger.warning(f"ê³ ê¸‰ ë¡œê¹… ì‹¤íŒ¨: {failed_features}")
                
                # ê²°ê³¼ ì¶”ì¶œ (ì•ˆì „í•˜ê²Œ)
                if result is None:
                    best_params = {}
                    best_score = 0.0
                    logger.warning("íŠœë‹ ê²°ê³¼ê°€ Noneì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                elif isinstance(result, tuple) and len(result) >= 2:
                    best_params = result[0] if result[0] is not None else {}
                    best_score = result[1] if result[1] is not None else 0.0
                elif isinstance(result, dict):
                    best_params = result.get('best_params', {})
                    best_score = result.get('best_score', 0.0)
                else:
                    best_params = {}
                    best_score = 0.0
                    logger.warning(f"ì˜ˆìƒì¹˜ ëª»í•œ ê²°ê³¼ íƒ€ì…: {type(result)}")
                
                logger.info(f"ìµœê³  ì„±ëŠ¥: {best_score:.4f}")
                logger.info("ìµœì  íŒŒë¼ë¯¸í„°:")
                for param, value in best_params.items():
                    logger.info(f"  {param}: {value}")
                
                # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
                execution_time = time.time() - start_time
                
                # ì‹¤í—˜ ìš”ì•½ ë¡œê¹…
                n_trials = len(tuner.study.trials) if hasattr(tuner, 'study') and tuner.study else 0
                log_experiment_summary(
                    experiment_type=experiment_type,
                    model_type=model_type,
                    best_score=best_score,
                    best_params=best_params,
                    execution_time=execution_time,
                    n_trials=n_trials,
                    data_info=data_info,
                    log_file_path=log_file_path
                )
                
                logger.info(f"=== í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ ===")
                logger.info(f"ìµœê³  ì„±ëŠ¥: {best_score:.6f}")
                logger.info(f"ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")
                logger.info(f"ì‹œë„ íšŸìˆ˜: {n_trials}")
                
                # ì‹¤í—˜ ê²°ê³¼ ì €ì¥
                try:
                    save_experiment_results(
                        result=(best_params, best_score),
                        model_type=model_type,
                        experiment_type=experiment_type,
                        nrows=nrows,
                        experiment_id=run.info.experiment_id,
                        run_id=run.info.run_id,
                        config=config,
                        data_info=data_info
                    )
                    logger.info("ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
                
                return best_params, best_score, run.info.experiment_id, run.info.run_id
                
        except Exception as e:
            logger.error(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise


def save_tuning_log(result, model_type, experiment_type, nrows=None, experiment_id=None, run_id=None, log_file_path=None):
    """
    íŠœë‹ ë¡œê·¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        result: íŠœë‹ ê²°ê³¼ (best_params, best_score) ë˜ëŠ” íŠœí”Œ
        model_type: ëª¨ë¸ íƒ€ì…
        experiment_type: ì‹¤í—˜ íƒ€ì…
        nrows: ì‚¬ìš©í•œ ë°ì´í„° í–‰ ìˆ˜
        experiment_id: MLflow ì‹¤í—˜ ID
        run_id: MLflow Run ID
        log_file_path: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©)
    """
    try:
        # ê²°ê³¼ íŒŒì‹±
        if isinstance(result, tuple) and len(result) >= 2:
            best_params = result[0] if result[0] is not None else {}
            best_score = result[1] if result[1] is not None else 0.0
        elif isinstance(result, dict):
            best_params = result.get('best_params', {})
            best_score = result.get('best_score', 0.0)
        else:
            best_params = {}
            best_score = 0.0
        
        # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        if log_file_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            log_file_path = f"logs/tuning_log_{model_type}_{experiment_type}_{timestamp}.txt"
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(log_file_path), exist_ok=True)
        
        # ë¡œê·¸ ë‚´ìš© ì‘ì„±
        log_content = f"""
=== íŠœë‹ ë¡œê·¸ ===
ëª¨ë¸ íƒ€ì…: {model_type}
ì‹¤í—˜ íƒ€ì…: {experiment_type}
ë°ì´í„° í–‰ ìˆ˜: {nrows}
MLflow ì‹¤í—˜ ID: {experiment_id}
MLflow Run ID: {run_id}
ìµœê³  ì„±ëŠ¥: {best_score:.6f}
ìµœì  íŒŒë¼ë¯¸í„°:
"""
        
        for param, value in best_params.items():
            log_content += f"  {param}: {value}\n"
        
        log_content += f"\nìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        
        # íŒŒì¼ ì €ì¥
        with open(log_file_path, 'w', encoding='utf-8') as f:
            f.write(log_content)
        
        logger.info(f"íŠœë‹ ë¡œê·¸ ì €ì¥ ì™„ë£Œ: {log_file_path}")
        
    except Exception as e:
        logger.error(f"íŠœë‹ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")





def collect_data_info(df: pd.DataFrame, config: Dict[str, Any], train_val_df: pd.DataFrame = None, test_df: pd.DataFrame = None) -> Dict[str, Any]:
    """
    ì‹¤í—˜ì— ì‚¬ìš©ëœ ë°ì´í„°ì˜ ìƒì„¸ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    
    Args:
        df: ì „ì²´ ë°ì´í„°í”„ë ˆì„
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        train_val_df: í›ˆë ¨/ê²€ì¦ ë°ì´í„°í”„ë ˆì„ (ì„ íƒì‚¬í•­)
        test_df: í…ŒìŠ¤íŠ¸ ë°ì´í„°í”„ë ˆì„ (ì„ íƒì‚¬í•­)
        
    Returns:
        ë°ì´í„° ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    data_info = {}
    
    # ê¸°ë³¸ ë°ì´í„° ì •ë³´
    data_info['total_rows'] = len(df)
    data_info['total_columns'] = len(df.columns)
    
    # ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    if 'data' in config:
        data_info['data_path'] = config['data'].get('data_path', 'N/A')
    
    # ê°œì¸ ìˆ˜ (ID ì»¬ëŸ¼ ê¸°ì¤€)
    id_column = config.get('time_series', {}).get('id_column', 'id')
    if id_column in df.columns:
        data_info['unique_ids'] = df[id_column].nunique()
    
    # ë°ì´í„° ê¸°ê°„
    date_column = config.get('time_series', {}).get('date_column', 'dov')
    year_column = config.get('time_series', {}).get('year_column', 'yov')
    
    if date_column in df.columns:
        try:
            df[date_column] = pd.to_datetime(df[date_column])
            date_range = f"{df[date_column].min().strftime('%Y-%m-%d')} ~ {df[date_column].max().strftime('%Y-%m-%d')}"
            data_info['date_range'] = date_range
        except:
            data_info['date_range'] = 'N/A'
    elif year_column in df.columns:
        try:
            year_range = f"{df[year_column].min()} ~ {df[year_column].max()}"
            data_info['date_range'] = year_range
        except:
            data_info['date_range'] = 'N/A'
    else:
        data_info['date_range'] = 'N/A'
    
    # í´ë˜ìŠ¤ ë¶„í¬ ì •ë³´
    class_distributions = {}
    target_columns = config.get('features', {}).get('target_columns', [])
    
    # ì „ì²´ ë°ì´í„°ì˜ í´ë˜ìŠ¤ ë¶„í¬
    for target in target_columns:
        if target in df.columns:
            class_distributions[f'full_{target}'] = df[target].value_counts().to_dict()
    
    # í›ˆë ¨/ê²€ì¦ ë°ì´í„°ì˜ í´ë˜ìŠ¤ ë¶„í¬
    if train_val_df is not None:
        train_original = {}
        for target in target_columns:
            if target in train_val_df.columns:
                train_original[target] = train_val_df[target].value_counts().to_dict()
        class_distributions['train_original'] = train_original
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ í´ë˜ìŠ¤ ë¶„í¬
    if test_df is not None:
        test_dist = {}
        for target in target_columns:
            if target in test_df.columns:
                test_dist[target] = test_df[target].value_counts().to_dict()
        class_distributions['test'] = test_dist
    
    data_info['class_distributions'] = class_distributions
    
    return data_info








def main():
    """ë©”ì¸ í•¨ìˆ˜ (ConfigManager ê¸°ë°˜ ê³„ì¸µì  config ë³‘í•© ì§€ì›)"""
    parser = argparse.ArgumentParser(
        description="í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í–‰ (ConfigManager ì§€ì›)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ê³„ì¸µì  configë¡œ íŠœë‹ ì‹¤í–‰
  python scripts/run_hyperparameter_tuning.py --model-type xgboost --experiment-type hyperparameter_tuning
  # legacy ë‹¨ì¼ íŒŒì¼ configë¡œ íŠœë‹ ì‹¤í–‰
  python scripts/run_hyperparameter_tuning.py --tuning_config configs/hyperparameter_tuning.yaml --base_config configs/default_config.yaml
        """
    )
    parser.add_argument("--model-type", type=str, choices=['xgboost', 'lightgbm', 'random_forest', 'catboost'], default=None, help="ì‚¬ìš©í•  ëª¨ë¸ íƒ€ì… (ê³„ì¸µì  config ë³‘í•©)")
    parser.add_argument("--experiment-type", type=str, default=None, help="ì‹¤í—˜ íƒ€ì… (focal_loss, resampling, hyperparameter_tuning ë“±)")
    parser.add_argument("--tuning_config", type=str, default=None, help="(legacy) íŠœë‹ ì„¤ì • íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--base_config", type=str, default=None, help="(legacy) ê¸°ë³¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--data_path", type=str, default=None, help="ë°ì´í„° íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--nrows", type=int, default=None, help="ì‚¬ìš©í•  ë°ì´í„° í–‰ ìˆ˜")

    parser.add_argument("--mlflow_ui", action="store_true", help="íŠœë‹ ì™„ë£Œ í›„ MLflow UI ì‹¤í–‰")
    
    # ìë™í™”ëœ ì‹¤í—˜ì„ ìœ„í•œ ì¶”ê°€ ì¸ìë“¤
    parser.add_argument("--split-strategy", type=str, choices=['group_kfold', 'time_series_walk_forward', 'time_series_group_kfold'], default=None, help="ë°ì´í„° ë¶„í•  ì „ëµ")
    parser.add_argument("--cv-folds", type=int, default=None, help="êµì°¨ ê²€ì¦ í´ë“œ ìˆ˜")
    parser.add_argument("--n-trials", type=int, default=None, help="í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œë„ íšŸìˆ˜")
    parser.add_argument("--tuning-direction", type=str, choices=['maximize', 'minimize'], default=None, help="íŠœë‹ ë°©í–¥ (maximize/minimize)")
    parser.add_argument("--primary-metric", type=str, default=None, help="ì£¼ìš” í‰ê°€ ì§€í‘œ (f1, precision, recall, mcc, roc_auc, pr_auc ë“±)")
    parser.add_argument("--test-size", type=float, default=None, help="í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¹„ìœ¨ (0.0-1.0)")
    parser.add_argument("--random-state", type=int, default=None, help="ëœë¤ ì‹œë“œ")
    parser.add_argument("--n-jobs", type=int, default=None, help="ë³‘ë ¬ ì²˜ë¦¬ ì‘ì—… ìˆ˜")
    parser.add_argument("--timeout", type=int, default=None, help="íŠœë‹ íƒ€ì„ì•„ì›ƒ (ì´ˆ)")
    parser.add_argument("--early-stopping", action="store_true", help="Early stopping í™œì„±í™”")
    parser.add_argument("--early-stopping-rounds", type=int, default=None, help="Early stopping ë¼ìš´ë“œ ìˆ˜")
    parser.add_argument("--feature-selection", action="store_true", help="í”¼ì²˜ ì„ íƒ í™œì„±í™”")
    parser.add_argument("--feature-selection-method", type=str, choices=['mutual_info', 'chi2', 'f_classif', 'recursive'], default=None, help="í”¼ì²˜ ì„ íƒ ë°©ë²•")
    parser.add_argument("--feature-selection-k", type=int, default=None, help="ì„ íƒí•  í”¼ì²˜ ìˆ˜")

    parser.add_argument("--experiment-name", type=str, default=None, help="MLflow ì‹¤í—˜ ì´ë¦„ (ê¸°ë³¸ê°’: model_type_experiment_type)")
    parser.add_argument("--save-model", action="store_true", help="ìµœì  ëª¨ë¸ ì €ì¥")
    parser.add_argument("--save-predictions", action="store_true", help="ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥")
    parser.add_argument("--verbose", type=int, choices=[0, 1, 2], default=1, help="ë¡œê·¸ ë ˆë²¨ (0: ìµœì†Œ, 1: ê¸°ë³¸, 2: ìƒì„¸)")
    args = parser.parse_args()
    try:
        if args.model_type:
            # ConfigManager ê¸°ë°˜ ê³„ì¸µì  config ë³‘í•©
            config_manager = ConfigManager()
            config = config_manager.create_experiment_config(args.model_type, args.experiment_type)
            
            # ëª…ë ¹í–‰ ì¸ìë¥¼ configì— ì ìš©
            config = config_manager.apply_command_line_args(config, args)
            
            # ì„¤ì • ìš”ì•½ ì¶œë ¥
            config_manager.print_config_summary(config)
            
            result = run_hyperparameter_tuning_with_config(
                config,
                data_path=args.data_path,
                nrows=args.nrows
            )
            # ì‹¤í—˜ ê²°ê³¼ ì €ì¥ì€ run_hyperparameter_tuning_with_config ë‚´ë¶€ì—ì„œ ì²˜ë¦¬ë¨
        elif args.tuning_config and args.base_config:
            # legacy: ë‹¨ì¼ íŒŒì¼ ê¸°ë°˜ (ë¦¬ìƒ˜í”Œë§ ê¸°ëŠ¥ ì œê±°ë¨)
            raise NotImplementedError("Legacy ëª¨ë“œëŠ” ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ConfigManagerë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")
        else:
            raise ValueError("model-type ë˜ëŠ” tuning_config+base_config ì¤‘ í•˜ë‚˜ëŠ” ë°˜ë“œì‹œ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
        best_params, best_score, _, _ = result
        print(f"\nğŸ‰ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ğŸ“Š ìµœê³  ì„±ëŠ¥: {best_score:.4f}")
        print(f"âš™ï¸  ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")
        if args.mlflow_ui:
            print(f"\nğŸŒ MLflow UIë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤...")
            print(f"   ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:5000 ìœ¼ë¡œ ì ‘ì†í•˜ì„¸ìš”.")
            print(f"   ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”.")
            import subprocess
            try:
                subprocess.run(["mlflow", "ui"], check=True)
            except KeyboardInterrupt:
                print(f"\nğŸ‘‹ MLflow UIë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            except Exception as e:
                print(f"âŒ MLflow UI ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
    except Exception as e:
        logger.error(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤íŒ¨: {str(e)}")
        # robust error logging (MLflowë¥¼ í†µí•´ ìë™ìœ¼ë¡œ ë¡œê¹…ë¨)
        logger.error(f"ì‹¤í—˜ ì‹¤íŒ¨ - ëª¨ë¸: {args.model_type if 'args' in locals() and args.model_type else 'unknown'}, ì‹¤í—˜: {args.experiment_type if 'args' in locals() and args.experiment_type else 'unknown'}")
        sys.exit(1)


if __name__ == "__main__":
    main() 