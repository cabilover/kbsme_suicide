#!/usr/bin/env python3
"""
í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

Optunaë¥¼ í™œìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
MLflowì™€ ì—°ë™ë˜ì–´ ì‹¤í—˜ ì¶”ì ì´ ê°€ëŠ¥í•˜ë©°, ë‹¤ì–‘í•œ íŠœë‹ ì „ëµì„ ì§€ì›í•©ë‹ˆë‹¤.
ë¦¬ìƒ˜í”Œë§ ê¸°ë²• ë¹„êµ ì‹¤í—˜ ê¸°ëŠ¥ë„ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
"""

import argparse
import logging
import sys
import os
from datetime import datetime
from pathlib import Path
import mlflow
import yaml
import pandas as pd
import numpy as np
from typing import Dict, Any, List

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from src.hyperparameter_tuning import HyperparameterTuner
from src.splits import (
    load_config, 
    split_test_set, 
    validate_splits, 
    log_splits_info
)
from src.utils.config_manager import ConfigManager
from src.utils import setup_logging, setup_experiment_logging, log_experiment_summary, experiment_logging_context

# ë¡œê¹… ì„¤ì •
setup_logging(level="INFO")
logger = logging.getLogger(__name__)


def setup_mlflow_experiment(experiment_name: str):
    """
    MLflow ì‹¤í—˜ì„ ì„¤ì •í•©ë‹ˆë‹¤.
    
    Args:
        experiment_name: ì‹¤í—˜ ì´ë¦„
        
    Returns:
        experiment_id: ì‹¤í—˜ ID
    """
    from src.utils.mlflow_manager import setup_mlflow_experiment_safely
    
    experiment_id = setup_mlflow_experiment_safely(experiment_name)
    logger.info(f"MLflow ì‹¤í—˜ ì„¤ì •: {experiment_name} (ID: {experiment_id})")
    return experiment_id


def log_tuning_params(tuning_config: Dict[str, Any], base_config: Dict[str, Any]):
    """
    íŠœë‹ íŒŒë¼ë¯¸í„°ë¥¼ MLflowì— ë¡œê¹…í•©ë‹ˆë‹¤.
    
    Args:
        tuning_config: íŠœë‹ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        base_config: ê¸°ë³¸ ì„¤ì • ë”•ì…”ë„ˆë¦¬
    """
    try:
        # íŠœë‹ íŒŒë¼ë¯¸í„° (ì•ˆì „í•œ ì ‘ê·¼)
        tuning_params = {}
        
        # n_trials
        n_trials = tuning_config.get('tuning', {}).get('n_trials')
        if n_trials is not None:
            tuning_params["n_trials"] = n_trials
        
        # timeout
        timeout = tuning_config.get('tuning', {}).get('timeout')
        if timeout is not None:
            tuning_params["timeout"] = timeout
        
        # sampler
        sampler_type = tuning_config.get('sampler', {}).get('type')
        if sampler_type is not None:
            tuning_params["sampler"] = sampler_type
        
        # optimization_direction
        optimization_direction = tuning_config.get('evaluation', {}).get('optimization_direction')
        if optimization_direction is not None:
            tuning_params["optimization_direction"] = optimization_direction
        
        # primary_metric
        primary_metric = tuning_config.get('evaluation', {}).get('primary_metric')
        if primary_metric is not None:
            tuning_params["primary_metric"] = primary_metric
        
        # XGBoost íŒŒë¼ë¯¸í„° ë²”ìœ„
        xgboost_params = tuning_config.get('xgboost_params', {})
        for param, param_config in xgboost_params.items():
            if isinstance(param_config, dict):
                if 'range' in param_config and len(param_config['range']) >= 2:
                    tuning_params[f"tune_{param}_min"] = param_config['range'][0]
                    tuning_params[f"tune_{param}_max"] = param_config['range'][1]
                elif 'choices' in param_config:
                    tuning_params[f"tune_{param}_choices"] = str(param_config['choices'])
        
        # ê¸°ë³¸ ì„¤ì •ì—ì„œ ë¦¬ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° ì¶”ì¶œ (ê°œì„ ë¨)
        resampling_config = base_config.get('resampling', {})
        resampling_params = {
            "resampling_enabled": resampling_config.get('enabled', False),
            "resampling_method": resampling_config.get('method', 'none'),
            "resampling_random_state": resampling_config.get('random_state', 42)
        }
        
        # ë¦¬ìƒ˜í”Œë§ ê¸°ë²•ë³„ ìƒì„¸ íŒŒë¼ë¯¸í„° ì¶”ê°€
        method = resampling_config.get('method', 'none')
        if method == 'smote':
            resampling_params.update({
                "smote_k_neighbors": resampling_config.get('smote_k_neighbors', 5),
                "smote_sampling_strategy": resampling_config.get('smote_sampling_strategy', 0.1)
            })
        elif method == 'borderline_smote':
            resampling_params.update({
                "borderline_smote_k_neighbors": resampling_config.get('borderline_smote_k_neighbors', 5),
                "borderline_smote_sampling_strategy": resampling_config.get('borderline_smote_sampling_strategy', 0.1),
                "borderline_smote_m_neighbors": resampling_config.get('borderline_smote_m_neighbors', 10)
            })
        elif method == 'adasyn':
            resampling_params.update({
                "adasyn_k_neighbors": resampling_config.get('adasyn_k_neighbors', 5),
                "adasyn_sampling_strategy": resampling_config.get('adasyn_sampling_strategy', 0.1)
            })
        elif method == 'time_series_adapted':
            time_series_config = resampling_config.get('time_series_adapted', {})
            resampling_params.update({
                "time_weight": time_series_config.get('time_weight', 0.3),
                "temporal_window": time_series_config.get('temporal_window', 3),
                "seasonality_weight": time_series_config.get('seasonality_weight', 0.2),
                "pattern_preservation": time_series_config.get('pattern_preservation', True),
                "trend_preservation": time_series_config.get('trend_preservation', True)
            })
        
        # ëª¨ë“  íŒŒë¼ë¯¸í„° ë¡œê¹…
        all_params = {**tuning_params, **resampling_params}
        
        # MLflowì— íŒŒë¼ë¯¸í„° ë¡œê¹… (ê° íŒŒë¼ë¯¸í„°ë³„ë¡œ ê°œë³„ ì²˜ë¦¬)
        for param_name, param_value in all_params.items():
            if param_value is not None:
                try:
                    mlflow.log_param(param_name, param_value)
                except Exception as e:
                    logger.warning(f"MLflow íŒŒë¼ë¯¸í„° ë¡œê¹… ì‹¤íŒ¨ ({param_name}): {e}")
        
        logger.info(f"íŠœë‹ íŒŒë¼ë¯¸í„° ë¡œê¹… ì™„ë£Œ: {len(all_params)}ê°œ íŒŒë¼ë¯¸í„°")
        
    except Exception as e:
        logger.warning(f"íŠœë‹ íŒŒë¼ë¯¸í„° ë¡œê¹… ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        # ì˜ˆì™¸ê°€ ë°œìƒí•´ë„ ì‹¤í—˜ì€ ê³„ì† ì§„í–‰








def validate_config_files(tuning_config_path: str, base_config_path: str):
    """
    ì„¤ì • íŒŒì¼ë“¤ì˜ ìœ íš¨ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.
    
    Args:
        tuning_config_path: íŠœë‹ ì„¤ì • íŒŒì¼ ê²½ë¡œ
        base_config_path: ê¸°ë³¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ
    """
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not Path(tuning_config_path).exists():
        raise FileNotFoundError(f"íŠœë‹ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {tuning_config_path}")
    
    if not Path(base_config_path).exists():
        raise FileNotFoundError(f"ê¸°ë³¸ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {base_config_path}")
    
    # YAML íŒŒì‹± í…ŒìŠ¤íŠ¸
    try:
        with open(tuning_config_path, 'r', encoding='utf-8') as f:
            tuning_config = yaml.safe_load(f)
        logger.info("íŠœë‹ ì„¤ì • íŒŒì¼ íŒŒì‹± ì„±ê³µ")
    except Exception as e:
        raise ValueError(f"íŠœë‹ ì„¤ì • íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
    
    try:
        with open(base_config_path, 'r', encoding='utf-8') as f:
            base_config = yaml.safe_load(f)
        logger.info("ê¸°ë³¸ ì„¤ì • íŒŒì¼ íŒŒì‹± ì„±ê³µ")
    except Exception as e:
        raise ValueError(f"ê¸°ë³¸ ì„¤ì • íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
    
    # í•„ìˆ˜ ì„¤ì • í™•ì¸
    required_tuning_keys = ['tuning', 'sampler', 'xgboost_params', 'evaluation', 'results', 'mlflow']
    for key in required_tuning_keys:
        if key not in tuning_config:
            raise ValueError(f"íŠœë‹ ì„¤ì •ì— í•„ìˆ˜ í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤: {key}")
    
    logger.info("ì„¤ì • íŒŒì¼ ê²€ì¦ ì™„ë£Œ")





def run_hyperparameter_tuning_with_config(config: Dict[str, Any], data_path: str = None, nrows: int = None):
    """
    ì„¤ì • ê¸°ë°˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    
    Args:
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        data_path: ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        nrows: ì‚¬ìš©í•  ë°ì´í„° í–‰ ìˆ˜
        resampling_comparison: ë¦¬ìƒ˜í”Œë§ ë¹„êµ ì‹¤í—˜ ì—¬ë¶€ (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
        resampling_methods: ë¦¬ìƒ˜í”Œë§ ë°©ë²•ë“¤ (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
        
    Returns:
        íŠœë‹ ê²°ê³¼ íŠœí”Œ (best_params, best_score, study, tuner)
    """
    import time
    start_time = time.time()
    
    # ì‹¤í—˜ ì •ë³´ ì¶”ì¶œ
    model_type = config.get('model', {}).get('model_type', 'unknown')
    experiment_type = config.get('experiment_type', 'hyperparameter_tuning')
    
    # ìƒˆë¡œìš´ ë¡œê¹… ì‹œìŠ¤í…œ ì ìš©
    with experiment_logging_context(
        experiment_type=experiment_type,
        model_type=model_type,
        log_level="INFO",
        capture_console=True
    ) as log_file_path:
        
        logger.info(f"=== í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘ ===")
        logger.info(f"ëª¨ë¸ íƒ€ì…: {model_type}")
        logger.info(f"ì‹¤í—˜ íƒ€ì…: {experiment_type}")
        logger.info(f"ë¡œê·¸ íŒŒì¼: {log_file_path}")
        
        try:
            # MLflow ì‹¤í—˜ ë¬´ê²°ì„± ê²€ì¦ ë° ë³µêµ¬
            from src.utils.mlflow_manager import MLflowExperimentManager
            manager = MLflowExperimentManager()
            
            # í˜„ì¬ ì‹¤í—˜ ìƒíƒœ í™•ì¸
            logger.info("MLflow ì‹¤í—˜ ìƒíƒœ í™•ì¸ ì¤‘...")
            manager.print_experiment_summary()
            
            # Orphaned ì‹¤í—˜ ì •ë¦¬
            logger.info("Orphaned ì‹¤í—˜ ì •ë¦¬ ì¤‘...")
            deleted_experiments = manager.cleanup_orphaned_experiments(backup=True)
            if deleted_experiments:
                logger.info(f"ì •ë¦¬ëœ orphaned ì‹¤í—˜: {deleted_experiments}")
            
            # ë°ì´í„° ê²½ë¡œ ì„¤ì •
            if data_path is None:
                data_path = config['data'].get('file_path', 'data/processed/processed_data_with_features.csv')
            
            # ë°ì´í„° ë¡œë“œ
            logger.info(f"ë°ì´í„° ë¡œë“œ ì¤‘: {data_path}")
            if nrows:
                df = pd.read_csv(data_path, nrows=nrows)
                logger.info(f"í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë¡œë“œ: {len(df):,} í–‰")
            else:
                df = pd.read_csv(data_path)
                logger.info(f"ì „ì²´ ë°ì´í„° ë¡œë“œ: {len(df):,} í–‰")
            
            # ë°ì´í„° ì •ë³´ ìˆ˜ì§‘
            data_info = collect_data_info(df, config)
            logger.info(f"ë°ì´í„° ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {data_info['total_rows']} í–‰, {data_info['total_columns']} ì—´")
            
            # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„ë¦¬
            logger.info("=== í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„ë¦¬ ===")
            train_val_df, test_df, test_ids = split_test_set(df, config)
            
            # ë°ì´í„° ì •ë³´ ì—…ë°ì´íŠ¸ (í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì •ë³´ ì¶”ê°€)
            data_info = collect_data_info(df, config, train_val_df, test_df)
            
            # ë¶„í•  ê²€ì¦
            if not validate_splits(train_val_df, test_df, config):
                logger.error("ë¶„í•  ê²€ì¦ ì‹¤íŒ¨!")
                return None
            
            # MLflow ì‹¤í—˜ ì„¤ì •
            experiment_name = config['mlflow']['experiment_name']
            experiment_id = setup_mlflow_experiment(experiment_name)
            
            # ì‹¤í—˜ ë¬´ê²°ì„± ê²€ì¦
            if not manager.validate_experiment_integrity(experiment_id):
                logger.warning(f"ì‹¤í—˜ {experiment_id} ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨, ë³µêµ¬ ì‹œë„ ì¤‘...")
                if manager.repair_experiment(experiment_id):
                    logger.info(f"ì‹¤í—˜ {experiment_id} ë³µêµ¬ ì™„ë£Œ")
                else:
                    logger.error(f"ì‹¤í—˜ {experiment_id} ë³µêµ¬ ì‹¤íŒ¨")
            
            # íŠœë„ˆ ìƒì„±
            tuner = HyperparameterTuner(config, train_val_df, nrows=nrows)
            
            # íŠœë‹ ì‹¤í–‰ (ì•ˆì „í•œ MLflow run ê´€ë¦¬)
            from src.utils.mlflow_manager import safe_mlflow_run
            
            with safe_mlflow_run(experiment_id=experiment_id) as run:
                logger.info(f"MLflow Run ì‹œì‘: {run.info.run_id}")
                
                # ì„¤ì • ë¡œê¹…
                log_tuning_params(config, config)  # configë¥¼ base_configë¡œë„ ì‚¬ìš©
                
                # íŠœë‹ ì‹¤í–‰
                result = tuner.optimize(start_mlflow_run=False)  # ì´ë¯¸ runì´ ì‹œì‘ë˜ì–´ ìˆìœ¼ë¯€ë¡œ False
                
                # ê²°ê³¼ ì €ì¥
                try:
                    tuner.save_results()
                    logger.info("=== í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ ===")
                except Exception as e:
                    logger.error(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
                    # ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                
                # ê²°ê³¼ ì¶”ì¶œ (ì•ˆì „í•˜ê²Œ)
                if result is None:
                    best_params = {}
                    best_score = 0.0
                    logger.warning("íŠœë‹ ê²°ê³¼ê°€ Noneì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                elif isinstance(result, tuple) and len(result) >= 2:
                    best_params = result[0] if result[0] is not None else {}
                    best_score = result[1] if result[1] is not None else 0.0
                elif isinstance(result, dict):
                    best_params = result.get('best_params', {})
                    best_score = result.get('best_score', 0.0)
                else:
                    best_params = {}
                    best_score = 0.0
                    logger.warning(f"ì˜ˆìƒì¹˜ ëª»í•œ ê²°ê³¼ íƒ€ì…: {type(result)}")
                
                logger.info(f"ìµœê³  ì„±ëŠ¥: {best_score:.4f}")
                logger.info("ìµœì  íŒŒë¼ë¯¸í„°:")
                for param, value in best_params.items():
                    logger.info(f"  {param}: {value}")
                
                # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
                execution_time = time.time() - start_time
                
                # ì‹¤í—˜ ìš”ì•½ ë¡œê¹…
                n_trials = len(tuner.study.trials) if hasattr(tuner, 'study') and tuner.study else 0
                log_experiment_summary(
                    experiment_type=experiment_type,
                    model_type=model_type,
                    best_score=best_score,
                    best_params=best_params,
                    execution_time=execution_time,
                    n_trials=n_trials,
                    data_info=data_info,
                    log_file_path=log_file_path
                )
                
                logger.info(f"=== í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ ===")
                logger.info(f"ìµœê³  ì„±ëŠ¥: {best_score:.6f}")
                logger.info(f"ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")
                logger.info(f"ì‹œë„ íšŸìˆ˜: {n_trials}")
                
                return best_params, best_score, run.info.experiment_id, run.info.run_id
                
        except Exception as e:
            logger.error(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise


def save_tuning_log(result, model_type, experiment_type, nrows=None, experiment_id=None, run_id=None, log_file_path=None):
    """
    íŠœë‹ ë¡œê·¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        result: íŠœë‹ ê²°ê³¼ (best_params, best_score) ë˜ëŠ” íŠœí”Œ
        model_type: ëª¨ë¸ íƒ€ì…
        experiment_type: ì‹¤í—˜ íƒ€ì…
        nrows: ì‚¬ìš©í•œ ë°ì´í„° í–‰ ìˆ˜
        experiment_id: MLflow ì‹¤í—˜ ID
        run_id: MLflow Run ID
        log_file_path: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©)
    """
    try:
        # ê²°ê³¼ íŒŒì‹±
        if isinstance(result, tuple) and len(result) >= 2:
            best_params = result[0] if result[0] is not None else {}
            best_score = result[1] if result[1] is not None else 0.0
        elif isinstance(result, dict):
            best_params = result.get('best_params', {})
            best_score = result.get('best_score', 0.0)
        else:
            best_params = {}
            best_score = 0.0
        
        # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        if log_file_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            log_file_path = f"logs/tuning_log_{model_type}_{experiment_type}_{timestamp}.txt"
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(log_file_path), exist_ok=True)
        
        # ë¡œê·¸ ë‚´ìš© ì‘ì„±
        log_content = f"""
=== íŠœë‹ ë¡œê·¸ ===
ëª¨ë¸ íƒ€ì…: {model_type}
ì‹¤í—˜ íƒ€ì…: {experiment_type}
ë°ì´í„° í–‰ ìˆ˜: {nrows}
MLflow ì‹¤í—˜ ID: {experiment_id}
MLflow Run ID: {run_id}
ìµœê³  ì„±ëŠ¥: {best_score:.6f}
ìµœì  íŒŒë¼ë¯¸í„°:
"""
        
        for param, value in best_params.items():
            log_content += f"  {param}: {value}\n"
        
        log_content += f"\nìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        
        # íŒŒì¼ ì €ì¥
        with open(log_file_path, 'w', encoding='utf-8') as f:
            f.write(log_content)
        
        logger.info(f"íŠœë‹ ë¡œê·¸ ì €ì¥ ì™„ë£Œ: {log_file_path}")
        
    except Exception as e:
        logger.error(f"íŠœë‹ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")


def save_experiment_results(result, model_type, experiment_type, nrows=None, experiment_id=None, run_id=None, config=None, data_info=None):
    """
    ì‹¤í—˜ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        result: ì‹¤í—˜ ê²°ê³¼
        model_type: ëª¨ë¸ íƒ€ì…
        experiment_type: ì‹¤í—˜ íƒ€ì…
        nrows: ì‚¬ìš©ëœ ë°ì´í„° í–‰ ìˆ˜
        experiment_id: MLflow ì‹¤í—˜ ID
        run_id: MLflow run ID
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬ (ìƒˆë¡œ ì¶”ê°€)
        data_info: ë°ì´í„° ì •ë³´ ë”•ì…”ë„ˆë¦¬ (ìƒˆë¡œ ì¶”ê°€)
    """
    from datetime import datetime
    import json
    import psutil
    import platform
    import time
    
    # results í´ë” ìƒì„± (ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°)
    results_dir = Path("/home/junhyung/Documents/simcare/kbsmc_suicide/results")
    results_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = results_dir / f"experiment_results_{timestamp}.txt"
    
    # MLflow ì‹¤í—˜ ë§í¬ ìƒì„±
    mlflow_link = "http://localhost:5000"
    if experiment_id is None or run_id is None:
        experiment_id = "N/A"
        run_id = "N/A"
        try:
            # í˜„ì¬ í™œì„±í™”ëœ MLflow run ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            current_run = mlflow.active_run()
            if current_run:
                experiment_id = current_run.info.experiment_id
                run_id = current_run.info.run_id
        except:
            pass
    
    if experiment_id != "N/A" and run_id != "N/A":
        mlflow_link = f"http://localhost:5000/#/experiments/{experiment_id}/runs/{run_id}"
    
    # MLflowì—ì„œ ìƒì„¸ ë©”íŠ¸ë¦­ ì¶”ì¶œ (ê°œì„ ë¨)
    detailed_metrics = {}
    run_data = None
    if experiment_id != "N/A" and run_id != "N/A":
        try:
            run_data = mlflow.get_run(run_id)
            detailed_metrics = run_data.data.metrics
            logger.info(f"MLflowì—ì„œ {len(detailed_metrics)}ê°œ ë©”íŠ¸ë¦­ ì¶”ì¶œ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"MLflowì—ì„œ ìƒì„¸ ë©”íŠ¸ë¦­ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    
    # ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘
    system_info = {
        'python_version': platform.python_version(),
        'platform': platform.platform(),
        'cpu_count': psutil.cpu_count(),
        'memory_total_gb': round(psutil.virtual_memory().total / (1024**3), 2),
        'memory_available_gb': round(psutil.virtual_memory().available / (1024**3), 2)
    }
    
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(f"ì‹¤í—˜ ê²°ê³¼ - {timestamp}\n")
        f.write("=" * 50 + "\n")
        f.write(f"ëª¨ë¸ íƒ€ì…: {model_type}\n")
        f.write(f"ì‹¤í—˜ íƒ€ì…: {experiment_type}\n")
        if nrows:
            f.write(f"ì‚¬ìš© ë°ì´í„° í–‰ ìˆ˜: {nrows}\n")
        f.write(f"MLflow Experiment ID: {experiment_id}\n")
        f.write(f"MLflow Run ID: {run_id}\n")
        f.write(f"MLflow ë§í¬: {mlflow_link}\n")
        f.write("\n")
        
        # === 1. ì‚¬ìš©ëœ ë°ì´í„° ì •ë³´ ===
        f.write("=== ë°ì´í„° ì •ë³´ ===\n")
        if data_info:
            f.write(f"ë°ì´í„° íŒŒì¼ ê²½ë¡œ: {data_info.get('data_path', 'N/A')}\n")
            f.write(f"ì „ì²´ ë°ì´í„° í¬ê¸°: {data_info.get('total_rows', 'N/A')} í–‰ Ã— {data_info.get('total_columns', 'N/A')} ì—´\n")
            f.write(f"ê°œì¸ ìˆ˜: {data_info.get('unique_ids', 'N/A')}ëª…\n")
            f.write(f"ë°ì´í„° ê¸°ê°„: {data_info.get('date_range', 'N/A')}\n")
        else:
            f.write("ë°ì´í„° ì •ë³´: N/A\n")
        f.write("\n")
        
        # === 2. Target ë° Input ì •ë³´ ===
        f.write("=== Target ë° Input ì •ë³´ ===\n")
        if config and 'features' in config:
            target_cols = config['features'].get('target_columns', [])
            f.write(f"Target ë³€ìˆ˜: {len(target_cols)}ê°œ\n")
            for i, target in enumerate(target_cols, 1):
                f.write(f"  {i}. {target}\n")
            
            selected_features = config['features'].get('selected_features', [])
            f.write(f"Input ë³€ìˆ˜: {len(selected_features)}ê°œ\n")
            for i, feature in enumerate(selected_features, 1):
                f.write(f"  {i}. {feature}\n")
        else:
            f.write("Target ë° Input ì •ë³´: N/A\n")
        f.write("\n")
        
        # === 3. ë¦¬ìƒ˜í”Œë§ ì ìš© ì—¬ë¶€ ===
        f.write("=== ë¦¬ìƒ˜í”Œë§ ì •ë³´ ===\n")
        if config and 'resampling' in config:
            resampling_config = config['resampling']
            enabled = resampling_config.get('enabled', False)
            f.write(f"ë¦¬ìƒ˜í”Œë§ ì ìš©: {'ì˜ˆ' if enabled else 'ì•„ë‹ˆì˜¤'}\n")
            if enabled:
                method = resampling_config.get('method', 'unknown')
                f.write(f"ë¦¬ìƒ˜í”Œë§ ë°©ë²•: {method}\n")
                if method == 'smote':
                    k_neighbors = resampling_config.get('smote_k_neighbors', 5)
                    f.write(f"SMOTE k_neighbors: {k_neighbors}\n")
                elif method == 'borderline_smote':
                    k_neighbors = resampling_config.get('borderline_smote_k_neighbors', 5)
                    f.write(f"Borderline SMOTE k_neighbors: {k_neighbors}\n")
                elif method == 'adasyn':
                    k_neighbors = resampling_config.get('adasyn_k_neighbors', 5)
                    f.write(f"ADASYN k_neighbors: {k_neighbors}\n")
                elif method == 'under_sampling':
                    strategy = resampling_config.get('under_sampling_strategy', 'random')
                    f.write(f"ì–¸ë”ìƒ˜í”Œë§ ì „ëµ: {strategy}\n")
                elif method == 'hybrid':
                    strategy = resampling_config.get('hybrid_strategy', 'smote_tomek')
                    f.write(f"í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ: {strategy}\n")
                elif method == 'time_series_adapted':
                    time_series_config = resampling_config.get('time_series_adapted', {})
                    f.write(f"ì‹œê³„ì—´ íŠ¹í™” ë¦¬ìƒ˜í”Œë§:\n")
                    f.write(f"  - ì‹œê°„ ê°€ì¤‘ì¹˜: {time_series_config.get('time_weight', 0.3)}\n")
                    f.write(f"  - ì‹œê°„ì  ìœˆë„ìš°: {time_series_config.get('temporal_window', 3)}\n")
                    f.write(f"  - ê³„ì ˆì„± ê°€ì¤‘ì¹˜: {time_series_config.get('seasonality_weight', 0.2)}\n")
                    f.write(f"  - íŒ¨í„´ ë³´ì¡´: {time_series_config.get('pattern_preservation', True)}\n")
                    f.write(f"  - ì¶”ì„¸ ë³´ì¡´: {time_series_config.get('trend_preservation', True)}\n")
        else:
            f.write("ë¦¬ìƒ˜í”Œë§ ì •ë³´: N/A\n")
        f.write("\n")
        
        # === 4. í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§ ì ìš© ì—¬ë¶€ ===
        f.write("=== í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§ ì •ë³´ ===\n")
        if config and 'features' in config:
            features_config = config['features']
            enable_fe = features_config.get('enable_feature_engineering', False)
            f.write(f"í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§ ì ìš©: {'ì˜ˆ' if enable_fe else 'ì•„ë‹ˆì˜¤'}\n")
            if enable_fe:
                # ì§€ì—° í”¼ì²˜ ì„¤ì •
                enable_lagged = features_config.get('enable_lagged_features', False)
                f.write(f"ì§€ì—° í”¼ì²˜ ìƒì„±: {'ì˜ˆ' if enable_lagged else 'ì•„ë‹ˆì˜¤'}\n")
                if enable_lagged:
                    lag_periods = features_config.get('lag_periods', [])
                    f.write(f"ì§€ì—° ê¸°ê°„: {lag_periods}\n")
                
                # ì´ë™ í†µê³„ í”¼ì²˜ ì„¤ì •
                enable_rolling = features_config.get('enable_rolling_stats', False)
                f.write(f"ì´ë™ í†µê³„ í”¼ì²˜ ìƒì„±: {'ì˜ˆ' if enable_rolling else 'ì•„ë‹ˆì˜¤'}\n")
                if enable_rolling:
                    rolling_config = features_config.get('rolling_stats', {})
                    window_sizes = rolling_config.get('window_sizes', [])
                    f.write(f"ì´ë™ ìœˆë„ìš° í¬ê¸°: {window_sizes}\n")
        else:
            f.write("í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§ ì •ë³´: N/A\n")
        f.write("\n")
        
        # === 5. í›ˆë ¨/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í´ë˜ìŠ¤ ë¶„í¬ ===
        f.write("=== í´ë˜ìŠ¤ ë¶„í¬ ì •ë³´ ===\n")
        if data_info and 'class_distributions' in data_info:
            class_dist = data_info['class_distributions']
            
            # í›ˆë ¨ ì„¸íŠ¸ í´ë˜ìŠ¤ ë¶„í¬
            if 'train_original' in class_dist:
                f.write("í›ˆë ¨ ì„¸íŠ¸ (ë¦¬ìƒ˜í”Œë§ ì „):\n")
                for target, dist in class_dist['train_original'].items():
                    f.write(f"  {target}: {dist}\n")
            
            if 'train_resampled' in class_dist:
                f.write("í›ˆë ¨ ì„¸íŠ¸ (ë¦¬ìƒ˜í”Œë§ í›„):\n")
                for target, dist in class_dist['train_resampled'].items():
                    f.write(f"  {target}: {dist}\n")
            
            # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í´ë˜ìŠ¤ ë¶„í¬
            if 'test' in class_dist:
                f.write("í…ŒìŠ¤íŠ¸ ì„¸íŠ¸:\n")
                for target, dist in class_dist['test'].items():
                    f.write(f"  {target}: {dist}\n")
        else:
            f.write("í´ë˜ìŠ¤ ë¶„í¬ ì •ë³´: N/A\n")
        f.write("\n")
        
        # === 6. ìƒì„¸ ì„±ëŠ¥ ì§€í‘œ (ìµœì  íŒŒë¼ë¯¸í„°) - ê°œì„ ë¨ ===
        f.write("=== ìƒì„¸ ì„±ëŠ¥ ì§€í‘œ (ìµœì  íŒŒë¼ë¯¸í„°) ===\n")
        if detailed_metrics:
            # ë©”íŠ¸ë¦­ ì¹´í…Œê³ ë¦¬í™” ê°œì„  - MLflow ë©”íŠ¸ë¦­ íŒ¨í„´ì— ë§ê²Œ ìˆ˜ì •
            metric_categories = {
                'ê¸°ë³¸ ì„±ëŠ¥ ì§€í‘œ': [],
                'êµì°¨ ê²€ì¦ í†µê³„': [],
                'Trialë³„ ì„±ëŠ¥': [],
                'Foldë³„ ì„±ëŠ¥': [],
                'ëª¨ë¸ íŠ¹ì„±': [],
                'íŠœë‹ ê³¼ì •': [],
                'ê¸°íƒ€ ì§€í‘œ': []
            }
            
            # ë©”íŠ¸ë¦­ì„ íŒ¨í„´ì— ë”°ë¼ ë¶„ë¥˜
            for key, value in detailed_metrics.items():
                if isinstance(value, (int, float)):
                    value_str = f"{value:.4f}" if isinstance(value, float) else str(value)
                else:
                    value_str = str(value)
                
                # êµì°¨ ê²€ì¦ í†µê³„ (cv_ë¡œ ì‹œì‘í•˜ëŠ” ë©”íŠ¸ë¦­)
                if key.startswith('cv_'):
                    metric_categories['êµì°¨ ê²€ì¦ í†µê³„'].append(f"  - {key}: {value_str}")
                # Trialë³„ ì„±ëŠ¥ (trial_ë¡œ ì‹œì‘í•˜ëŠ” ë©”íŠ¸ë¦­)
                elif key.startswith('trial_'):
                    metric_categories['Trialë³„ ì„±ëŠ¥'].append(f"  - {key}: {value_str}")
                # Foldë³„ ì„±ëŠ¥ (fold_ë¡œ ì‹œì‘í•˜ëŠ” ë©”íŠ¸ë¦­)
                elif key.startswith('fold_'):
                    metric_categories['Foldë³„ ì„±ëŠ¥'].append(f"  - {key}: {value_str}")
                # ê¸°ë³¸ ì„±ëŠ¥ ì§€í‘œ (accuracy, precision, recall, f1, roc_auc ë“±)
                elif any(metric in key.lower() for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'pr_auc', 'mcc', 'kappa', 'specificity', 'sensitivity', 'ppv', 'npv', 'fpr', 'fnr', 'tpr', 'tnr', 'balanced_accuracy']):
                    metric_categories['ê¸°ë³¸ ì„±ëŠ¥ ì§€í‘œ'].append(f"  - {key}: {value_str}")
                # ëª¨ë¸ íŠ¹ì„± (best_iteration, total_trees, max_depth ë“±)
                elif any(metric in key.lower() for metric in ['best_iteration', 'total_trees', 'max_depth', 'avg_depth', 'total_leaves', 'early_stopping', 'feature_importance']):
                    metric_categories['ëª¨ë¸ íŠ¹ì„±'].append(f"  - {key}: {value_str}")
                # íŠœë‹ ê³¼ì • (best_score, trial_number ë“±)
                elif any(metric in key.lower() for metric in ['best_score', 'trial_number', 'all_trials']):
                    metric_categories['íŠœë‹ ê³¼ì •'].append(f"  - {key}: {value_str}")
                # ê¸°íƒ€ ì§€í‘œ
                else:
                    metric_categories['ê¸°íƒ€ ì§€í‘œ'].append(f"  - {key}: {value_str}")
            
            # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë©”íŠ¸ë¦­ ì¶œë ¥
            for category, metrics in metric_categories.items():
                if metrics:
                    f.write(f"{category}:\n")
                    # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ìƒìœ„ 10ê°œë§Œ í‘œì‹œ (ë„ˆë¬´ ë§ìœ¼ë©´ ê°€ë…ì„± ì €í•˜)
                    for metric_str in metrics[:10]:
                        f.write(f"{metric_str}\n")
                    if len(metrics) > 10:
                        f.write(f"  ... (ì´ {len(metrics)}ê°œ ì¤‘ ìƒìœ„ 10ê°œ í‘œì‹œ)\n")
                    f.write("\n")
        else:
            f.write("ìƒì„¸ ë©”íŠ¸ë¦­ ì •ë³´: MLflowì—ì„œ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ\n")
        f.write("\n")
        
        # === 7. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê³¼ì • - ê°œì„ ë¨ ===
        f.write("=== í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê³¼ì • ===\n")
        if config:
            # íŠœë‹ ì„¤ì • ì •ë³´
            tuning_config = config.get('tuning', {})
            f.write("íŠœë‹ ì„¤ì •:\n")
            f.write(f"  - ì´ ì‹œë„ íšŸìˆ˜: {tuning_config.get('n_trials', 'N/A')}\n")
            f.write(f"  - ìµœì í™” ë°©í–¥: {tuning_config.get('direction', 'N/A')}\n")
            f.write(f"  - ì£¼ìš” ë©”íŠ¸ë¦­: {tuning_config.get('metric', 'N/A')}\n")
            f.write(f"  - íƒ€ì„ì•„ì›ƒ: {tuning_config.get('timeout', 'N/A')}ì´ˆ\n")
            
            # íŠœë‹ ë²”ìœ„ ì •ë³´ - ê°œì„ ë¨
            f.write("íŠœë‹ ë²”ìœ„:\n")
            if model_type == 'xgboost' and 'xgboost_params' in config:
                xgb_params = config['xgboost_params']
                for param, param_config in xgb_params.items():
                    if isinstance(param_config, dict):
                        if 'low' in param_config and 'high' in param_config:
                            low_val = param_config['low']
                            high_val = param_config['high']
                            log_scale = param_config.get('log', False)
                            if log_scale:
                                f.write(f"  - {param}: ë¡œê·¸ ìŠ¤ì¼€ì¼ [{low_val}, {high_val}]\n")
                            else:
                                f.write(f"  - {param}: [{low_val}, {high_val}]\n")
                        elif 'choices' in param_config:
                            f.write(f"  - {param}: {param_config['choices']}\n")
            elif model_type == 'catboost' and 'catboost_params' in config:
                cb_params = config['catboost_params']
                for param, param_config in cb_params.items():
                    if isinstance(param_config, dict):
                        if 'low' in param_config and 'high' in param_config:
                            low_val = param_config['low']
                            high_val = param_config['high']
                            log_scale = param_config.get('log', False)
                            if log_scale:
                                f.write(f"  - {param}: ë¡œê·¸ ìŠ¤ì¼€ì¼ [{low_val}, {high_val}]\n")
                            else:
                                f.write(f"  - {param}: [{low_val}, {high_val}]\n")
                        elif 'choices' in param_config:
                            f.write(f"  - {param}: {param_config['choices']}\n")
            elif model_type == 'lightgbm' and 'lightgbm_params' in config:
                lgb_params = config['lightgbm_params']
                for param, param_config in lgb_params.items():
                    if isinstance(param_config, dict):
                        if 'low' in param_config and 'high' in param_config:
                            low_val = param_config['low']
                            high_val = param_config['high']
                            log_scale = param_config.get('log', False)
                            if log_scale:
                                f.write(f"  - {param}: ë¡œê·¸ ìŠ¤ì¼€ì¼ [{low_val}, {high_val}]\n")
                            else:
                                f.write(f"  - {param}: [{low_val}, {high_val}]\n")
                        elif 'choices' in param_config:
                            f.write(f"  - {param}: {param_config['choices']}\n")
            elif model_type == 'random_forest' and 'random_forest_params' in config:
                rf_params = config['random_forest_params']
                for param, param_config in rf_params.items():
                    if isinstance(param_config, dict):
                        if 'low' in param_config and 'high' in param_config:
                            low_val = param_config['low']
                            high_val = param_config['high']
                            log_scale = param_config.get('log', False)
                            if log_scale:
                                f.write(f"  - {param}: ë¡œê·¸ ìŠ¤ì¼€ì¼ [{low_val}, {high_val}]\n")
                            else:
                                f.write(f"  - {param}: [{low_val}, {high_val}]\n")
                        elif 'choices' in param_config:
                            f.write(f"  - {param}: {param_config['choices']}\n")
            
            # íŠœë‹ ë²”ìœ„ê°€ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ê°’ í‘œì‹œ
            if not any(key in config for key in ['xgboost_params', 'catboost_params', 'lightgbm_params', 'random_forest_params']):
                f.write("  - íŠœë‹ ë²”ìœ„: ê¸°ë³¸ Optuna ì„¤ì • ì‚¬ìš©\n")
        else:
            f.write("íŠœë‹ ê³¼ì • ì •ë³´: N/A\n")
        f.write("\n")
        
        # === 8. êµì°¨ ê²€ì¦ ìƒì„¸ ê²°ê³¼ - ê°œì„ ë¨ ===
        f.write("=== êµì°¨ ê²€ì¦ ìƒì„¸ ê²°ê³¼ ===\n")
        if detailed_metrics:
            # í´ë“œë³„ ì„±ëŠ¥ ì¶”ì¶œ (MLflowì—ì„œ í´ë“œë³„ ë©”íŠ¸ë¦­ì´ ìˆë‹¤ë©´)
            fold_metrics = {}
            cv_stats = {}
            
            # í´ë“œë³„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ - ê°œì„ ëœ ë¡œì§
            for key, value in detailed_metrics.items():
                # fold_ë¡œ ì‹œì‘í•˜ëŠ” ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                if key.startswith('fold_'):
                    parts = key.split('_')
                    if len(parts) >= 3:
                        fold_num = parts[1]
                        metric_name = '_'.join(parts[2:])
                        if fold_num not in fold_metrics:
                            fold_metrics[fold_num] = {}
                        fold_metrics[fold_num][metric_name] = value
                
                # trial_fold_ë¡œ ì‹œì‘í•˜ëŠ” ë©”íŠ¸ë¦­ë„ ìˆ˜ì§‘
                elif 'trial_' in key and 'fold_' in key:
                    parts = key.split('_')
                    if len(parts) >= 4:
                        trial_num = parts[1]
                        fold_num = parts[3]
                        metric_name = '_'.join(parts[4:])
                        fold_key = f"trial_{trial_num}_fold_{fold_num}"
                        if fold_key not in fold_metrics:
                            fold_metrics[fold_key] = {}
                        fold_metrics[fold_key][metric_name] = value
            
            # êµì°¨ ê²€ì¦ í†µê³„ ìˆ˜ì§‘ (mean, std í˜•íƒœ)
            for key, value in detailed_metrics.items():
                if '_mean' in key or '_std' in key:
                    cv_stats[key] = value
            
            if fold_metrics:
                f.write("í´ë“œë³„ ì„±ëŠ¥:\n")
                for fold_key in sorted(fold_metrics.keys()):
                    fold_data = fold_metrics[fold_key]
                    f.write(f"  - {fold_key}: ")
                    metrics_str = []
                    
                    # ì£¼ìš” ë©”íŠ¸ë¦­ë“¤ì„ ìš°ì„ ì ìœ¼ë¡œ í‘œì‹œ
                    priority_metrics = ['f1', 'accuracy', 'roc_auc', 'precision', 'recall', 'balanced_accuracy']
                    for metric in priority_metrics:
                        if metric in fold_data:
                            value = fold_data[metric]
                            if isinstance(value, float):
                                metrics_str.append(f"{metric.upper()}={value:.4f}")
                            else:
                                metrics_str.append(f"{metric.upper()}={value}")
                    
                    # ì£¼ìš” ë©”íŠ¸ë¦­ì´ ì—†ìœ¼ë©´ ë‹¤ë¥¸ ë©”íŠ¸ë¦­ë“¤ í‘œì‹œ
                    if not metrics_str:
                        for metric, value in fold_data.items():
                            if isinstance(value, (int, float)):
                                metrics_str.append(f"{metric}={value:.4f}")
                            else:
                                metrics_str.append(f"{metric}={value}")
                            if len(metrics_str) >= 5:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                                break
                    
                    f.write(", ".join(metrics_str) + "\n")
                
                # êµì°¨ ê²€ì¦ í†µê³„ ê³„ì‚° ë° ì¶œë ¥
                f.write("êµì°¨ ê²€ì¦ í†µê³„:\n")
                for metric in ['f1', 'accuracy', 'roc_auc', 'precision', 'recall', 'balanced_accuracy']:
                    values = []
                    for fold_data in fold_metrics.values():
                        if metric in fold_data:
                            values.append(fold_data[metric])
                    if values:
                        mean_val = sum(values) / len(values)
                        std_val = (sum((x - mean_val) ** 2 for x in values) / len(values)) ** 0.5
                        f.write(f"  - {metric.upper()} í‰ê·  Â± í‘œì¤€í¸ì°¨: {mean_val:.4f} Â± {std_val:.4f}\n")
                
                # MLflowì—ì„œ ê°€ì ¸ì˜¨ í†µê³„ ì •ë³´ë„ ì¶œë ¥
                if cv_stats:
                    f.write("MLflow êµì°¨ ê²€ì¦ í†µê³„:\n")
                    # ì£¼ìš” í†µê³„ë§Œ í‘œì‹œ (ë„ˆë¬´ ë§ìœ¼ë©´ ê°€ë…ì„± ì €í•˜)
                    priority_stats = ['accuracy_mean', 'f1_mean', 'roc_auc_mean', 'precision_mean', 'recall_mean', 
                                    'balanced_accuracy_mean', 'pr_auc_mean']
                    displayed_stats = 0
                    for key, value in cv_stats.items():
                        if any(priority in key for priority in priority_stats):
                            if isinstance(value, (int, float)):
                                f.write(f"  - {key}: {value:.4f}\n")
                            else:
                                f.write(f"  - {key}: {value}\n")
                            displayed_stats += 1
                            if displayed_stats >= 10:  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
                                break
                    
                    # í‘œì‹œë˜ì§€ ì•Šì€ í†µê³„ê°€ ìˆìœ¼ë©´ ê°œìˆ˜ í‘œì‹œ
                    remaining_stats = len(cv_stats) - displayed_stats
                    if remaining_stats > 0:
                        f.write(f"  ... (ì´ {len(cv_stats)}ê°œ ì¤‘ {displayed_stats}ê°œ í‘œì‹œ, {remaining_stats}ê°œ ìƒëµ)\n")
            else:
                f.write("í´ë“œë³„ ìƒì„¸ ì„±ëŠ¥: MLflowì—ì„œ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ\n")
        else:
            f.write("êµì°¨ ê²€ì¦ ê²°ê³¼: N/A\n")
        f.write("\n")
        
        # === 9. ëª¨ë¸ íŠ¹ì„± ë¶„ì„ - ê°œì„ ë¨ ===
        f.write("=== ëª¨ë¸ íŠ¹ì„± ë¶„ì„ ===\n")
        if detailed_metrics:
            # í”¼ì²˜ ì¤‘ìš”ë„ ì •ë³´
            feature_importance = {}
            for key, value in detailed_metrics.items():
                if key.startswith('feature_importance_'):
                    feature_name = key.replace('feature_importance_', '')
                    feature_importance[feature_name] = value
            
            if feature_importance:
                f.write("í”¼ì²˜ ì¤‘ìš”ë„ (ìƒìœ„ 10ê°œ):\n")
                sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:10]
                for i, (feature, importance) in enumerate(sorted_features, 1):
                    f.write(f"  {i}. {feature}: {importance:.4f}\n")
            
            # ëª¨ë¸ ë³µì¡ë„ ì •ë³´
            model_complexity = {}
            complexity_metrics = ['total_trees', 'max_depth', 'avg_depth', 'total_leaves', 'best_iteration']
            for metric in complexity_metrics:
                if metric in detailed_metrics:
                    model_complexity[metric] = detailed_metrics[metric]
            
            if model_complexity:
                f.write("ëª¨ë¸ ë³µì¡ë„:\n")
                for metric, value in model_complexity.items():
                    f.write(f"  - {metric}: {value}\n")
            
            # í•™ìŠµ ê³¡ì„  ì •ë³´
            if 'best_iteration' in detailed_metrics:
                f.write("í•™ìŠµ ê³¡ì„ :\n")
                f.write(f"  - ìµœì  ë°˜ë³µ íšŸìˆ˜: {detailed_metrics['best_iteration']}\n")
                if 'early_stopping_rounds' in detailed_metrics:
                    f.write(f"  - Early Stopping ë¼ìš´ë“œ: {detailed_metrics['early_stopping_rounds']}\n")
            
            # Early Stopping ì‚¬ìš© ì—¬ë¶€
            early_stopping_used = False
            for key, value in detailed_metrics.items():
                if 'early_stopping_used' in key and value == 1:
                    early_stopping_used = True
                    break
            if early_stopping_used:
                f.write("  - Early Stopping: ì‚¬ìš©ë¨\n")
            else:
                f.write("  - Early Stopping: ì‚¬ìš©ë˜ì§€ ì•ŠìŒ\n")
        else:
            f.write("ëª¨ë¸ íŠ¹ì„± ë¶„ì„: N/A\n")
        f.write("\n")
        
        # === 10. ë°ì´í„° í’ˆì§ˆ ë° ì „ì²˜ë¦¬ ì •ë³´ ===
        f.write("=== ë°ì´í„° í’ˆì§ˆ ë° ì „ì²˜ë¦¬ ===\n")
        if config:
            f.write("ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸:\n")
            if 'preprocessing' in config:
                preproc_config = config['preprocessing']
                f.write(f"  - ê²°ì¸¡ì¹˜ ì²˜ë¦¬: {preproc_config.get('missing_value_strategy', 'N/A')}\n")
                f.write(f"  - ë²”ì£¼í˜• ì¸ì½”ë”©: {preproc_config.get('categorical_encoding', 'N/A')}\n")
                f.write(f"  - ìŠ¤ì¼€ì¼ë§: {preproc_config.get('scaling', 'N/A')}\n")
                f.write(f"  - ë°ì´í„° íƒ€ì…: {preproc_config.get('dtype', 'N/A')}\n")
            else:
                f.write("  - ì „ì²˜ë¦¬ ì„¤ì •: ê¸°ë³¸ê°’ ì‚¬ìš©\n")
            
            # ë°ì´í„° í’ˆì§ˆ ì •ë³´ (ê°€ëŠ¥í•œ ê²½ìš°)
            if data_info:
                f.write("ë°ì´í„° í’ˆì§ˆ:\n")
                f.write(f"  - ë°ì´í„° ì¼ê´€ì„±: ì–‘í˜¸\n")
                if 'total_rows' in data_info and 'total_columns' in data_info:
                    f.write(f"  - ë°ì´í„° í¬ê¸°: {data_info['total_rows']} í–‰ Ã— {data_info['total_columns']} ì—´\n")
                if 'unique_ids' in data_info:
                    f.write(f"  - ê³ ìœ  ê°œì¸ ìˆ˜: {data_info['unique_ids']}ëª…\n")
        else:
            f.write("ë°ì´í„° í’ˆì§ˆ ë° ì „ì²˜ë¦¬ ì •ë³´: N/A\n")
        f.write("\n")
        
        # === 11. ì‹¤í—˜ í™˜ê²½ ì •ë³´ ===
        f.write("=== ì‹¤í—˜ í™˜ê²½ ===\n")
        f.write("ì‹œìŠ¤í…œ ì •ë³´:\n")
        f.write(f"  - Python ë²„ì „: {system_info['python_version']}\n")
        f.write(f"  - í”Œë«í¼: {system_info['platform']}\n")
        f.write(f"  - CPU ì½”ì–´ ìˆ˜: {system_info['cpu_count']}\n")
        f.write(f"  - ì´ ë©”ëª¨ë¦¬: {system_info['memory_total_gb']}GB\n")
        f.write(f"  - ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: {system_info['memory_available_gb']}GB\n")
        
        # ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ (ê°€ëŠ¥í•œ ê²½ìš°)
        try:
            import xgboost as xgb
            import catboost as cb
            import lightgbm as lgb
            import sklearn
            import optuna
            
            f.write("ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „:\n")
            f.write(f"  - XGBoost: {xgb.__version__}\n")
            f.write(f"  - CatBoost: {cb.__version__}\n")
            f.write(f"  - LightGBM: {lgb.__version__}\n")
            f.write(f"  - scikit-learn: {sklearn.__version__}\n")
            f.write(f"  - Optuna: {optuna.__version__}\n")
        except ImportError:
            f.write("ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „: ì¼ë¶€ ëª¨ë“ˆì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ\n")
        
        # ì„¤ì • íŒŒì¼ ì •ë³´
        if config:
            f.write("ì„¤ì • íŒŒì¼:\n")
            if 'config_files' in config:
                for config_type, config_path in config['config_files'].items():
                    f.write(f"  - {config_type}: {config_path}\n")
            else:
                f.write("  - ì„¤ì • íŒŒì¼ ê²½ë¡œ: ConfigManagerì—ì„œ ìë™ ìƒì„±\n")
        f.write("\n")
        
        # === 12. ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ - ê°œì„ ë¨ ===
        f.write("=== ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ ===\n")
        if isinstance(result, tuple) and len(result) == 2:
            # ì¼ë°˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼
            best_params, best_score = result
            f.write(f"ğŸ¯ ìµœê³  ì„±ëŠ¥: {best_score:.4f}\n")
            f.write(f"ğŸ“Š ìµœì í™” ë©”íŠ¸ë¦­: {tuning_config.get('metric', 'N/A')}\n")
            f.write(f"ğŸ”„ ì´ ì‹œë„ íšŸìˆ˜: {tuning_config.get('n_trials', 'N/A')}\n")
            f.write(f"â±ï¸  ìµœì í™” ë°©í–¥: {tuning_config.get('direction', 'N/A')}\n")
            f.write("\n")
            f.write("âš™ï¸ ìµœì  íŒŒë¼ë¯¸í„°:\n")
            for param, value in best_params.items():
                if isinstance(value, float):
                    f.write(f"  - {param}: {value:.6f}\n")
                else:
                    f.write(f"  - {param}: {value}\n")
            
            # êµì°¨ ê²€ì¦ ì„±ëŠ¥ ìš”ì•½
            if detailed_metrics:
                cv_accuracy = None
                cv_f1 = None
                cv_roc_auc = None
                
                for key, value in detailed_metrics.items():
                    if key.endswith('_accuracy_mean'):
                        cv_accuracy = value
                    elif key.endswith('_f1_mean'):
                        cv_f1 = value
                    elif key.endswith('_roc_auc_mean'):
                        cv_roc_auc = value
                
                f.write("\nğŸ“ˆ êµì°¨ ê²€ì¦ ì„±ëŠ¥ ìš”ì•½:\n")
                if cv_accuracy is not None:
                    f.write(f"  - Accuracy: {cv_accuracy:.4f}\n")
                if cv_f1 is not None:
                    f.write(f"  - F1-Score: {cv_f1:.4f}\n")
                if cv_roc_auc is not None:
                    f.write(f"  - ROC-AUC: {cv_roc_auc:.4f}\n")
        else:
            # ë¦¬ìƒ˜í”Œë§ ë¹„êµ ê²°ê³¼
            f.write("ğŸ”„ ë¦¬ìƒ˜í”Œë§ ë¹„êµ ê²°ê³¼:\n")
            f.write(f"ğŸ† ìµœê³  ì„±ëŠ¥ ê¸°ë²•: {result.get('best_method', 'none')}\n")
            f.write(f"ğŸ“Š ìµœê³  ì„±ëŠ¥: {result.get('best_score', 0.0):.4f}\n")
            f.write("\nğŸ“‹ ê° ê¸°ë²•ë³„ ì„±ëŠ¥:\n")
            for method, method_result in result.get('methods', {}).items():
                f.write(f"  - {method}: {method_result['best_score']:.4f}\n")
        
        # ì‹¤í—˜ ì¢…ë£Œ ì‹œê°„ ë° ì†Œìš” ì‹œê°„
        end_time = time.time()
        if 'start_time' in locals():
            elapsed_time = end_time - start_time
            f.write(f"\nâ° ì‹¤í—˜ ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ ({elapsed_time/60:.2f}ë¶„)\n")
        
        f.write(f"\nğŸ“… ì‹¤í—˜ ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"ğŸ”— MLflow ë§í¬: {mlflow_link}\n")
    
    logger.info(f"ì‹¤í—˜ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    logger.info(f"MLflow Experiment ID: {experiment_id}")
    logger.info(f"MLflow Run ID: {run_id}")
    logger.info(f"MLflow ë§í¬: {mlflow_link}")
    return filename


def collect_data_info(df: pd.DataFrame, config: Dict[str, Any], train_val_df: pd.DataFrame = None, test_df: pd.DataFrame = None) -> Dict[str, Any]:
    """
    ì‹¤í—˜ì— ì‚¬ìš©ëœ ë°ì´í„°ì˜ ìƒì„¸ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    
    Args:
        df: ì „ì²´ ë°ì´í„°í”„ë ˆì„
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        train_val_df: í›ˆë ¨/ê²€ì¦ ë°ì´í„°í”„ë ˆì„ (ì„ íƒì‚¬í•­)
        test_df: í…ŒìŠ¤íŠ¸ ë°ì´í„°í”„ë ˆì„ (ì„ íƒì‚¬í•­)
        
    Returns:
        ë°ì´í„° ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    data_info = {}
    
    # ê¸°ë³¸ ë°ì´í„° ì •ë³´
    data_info['total_rows'] = len(df)
    data_info['total_columns'] = len(df.columns)
    
    # ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    if 'data' in config:
        data_info['data_path'] = config['data'].get('data_path', 'N/A')
    
    # ê°œì¸ ìˆ˜ (ID ì»¬ëŸ¼ ê¸°ì¤€)
    id_column = config.get('time_series', {}).get('id_column', 'id')
    if id_column in df.columns:
        data_info['unique_ids'] = df[id_column].nunique()
    
    # ë°ì´í„° ê¸°ê°„
    date_column = config.get('time_series', {}).get('date_column', 'dov')
    year_column = config.get('time_series', {}).get('year_column', 'yov')
    
    if date_column in df.columns:
        try:
            df[date_column] = pd.to_datetime(df[date_column])
            date_range = f"{df[date_column].min().strftime('%Y-%m-%d')} ~ {df[date_column].max().strftime('%Y-%m-%d')}"
            data_info['date_range'] = date_range
        except:
            data_info['date_range'] = 'N/A'
    elif year_column in df.columns:
        try:
            year_range = f"{df[year_column].min()} ~ {df[year_column].max()}"
            data_info['date_range'] = year_range
        except:
            data_info['date_range'] = 'N/A'
    else:
        data_info['date_range'] = 'N/A'
    
    # í´ë˜ìŠ¤ ë¶„í¬ ì •ë³´
    class_distributions = {}
    target_columns = config.get('features', {}).get('target_columns', [])
    
    # ì „ì²´ ë°ì´í„°ì˜ í´ë˜ìŠ¤ ë¶„í¬
    for target in target_columns:
        if target in df.columns:
            class_distributions[f'full_{target}'] = df[target].value_counts().to_dict()
    
    # í›ˆë ¨/ê²€ì¦ ë°ì´í„°ì˜ í´ë˜ìŠ¤ ë¶„í¬
    if train_val_df is not None:
        train_original = {}
        for target in target_columns:
            if target in train_val_df.columns:
                train_original[target] = train_val_df[target].value_counts().to_dict()
        class_distributions['train_original'] = train_original
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ í´ë˜ìŠ¤ ë¶„í¬
    if test_df is not None:
        test_dist = {}
        for target in target_columns:
            if target in test_df.columns:
                test_dist[target] = test_df[target].value_counts().to_dict()
        class_distributions['test'] = test_dist
    
    data_info['class_distributions'] = class_distributions
    
    return data_info








def main():
    """ë©”ì¸ í•¨ìˆ˜ (ConfigManager ê¸°ë°˜ ê³„ì¸µì  config ë³‘í•© ì§€ì›)"""
    parser = argparse.ArgumentParser(
        description="í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í–‰ (ConfigManager ì§€ì›)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ê³„ì¸µì  configë¡œ íŠœë‹ ì‹¤í–‰
  python scripts/run_hyperparameter_tuning.py --model-type xgboost --experiment-type hyperparameter_tuning
  # legacy ë‹¨ì¼ íŒŒì¼ configë¡œ íŠœë‹ ì‹¤í–‰
  python scripts/run_hyperparameter_tuning.py --tuning_config configs/hyperparameter_tuning.yaml --base_config configs/default_config.yaml
        """
    )
    parser.add_argument("--model-type", type=str, choices=['xgboost', 'lightgbm', 'random_forest', 'catboost'], default=None, help="ì‚¬ìš©í•  ëª¨ë¸ íƒ€ì… (ê³„ì¸µì  config ë³‘í•©)")
    parser.add_argument("--experiment-type", type=str, default=None, help="ì‹¤í—˜ íƒ€ì… (focal_loss, resampling, hyperparameter_tuning ë“±)")
    parser.add_argument("--tuning_config", type=str, default=None, help="(legacy) íŠœë‹ ì„¤ì • íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--base_config", type=str, default=None, help="(legacy) ê¸°ë³¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--data_path", type=str, default=None, help="ë°ì´í„° íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--nrows", type=int, default=None, help="ì‚¬ìš©í•  ë°ì´í„° í–‰ ìˆ˜")

    parser.add_argument("--mlflow_ui", action="store_true", help="íŠœë‹ ì™„ë£Œ í›„ MLflow UI ì‹¤í–‰")
    
    # ìë™í™”ëœ ì‹¤í—˜ì„ ìœ„í•œ ì¶”ê°€ ì¸ìë“¤
    parser.add_argument("--split-strategy", type=str, choices=['group_kfold', 'time_series_walk_forward', 'time_series_group_kfold'], default=None, help="ë°ì´í„° ë¶„í•  ì „ëµ")
    parser.add_argument("--cv-folds", type=int, default=None, help="êµì°¨ ê²€ì¦ í´ë“œ ìˆ˜")
    parser.add_argument("--n-trials", type=int, default=None, help="í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œë„ íšŸìˆ˜")
    parser.add_argument("--tuning-direction", type=str, choices=['maximize', 'minimize'], default=None, help="íŠœë‹ ë°©í–¥ (maximize/minimize)")
    parser.add_argument("--primary-metric", type=str, default=None, help="ì£¼ìš” í‰ê°€ ì§€í‘œ (f1, precision, recall, mcc, roc_auc, pr_auc ë“±)")
    parser.add_argument("--test-size", type=float, default=None, help="í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¹„ìœ¨ (0.0-1.0)")
    parser.add_argument("--random-state", type=int, default=None, help="ëœë¤ ì‹œë“œ")
    parser.add_argument("--n-jobs", type=int, default=None, help="ë³‘ë ¬ ì²˜ë¦¬ ì‘ì—… ìˆ˜")
    parser.add_argument("--timeout", type=int, default=None, help="íŠœë‹ íƒ€ì„ì•„ì›ƒ (ì´ˆ)")
    parser.add_argument("--early-stopping", action="store_true", help="Early stopping í™œì„±í™”")
    parser.add_argument("--early-stopping-rounds", type=int, default=None, help="Early stopping ë¼ìš´ë“œ ìˆ˜")
    parser.add_argument("--feature-selection", action="store_true", help="í”¼ì²˜ ì„ íƒ í™œì„±í™”")
    parser.add_argument("--feature-selection-method", type=str, choices=['mutual_info', 'chi2', 'f_classif', 'recursive'], default=None, help="í”¼ì²˜ ì„ íƒ ë°©ë²•")
    parser.add_argument("--feature-selection-k", type=int, default=None, help="ì„ íƒí•  í”¼ì²˜ ìˆ˜")

    parser.add_argument("--experiment-name", type=str, default=None, help="MLflow ì‹¤í—˜ ì´ë¦„ (ê¸°ë³¸ê°’: model_type_experiment_type)")
    parser.add_argument("--save-model", action="store_true", help="ìµœì  ëª¨ë¸ ì €ì¥")
    parser.add_argument("--save-predictions", action="store_true", help="ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥")
    parser.add_argument("--verbose", type=int, choices=[0, 1, 2], default=1, help="ë¡œê·¸ ë ˆë²¨ (0: ìµœì†Œ, 1: ê¸°ë³¸, 2: ìƒì„¸)")
    args = parser.parse_args()
    try:
        if args.model_type:
            # ConfigManager ê¸°ë°˜ ê³„ì¸µì  config ë³‘í•©
            config_manager = ConfigManager()
            config = config_manager.create_experiment_config(args.model_type, args.experiment_type)
            
            # ëª…ë ¹í–‰ ì¸ìë¥¼ configì— ì ìš©
            config = config_manager.apply_command_line_args(config, args)
            
            # ì„¤ì • ìš”ì•½ ì¶œë ¥
            config_manager.print_config_summary(config)
            
            result = run_hyperparameter_tuning_with_config(
                config,
                data_path=args.data_path,
                nrows=args.nrows
            )
            # ì‹¤í—˜ ê²°ê³¼ ì €ì¥ì€ run_hyperparameter_tuning_with_config ë‚´ë¶€ì—ì„œ ì²˜ë¦¬ë¨
        elif args.tuning_config and args.base_config:
            # legacy: ë‹¨ì¼ íŒŒì¼ ê¸°ë°˜ (ë¦¬ìƒ˜í”Œë§ ê¸°ëŠ¥ ì œê±°ë¨)
            raise NotImplementedError("Legacy ëª¨ë“œëŠ” ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ConfigManagerë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")
        else:
            raise ValueError("model-type ë˜ëŠ” tuning_config+base_config ì¤‘ í•˜ë‚˜ëŠ” ë°˜ë“œì‹œ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
        best_params, best_score, _, _ = result
        print(f"\nğŸ‰ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ğŸ“Š ìµœê³  ì„±ëŠ¥: {best_score:.4f}")
        print(f"âš™ï¸  ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")
        if args.mlflow_ui:
            print(f"\nğŸŒ MLflow UIë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤...")
            print(f"   ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:5000 ìœ¼ë¡œ ì ‘ì†í•˜ì„¸ìš”.")
            print(f"   ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”.")
            import subprocess
            try:
                subprocess.run(["mlflow", "ui"], check=True)
            except KeyboardInterrupt:
                print(f"\nğŸ‘‹ MLflow UIë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            except Exception as e:
                print(f"âŒ MLflow UI ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
    except Exception as e:
        logger.error(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤íŒ¨: {str(e)}")
        # robust error logging (MLflowë¥¼ í†µí•´ ìë™ìœ¼ë¡œ ë¡œê¹…ë¨)
        logger.error(f"ì‹¤í—˜ ì‹¤íŒ¨ - ëª¨ë¸: {args.model_type if 'args' in locals() and args.model_type else 'unknown'}, ì‹¤í—˜: {args.experiment_type if 'args' in locals() and args.experiment_type else 'unknown'}")
        sys.exit(1)


if __name__ == "__main__":
    main() 