#!/usr/bin/env python3
"""
í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

Optunaë¥¼ í™œìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
MLflowì™€ ì—°ë™ë˜ì–´ ì‹¤í—˜ ì¶”ì ì´ ê°€ëŠ¥í•˜ë©°, ë‹¤ì–‘í•œ íŠœë‹ ì „ëµì„ ì§€ì›í•©ë‹ˆë‹¤.
ë¦¬ìƒ˜í”Œë§ ê¸°ë²• ë¹„êµ ì‹¤í—˜ ê¸°ëŠ¥ë„ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
"""

import argparse
import logging
import sys
from pathlib import Path
import mlflow
import yaml
import pandas as pd
import numpy as np
from typing import Dict, Any, List

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from src.hyperparameter_tuning import HyperparameterTuner, save_tuning_log
from src.splits import (
    load_config, 
    split_test_set, 
    validate_splits, 
    log_splits_info
)
from src.utils.config_manager import ConfigManager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def setup_mlflow_experiment(experiment_name: str):
    """
    MLflow ì‹¤í—˜ì„ ì„¤ì •í•©ë‹ˆë‹¤.
    
    Args:
        experiment_name: ì‹¤í—˜ ì´ë¦„
        
    Returns:
        experiment_id: ì‹¤í—˜ ID
    """
    mlflow.set_experiment(experiment_name)
    experiment = mlflow.get_experiment_by_name(experiment_name)
    if experiment is None:
        experiment_id = mlflow.create_experiment(experiment_name)
    else:
        experiment_id = experiment.experiment_id
    
    logger.info(f"MLflow ì‹¤í—˜ ì„¤ì •: {experiment_name} (ID: {experiment_id})")
    return experiment_id


def log_tuning_params(tuning_config: Dict[str, Any], base_config: Dict[str, Any]):
    """
    íŠœë‹ íŒŒë¼ë¯¸í„°ë¥¼ MLflowì— ë¡œê¹…í•©ë‹ˆë‹¤.
    
    Args:
        tuning_config: íŠœë‹ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        base_config: ê¸°ë³¸ ì„¤ì • ë”•ì…”ë„ˆë¦¬
    """
    try:
        # íŠœë‹ íŒŒë¼ë¯¸í„° (ì•ˆì „í•œ ì ‘ê·¼)
        tuning_params = {}
        
        # n_trials
        n_trials = tuning_config.get('tuning', {}).get('n_trials')
        if n_trials is not None:
            tuning_params["n_trials"] = n_trials
        
        # timeout
        timeout = tuning_config.get('tuning', {}).get('timeout')
        if timeout is not None:
            tuning_params["timeout"] = timeout
        
        # sampler
        sampler_type = tuning_config.get('sampler', {}).get('type')
        if sampler_type is not None:
            tuning_params["sampler"] = sampler_type
        
        # optimization_direction
        optimization_direction = tuning_config.get('evaluation', {}).get('optimization_direction')
        if optimization_direction is not None:
            tuning_params["optimization_direction"] = optimization_direction
        
        # primary_metric
        primary_metric = tuning_config.get('evaluation', {}).get('primary_metric')
        if primary_metric is not None:
            tuning_params["primary_metric"] = primary_metric
        
        # XGBoost íŒŒë¼ë¯¸í„° ë²”ìœ„
        xgboost_params = tuning_config.get('xgboost_params', {})
        for param, param_config in xgboost_params.items():
            if isinstance(param_config, dict):
                if 'range' in param_config and len(param_config['range']) >= 2:
                    tuning_params[f"tune_{param}_min"] = param_config['range'][0]
                    tuning_params[f"tune_{param}_max"] = param_config['range'][1]
                elif 'choices' in param_config:
                    tuning_params[f"tune_{param}_choices"] = str(param_config['choices'])
        
        # ê¸°ë³¸ ì„¤ì •ì—ì„œ ë¦¬ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        resampling_config = base_config.get('resampling', {})
        resampling_params = {
            "resampling_enabled": resampling_config.get('enabled', False),
            "resampling_method": resampling_config.get('method', 'none'),
            "resampling_random_state": resampling_config.get('random_state', 42)
        }
        
        # ëª¨ë“  íŒŒë¼ë¯¸í„° ë¡œê¹…
        all_params = {**tuning_params, **resampling_params}
        
        # MLflowì— íŒŒë¼ë¯¸í„° ë¡œê¹… (ê° íŒŒë¼ë¯¸í„°ë³„ë¡œ ê°œë³„ ì²˜ë¦¬)
        for param_name, param_value in all_params.items():
            if param_value is not None:
                try:
                    mlflow.log_param(param_name, param_value)
                except Exception as e:
                    logger.warning(f"MLflow íŒŒë¼ë¯¸í„° ë¡œê¹… ì‹¤íŒ¨ ({param_name}): {e}")
        
        logger.info(f"íŠœë‹ íŒŒë¼ë¯¸í„° ë¡œê¹… ì™„ë£Œ: {len(all_params)}ê°œ íŒŒë¼ë¯¸í„°")
        
    except Exception as e:
        logger.warning(f"íŠœë‹ íŒŒë¼ë¯¸í„° ë¡œê¹… ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        # ì˜ˆì™¸ê°€ ë°œìƒí•´ë„ ì‹¤í—˜ì€ ê³„ì† ì§„í–‰


def run_resampling_tuning_comparison(tuning_config_path: str, base_config_path: str,
                                   data_path: str, resampling_methods: List[str],
                                   nrows: int = None) -> Dict[str, Any]:
    """
    ë‹¤ì–‘í•œ ë¦¬ìƒ˜í”Œë§ ê¸°ë²•ì— ëŒ€í•´ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ë¹„êµí•˜ëŠ” ì‹¤í—˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    
    Args:
        tuning_config_path: íŠœë‹ ì„¤ì • íŒŒì¼ ê²½ë¡œ
        base_config_path: ê¸°ë³¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ
        data_path: ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        resampling_methods: í…ŒìŠ¤íŠ¸í•  ë¦¬ìƒ˜í”Œë§ ê¸°ë²• ë¦¬ìŠ¤íŠ¸
        nrows: ì‚¬ìš©í•  ë°ì´í„° í–‰ ìˆ˜ (Noneì´ë©´ ì „ì²´ ì‚¬ìš©)
        
    Returns:
        ë¹„êµ ì‹¤í—˜ ê²°ê³¼
    """
    logger.info("=== ë¦¬ìƒ˜í”Œë§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë¹„êµ ì‹¤í—˜ ì‹œì‘ ===")
    
    # ê¸°ë³¸ ì„¤ì • ë¡œë“œ
    base_config = load_config(base_config_path)
    
    # ë°ì´í„° ë¡œë“œ
    logger.info(f"ë°ì´í„° ë¡œë“œ ì¤‘: {data_path}")
    if nrows:
        df = pd.read_csv(data_path, nrows=nrows)
        logger.info(f"í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë¡œë“œ: {len(df):,} í–‰")
    else:
        df = pd.read_csv(data_path)
        logger.info(f"ì „ì²´ ë°ì´í„° ë¡œë“œ: {len(df):,} í–‰")
    
    # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„ë¦¬
    logger.info("=== í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„ë¦¬ ===")
    train_val_df, test_df, test_ids = split_test_set(df, base_config)
    
    # ë¶„í•  ê²€ì¦
    if not validate_splits(train_val_df, test_df, base_config):
        logger.error("ë¶„í•  ê²€ì¦ ì‹¤íŒ¨!")
        return {}
    
    comparison_results = {
        'methods': {},
        'best_method': None,
        'best_score': -np.inf,
        'summary': {}
    }
    
    for method in resampling_methods:
        logger.info(f"\n--- {method.upper()} í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘ ---")
        
        # ì„¤ì • ë³µì‚¬ ë° ë¦¬ìƒ˜í”Œë§ ì„¤ì • ì—…ë°ì´íŠ¸
        method_base_config = base_config.copy()
        method_base_config['resampling'] = {
            'enabled': method != 'none',
            'method': method if method != 'none' else 'smote',  # noneì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì„¤ì •
            'random_state': 42
        }
        
        # ë¦¬ìƒ˜í”Œë§ ê¸°ë²•ë³„ íŒŒë¼ë¯¸í„° ì„¤ì •
        if method == 'smote':
            method_base_config['resampling']['smote_k_neighbors'] = 5
        elif method == 'borderline_smote':
            method_base_config['resampling']['borderline_smote_k_neighbors'] = 5
        elif method == 'adasyn':
            method_base_config['resampling']['adasyn_k_neighbors'] = 5
        elif method == 'under_sampling':
            method_base_config['resampling']['under_sampling_strategy'] = 'random'
        elif method == 'hybrid':
            method_base_config['resampling']['hybrid_strategy'] = 'smote_tomek'
        
        # MLflow ì¤‘ì²© ì‹¤í–‰
        with mlflow.start_run(run_name=f"resampling_tuning_{method}", nested=True):
            # íŠœë‹ ì„¤ì • ë¡œë“œ
            with open(tuning_config_path, 'r', encoding='utf-8') as f:
                method_tuning_config = yaml.safe_load(f)
            
            # ì‹¤í—˜ íŒŒë¼ë¯¸í„° ë¡œê¹…
            log_tuning_params(method_tuning_config, method_base_config)
            
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë„ˆ ìƒì„± (ì„ì‹œ ì„¤ì • íŒŒì¼ ì‚¬ìš©)
            import tempfile
            import os
            
            # ì„ì‹œ ê¸°ë³¸ ì„¤ì • íŒŒì¼ ìƒì„±
            with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
                yaml.dump(method_base_config, f, default_flow_style=False)
                temp_base_config_path = f.name
            
            try:
                # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë„ˆ ìƒì„±
                tuner = HyperparameterTuner(temp_base_config_path, temp_base_config_path, nrows=nrows)
                
                # ìµœì í™” ì‹¤í–‰
                logger.info(f"{method} í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹¤í–‰ ì¤‘...")
                best_params, best_score = tuner.optimize(start_mlflow_run=False)
                
                # ê²°ê³¼ ì €ì¥
                logger.info(f"{method} íŠœë‹ ê²°ê³¼ ì €ì¥ ì¤‘...")
                tuner.save_results()
                
                # ê²°ê³¼ ìˆ˜ì§‘
                method_results = {
                    'best_params': best_params,
                    'best_score': best_score,
                    'tuning_config': method_tuning_config,
                    'base_config': method_base_config
                }
                
                comparison_results['methods'][method] = method_results
                
                # ìµœê³  ì„±ëŠ¥ ì—…ë°ì´íŠ¸
                if best_score > comparison_results['best_score']:
                    comparison_results['best_score'] = best_score
                    comparison_results['best_method'] = method
                
                logger.info(f"{method} í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ: {best_score:.4f}")
                
            finally:
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                if os.path.exists(temp_base_config_path):
                    os.unlink(temp_base_config_path)
    
    # ë¹„êµ ê²°ê³¼ ìš”ì•½
    logger.info("\n=== ë¦¬ìƒ˜í”Œë§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë¹„êµ ê²°ê³¼ ìš”ì•½ ===")
    for method, results in comparison_results['methods'].items():
        best_score = results['best_score']
        logger.info(f"{method}: ìµœê³  ì„±ëŠ¥ = {best_score:.4f}")
    
    logger.info(f"ìµœê³  ì„±ëŠ¥ ê¸°ë²•: {comparison_results['best_method']} (ì„±ëŠ¥: {comparison_results['best_score']:.4f})")
    
    return comparison_results


def validate_config_files(tuning_config_path: str, base_config_path: str):
    """
    ì„¤ì • íŒŒì¼ë“¤ì˜ ìœ íš¨ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.
    
    Args:
        tuning_config_path: íŠœë‹ ì„¤ì • íŒŒì¼ ê²½ë¡œ
        base_config_path: ê¸°ë³¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ
    """
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not Path(tuning_config_path).exists():
        raise FileNotFoundError(f"íŠœë‹ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {tuning_config_path}")
    
    if not Path(base_config_path).exists():
        raise FileNotFoundError(f"ê¸°ë³¸ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {base_config_path}")
    
    # YAML íŒŒì‹± í…ŒìŠ¤íŠ¸
    try:
        with open(tuning_config_path, 'r', encoding='utf-8') as f:
            tuning_config = yaml.safe_load(f)
        logger.info("íŠœë‹ ì„¤ì • íŒŒì¼ íŒŒì‹± ì„±ê³µ")
    except Exception as e:
        raise ValueError(f"íŠœë‹ ì„¤ì • íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
    
    try:
        with open(base_config_path, 'r', encoding='utf-8') as f:
            base_config = yaml.safe_load(f)
        logger.info("ê¸°ë³¸ ì„¤ì • íŒŒì¼ íŒŒì‹± ì„±ê³µ")
    except Exception as e:
        raise ValueError(f"ê¸°ë³¸ ì„¤ì • íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
    
    # í•„ìˆ˜ ì„¤ì • í™•ì¸
    required_tuning_keys = ['tuning', 'sampler', 'xgboost_params', 'evaluation', 'results', 'mlflow']
    for key in required_tuning_keys:
        if key not in tuning_config:
            raise ValueError(f"íŠœë‹ ì„¤ì •ì— í•„ìˆ˜ í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤: {key}")
    
    logger.info("ì„¤ì • íŒŒì¼ ê²€ì¦ ì™„ë£Œ")


def run_resampling_tuning_comparison_with_configmanager(
    model_type: str,
    experiment_type: str,
    data_path: str,
    resampling_methods: List[str],
    nrows: int = None
) -> Dict[str, Any]:
    """
    ConfigManager ê¸°ë°˜ ë‹¤ì–‘í•œ ë¦¬ìƒ˜í”Œë§ ê¸°ë²•ì— ëŒ€í•´ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ë¹„êµí•˜ëŠ” ì‹¤í—˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    logger.info("=== [ConfigManager] ë¦¬ìƒ˜í”Œë§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë¹„êµ ì‹¤í—˜ ì‹œì‘ ===")
    config_manager = ConfigManager()

    # ë°ì´í„° ë¡œë“œ
    logger.info(f"ë°ì´í„° ë¡œë“œ ì¤‘: {data_path}")
    if nrows:
        df = pd.read_csv(data_path, nrows=nrows)
        logger.info(f"í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë¡œë“œ: {len(df):,} í–‰")
    else:
        df = pd.read_csv(data_path)
        logger.info(f"ì „ì²´ ë°ì´í„° ë¡œë“œ: {len(df):,} í–‰")

    # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„ë¦¬
    logger.info("=== í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„ë¦¬ ===")
    # configëŠ” ë¦¬ìƒ˜í”Œë§ê³¼ ë¬´ê´€í•˜ê²Œ ë¶„í• ì—ë§Œ ì‚¬ìš©
    base_config = config_manager.create_experiment_config(model_type, experiment_type)
    train_val_df, test_df, test_ids = split_test_set(df, base_config)

    # ë¶„í•  ê²€ì¦
    if not validate_splits(train_val_df, test_df, base_config):
        logger.error("ë¶„í•  ê²€ì¦ ì‹¤íŒ¨!")
        return {}

    # ë°ì´í„° ì •ë³´ ìˆ˜ì§‘
    data_info = collect_data_info(df, base_config, train_val_df, test_df)
    logger.info(f"ë°ì´í„° ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {data_info['total_rows']} í–‰, {data_info['total_columns']} ì—´")

    comparison_results = {
        'methods': {},
        'best_method': None,
        'best_score': -np.inf,
        'summary': {}
    }

    # ìƒìœ„ MLflow run ì‹œì‘
    with mlflow.start_run(run_name=f"resampling_comparison_{model_type}"):
        for method in resampling_methods:
            logger.info(f"\n--- {method.upper()} í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘ ---")
            # config ìƒì„± ë° ë¦¬ìƒ˜í”Œë§ ì„¤ì • ìˆ˜ì •
            config = config_manager.create_experiment_config(model_type, experiment_type)
            config['resampling'] = {
                'enabled': method != 'none',
                'method': method if method != 'none' else 'smote',
                'random_state': 42
            }
            # ë¦¬ìƒ˜í”Œë§ ê¸°ë²•ë³„ íŒŒë¼ë¯¸í„° ì„¤ì •
            if method == 'smote':
                config['resampling']['smote_k_neighbors'] = 5
            elif method == 'borderline_smote':
                config['resampling']['borderline_smote_k_neighbors'] = 5
            elif method == 'adasyn':
                config['resampling']['adasyn_k_neighbors'] = 5
            elif method == 'under_sampling':
                config['resampling']['under_sampling_strategy'] = 'random'
            elif method == 'hybrid':
                config['resampling']['hybrid_strategy'] = 'smote_tomek'
            # ë°ì´í„° ê²½ë¡œ ë°˜ì˜
            config['data']['data_path'] = data_path
            # MLflow ì¤‘ì²© ì‹¤í–‰
            with mlflow.start_run(run_name=f"resampling_tuning_{method}", nested=True):
                # íŠœë‹ íŒŒë¼ë¯¸í„° ë¡œê¹… 
                log_tuning_params(config, config)
                
                # íŠœë„ˆ ìƒì„± ë° íŠœë‹ ì‹¤í–‰
                tuner = HyperparameterTuner(config, train_val_df, nrows=nrows)
                result = tuner.optimize(start_mlflow_run=False)
                
                # ê²°ê³¼ ì¶”ì¶œ
                if result is None:
                    best_params = {}
                    best_score = 0.0
                    logger.warning(f"{method} íŠœë‹ ê²°ê³¼ê°€ Noneì…ë‹ˆë‹¤.")
                elif isinstance(result, tuple) and len(result) >= 2:
                    best_params = result[0] if result[0] is not None else {}
                    best_score = result[1] if result[1] is not None else 0.0
                else:
                    best_params = {}
                    best_score = 0.0
                    logger.warning(f"{method} ì˜ˆìƒì¹˜ ëª»í•œ ê²°ê³¼ íƒ€ì…: {type(result)}")
                
                # ê²°ê³¼ ì €ì¥
                comparison_results['methods'][method] = {
                    'best_params': best_params,
                    'best_score': best_score
                }
                
                # ìµœê³  ì„±ëŠ¥ ì—…ë°ì´íŠ¸
                if best_score > comparison_results['best_score']:
                    comparison_results['best_score'] = best_score
                    comparison_results['best_method'] = method
                
                logger.info(f"{method} ìµœê³  ì„±ëŠ¥: {best_score:.4f}")
                logger.info(f"{method} ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")
                
                # íŠœë‹ ë¡œê·¸ ì €ì¥
                try:
                    save_tuning_log(
                        result=(best_params, best_score),
                        model_type=model_type,
                        experiment_type=experiment_type,
                        nrows=nrows,
                        experiment_id=mlflow.active_run().info.experiment_id,
                        run_id=mlflow.active_run().info.run_id,
                        log_file_path=getattr(tuner, 'log_file_path', None)
                    )
                except Exception as e:
                    logger.error(f"{method} íŠœë‹ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")

    # ìµœì¢… ê²°ê³¼ ìš”ì•½
    logger.info("\n=== ë¦¬ìƒ˜í”Œë§ ë¹„êµ ì‹¤í—˜ ì™„ë£Œ ===")
    logger.info(f"ìµœê³  ì„±ëŠ¥ ê¸°ë²•: {comparison_results['best_method']}")
    logger.info(f"ìµœê³  ì„±ëŠ¥: {comparison_results['best_score']:.4f}")
    logger.info("\nê° ê¸°ë²•ë³„ ì„±ëŠ¥:")
    for method, method_result in comparison_results['methods'].items():
        logger.info(f"  {method}: {method_result['best_score']:.4f}")

    # ì‹¤í—˜ ê²°ê³¼ ì €ì¥
    save_experiment_results(
        comparison_results,
        model_type,
        experiment_type,
        nrows,
        config=base_config,
        data_info=data_info
    )

    return comparison_results


def run_hyperparameter_tuning_with_config(config: Dict[str, Any], data_path: str = None, nrows: int = None, resampling_comparison: bool = False, resampling_methods: List[str] = None):
    """
    ConfigManager ê¸°ë°˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    
    Args:
        config: ì™„ì „í•œ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        data_path: ë°ì´í„° íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ configì—ì„œ ê°€ì ¸ì˜´)
        nrows: ì‚¬ìš©í•  ë°ì´í„° í–‰ ìˆ˜ (Noneì´ë©´ ì „ì²´ ì‚¬ìš©)
        resampling_comparison: ë¦¬ìƒ˜í”Œë§ ë¹„êµ ì‹¤í—˜ ì—¬ë¶€
        resampling_methods: ë¹„êµí•  ë¦¬ìƒ˜í”Œë§ ê¸°ë²•ë“¤
        
    Returns:
        íŠœë‹ ê²°ê³¼ (best_params, best_score, experiment_id, run_id) ë˜ëŠ” ë¦¬ìƒ˜í”Œë§ ë¹„êµ ê²°ê³¼
    """
    logger.info("=== ConfigManager ê¸°ë°˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘ ===")
    
    # ë°ì´í„° ê²½ë¡œ ì„¤ì •
    if data_path is None:
        data_path = config['data'].get('file_path', 'data/processed/processed_data_with_features.csv')
    
    # ë°ì´í„° ë¡œë“œ
    logger.info(f"ë°ì´í„° ë¡œë“œ ì¤‘: {data_path}")
    if nrows:
        df = pd.read_csv(data_path, nrows=nrows)
        logger.info(f"í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë¡œë“œ: {len(df):,} í–‰")
    else:
        df = pd.read_csv(data_path)
        logger.info(f"ì „ì²´ ë°ì´í„° ë¡œë“œ: {len(df):,} í–‰")
    
    # ë°ì´í„° ì •ë³´ ìˆ˜ì§‘
    data_info = collect_data_info(df, config)
    logger.info(f"ë°ì´í„° ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {data_info['total_rows']} í–‰, {data_info['total_columns']} ì—´")
    
    # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„ë¦¬
    logger.info("=== í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„ë¦¬ ===")
    train_val_df, test_df, test_ids = split_test_set(df, config)
    
    # ë°ì´í„° ì •ë³´ ì—…ë°ì´íŠ¸ (í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì •ë³´ ì¶”ê°€)
    data_info = collect_data_info(df, config, train_val_df, test_df)
    
    # ë¶„í•  ê²€ì¦
    if not validate_splits(train_val_df, test_df, config):
        logger.error("ë¶„í•  ê²€ì¦ ì‹¤íŒ¨!")
        return None
    
    # ë¦¬ìƒ˜í”Œë§ ë¹„êµ ì‹¤í—˜
    if resampling_comparison:
        logger.info("=== ë¦¬ìƒ˜í”Œë§ ë¹„êµ ì‹¤í—˜ ì‹œì‘ ===")
        if resampling_methods is None:
            resampling_methods = ['none', 'smote', 'borderline_smote', 'adasyn', 'under_sampling', 'hybrid']
        
        result = run_resampling_tuning_comparison_with_configmanager(
            config.get('model', {}).get('model_type', 'xgboost'),
            'hyperparameter_tuning',
            data_path,
            resampling_methods,
            nrows
        )
        
        # ì‹¤í—˜ ê²°ê³¼ ì €ì¥ (ë¦¬ìƒ˜í”Œë§ ë¹„êµ)
        save_experiment_results(
            result, 
            config.get('model', {}).get('model_type', 'xgboost'), 
            'hyperparameter_tuning', 
            nrows, 
            config=config, 
            data_info=data_info
        )
        
        return result
    
    # ì¼ë°˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
    logger.info("=== ì¼ë°˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘ ===")
    
    # MLflow ì‹¤í—˜ ì„¤ì •
    experiment_name = config['mlflow']['experiment_name']
    experiment_id = setup_mlflow_experiment(experiment_name)
    
    # íŠœë„ˆ ìƒì„±
    tuner = HyperparameterTuner(config, train_val_df, nrows=nrows)
    
    # íŠœë‹ ì‹¤í–‰
    with mlflow.start_run(experiment_id=experiment_id) as run:
        logger.info(f"MLflow Run ì‹œì‘: {run.info.run_id}")
        
        # ì„¤ì • ë¡œê¹…
        log_tuning_params(config, config)  # configë¥¼ base_configë¡œë„ ì‚¬ìš©
        
        # íŠœë‹ ì‹¤í–‰
        result = tuner.optimize(start_mlflow_run=False)  # ì´ë¯¸ runì´ ì‹œì‘ë˜ì–´ ìˆìœ¼ë¯€ë¡œ False
        
        # ê²°ê³¼ ì €ì¥
        try:
            tuner.save_results()
            logger.info("=== í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ ===")
        except Exception as e:
            logger.error(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            # ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
        
        # ê²°ê³¼ ì¶”ì¶œ (ì•ˆì „í•˜ê²Œ)
        if result is None:
            best_params = {}
            best_score = 0.0
            logger.warning("íŠœë‹ ê²°ê³¼ê°€ Noneì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        elif isinstance(result, tuple) and len(result) >= 2:
            best_params = result[0] if result[0] is not None else {}
            best_score = result[1] if result[1] is not None else 0.0
        elif isinstance(result, dict):
            best_params = result.get('best_params', {})
            best_score = result.get('best_score', 0.0)
        else:
            best_params = {}
            best_score = 0.0
            logger.warning(f"ì˜ˆìƒì¹˜ ëª»í•œ ê²°ê³¼ íƒ€ì…: {type(result)}")
        
        logger.info(f"ìµœê³  ì„±ëŠ¥: {best_score:.4f}")
        logger.info("ìµœì  íŒŒë¼ë¯¸í„°:")
        for param, value in best_params.items():
            logger.info(f"  {param}: {value}")
        
        # íŠœë‹ ë¡œê·¸ ì €ì¥ (ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ì „ë‹¬)
        try:
            save_tuning_log(
                result=result,
                model_type=config.get('model', {}).get('model_type', 'unknown'),
                experiment_type='hyperparameter_tuning',
                nrows=nrows,
                experiment_id=run.info.experiment_id,
                run_id=run.info.run_id,
                log_file_path=getattr(tuner, 'log_file_path', None)
            )
        except Exception as e:
            logger.error(f"íŠœë‹ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        # ì‹¤í—˜ ê²°ê³¼ ì €ì¥ (ì¼ë°˜ íŠœë‹)
        save_experiment_results(
            (best_params, best_score), 
            config.get('model', {}).get('model_type', 'xgboost'), 
            'hyperparameter_tuning', 
            nrows, 
            run.info.experiment_id, 
            run.info.run_id,
            config=config,
            data_info=data_info
        )
        
        return best_params, best_score, run.info.experiment_id, run.info.run_id


def save_experiment_results(result, model_type, experiment_type, nrows=None, experiment_id=None, run_id=None, config=None, data_info=None):
    """
    ì‹¤í—˜ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        result: ì‹¤í—˜ ê²°ê³¼
        model_type: ëª¨ë¸ íƒ€ì…
        experiment_type: ì‹¤í—˜ íƒ€ì…
        nrows: ì‚¬ìš©ëœ ë°ì´í„° í–‰ ìˆ˜
        experiment_id: MLflow ì‹¤í—˜ ID
        run_id: MLflow run ID
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬ (ìƒˆë¡œ ì¶”ê°€)
        data_info: ë°ì´í„° ì •ë³´ ë”•ì…”ë„ˆë¦¬ (ìƒˆë¡œ ì¶”ê°€)
    """
    from datetime import datetime
    import json
    
    # results í´ë” ìƒì„± (ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°)
    results_dir = Path("/home/junhyung/Documents/simcare/kbsmc_suicide/results")
    results_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = results_dir / f"experiment_results_{timestamp}.txt"
    
    # MLflow ì‹¤í—˜ ë§í¬ ìƒì„±
    mlflow_link = "http://localhost:5000"
    if experiment_id is None or run_id is None:
        experiment_id = "N/A"
        run_id = "N/A"
        try:
            # í˜„ì¬ í™œì„±í™”ëœ MLflow run ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            current_run = mlflow.active_run()
            if current_run:
                experiment_id = current_run.info.experiment_id
                run_id = current_run.info.run_id
        except:
            pass
    
    if experiment_id != "N/A" and run_id != "N/A":
        mlflow_link = f"http://localhost:5000/#/experiments/{experiment_id}/runs/{run_id}"
    
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(f"ì‹¤í—˜ ê²°ê³¼ - {timestamp}\n")
        f.write("=" * 50 + "\n")
        f.write(f"ëª¨ë¸ íƒ€ì…: {model_type}\n")
        f.write(f"ì‹¤í—˜ íƒ€ì…: {experiment_type}\n")
        if nrows:
            f.write(f"ì‚¬ìš© ë°ì´í„° í–‰ ìˆ˜: {nrows}\n")
        f.write(f"MLflow Experiment ID: {experiment_id}\n")
        f.write(f"MLflow Run ID: {run_id}\n")
        f.write(f"MLflow ë§í¬: {mlflow_link}\n")
        f.write("\n")
        
        # === ìƒˆë¡œ ì¶”ê°€ëœ ì •ë³´ë“¤ ===
        
        # 1. ì‚¬ìš©ëœ ë°ì´í„° ì •ë³´
        f.write("=== ë°ì´í„° ì •ë³´ ===\n")
        if data_info:
            f.write(f"ë°ì´í„° íŒŒì¼ ê²½ë¡œ: {data_info.get('data_path', 'N/A')}\n")
            f.write(f"ì „ì²´ ë°ì´í„° í¬ê¸°: {data_info.get('total_rows', 'N/A')} í–‰ Ã— {data_info.get('total_columns', 'N/A')} ì—´\n")
            f.write(f"ê°œì¸ ìˆ˜: {data_info.get('unique_ids', 'N/A')}ëª…\n")
            f.write(f"ë°ì´í„° ê¸°ê°„: {data_info.get('date_range', 'N/A')}\n")
        else:
            f.write("ë°ì´í„° ì •ë³´: N/A\n")
        f.write("\n")
        
        # 2. Target ë° Input ì •ë³´
        f.write("=== Target ë° Input ì •ë³´ ===\n")
        if config and 'features' in config:
            target_cols = config['features'].get('target_columns', [])
            f.write(f"Target ë³€ìˆ˜: {len(target_cols)}ê°œ\n")
            for i, target in enumerate(target_cols, 1):
                f.write(f"  {i}. {target}\n")
            
            selected_features = config['features'].get('selected_features', [])
            f.write(f"Input ë³€ìˆ˜: {len(selected_features)}ê°œ\n")
            for i, feature in enumerate(selected_features, 1):
                f.write(f"  {i}. {feature}\n")
        else:
            f.write("Target ë° Input ì •ë³´: N/A\n")
        f.write("\n")
        
        # 3. ë¦¬ìƒ˜í”Œë§ ì ìš© ì—¬ë¶€
        f.write("=== ë¦¬ìƒ˜í”Œë§ ì •ë³´ ===\n")
        if config and 'resampling' in config:
            resampling_config = config['resampling']
            enabled = resampling_config.get('enabled', False)
            f.write(f"ë¦¬ìƒ˜í”Œë§ ì ìš©: {'ì˜ˆ' if enabled else 'ì•„ë‹ˆì˜¤'}\n")
            if enabled:
                method = resampling_config.get('method', 'unknown')
                f.write(f"ë¦¬ìƒ˜í”Œë§ ë°©ë²•: {method}\n")
                if method == 'smote':
                    k_neighbors = resampling_config.get('smote_k_neighbors', 5)
                    f.write(f"SMOTE k_neighbors: {k_neighbors}\n")
                elif method == 'borderline_smote':
                    k_neighbors = resampling_config.get('borderline_smote_k_neighbors', 5)
                    f.write(f"Borderline SMOTE k_neighbors: {k_neighbors}\n")
                elif method == 'adasyn':
                    k_neighbors = resampling_config.get('adasyn_k_neighbors', 5)
                    f.write(f"ADASYN k_neighbors: {k_neighbors}\n")
                elif method == 'under_sampling':
                    strategy = resampling_config.get('under_sampling_strategy', 'random')
                    f.write(f"ì–¸ë”ìƒ˜í”Œë§ ì „ëµ: {strategy}\n")
                elif method == 'hybrid':
                    strategy = resampling_config.get('hybrid_strategy', 'smote_tomek')
                    f.write(f"í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ: {strategy}\n")
        else:
            f.write("ë¦¬ìƒ˜í”Œë§ ì •ë³´: N/A\n")
        f.write("\n")
        
        # 4. í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§ ì ìš© ì—¬ë¶€
        f.write("=== í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§ ì •ë³´ ===\n")
        if config and 'features' in config:
            features_config = config['features']
            enable_fe = features_config.get('enable_feature_engineering', False)
            f.write(f"í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§ ì ìš©: {'ì˜ˆ' if enable_fe else 'ì•„ë‹ˆì˜¤'}\n")
            if enable_fe:
                # ì§€ì—° í”¼ì²˜ ì„¤ì •
                enable_lagged = features_config.get('enable_lagged_features', False)
                f.write(f"ì§€ì—° í”¼ì²˜ ìƒì„±: {'ì˜ˆ' if enable_lagged else 'ì•„ë‹ˆì˜¤'}\n")
                if enable_lagged:
                    lag_periods = features_config.get('lag_periods', [])
                    f.write(f"ì§€ì—° ê¸°ê°„: {lag_periods}\n")
                
                # ì´ë™ í†µê³„ í”¼ì²˜ ì„¤ì •
                enable_rolling = features_config.get('enable_rolling_stats', False)
                f.write(f"ì´ë™ í†µê³„ í”¼ì²˜ ìƒì„±: {'ì˜ˆ' if enable_rolling else 'ì•„ë‹ˆì˜¤'}\n")
                if enable_rolling:
                    rolling_config = features_config.get('rolling_stats', {})
                    window_sizes = rolling_config.get('window_sizes', [])
                    f.write(f"ì´ë™ ìœˆë„ìš° í¬ê¸°: {window_sizes}\n")
        else:
            f.write("í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§ ì •ë³´: N/A\n")
        f.write("\n")
        
        # 5. í›ˆë ¨/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í´ë˜ìŠ¤ ë¶„í¬ (ë¦¬ìƒ˜í”Œë§ ì ìš© í›„)
        f.write("=== í´ë˜ìŠ¤ ë¶„í¬ ì •ë³´ ===\n")
        if data_info and 'class_distributions' in data_info:
            class_dist = data_info['class_distributions']
            
            # í›ˆë ¨ ì„¸íŠ¸ í´ë˜ìŠ¤ ë¶„í¬
            if 'train_original' in class_dist:
                f.write("í›ˆë ¨ ì„¸íŠ¸ (ë¦¬ìƒ˜í”Œë§ ì „):\n")
                for target, dist in class_dist['train_original'].items():
                    f.write(f"  {target}: {dist}\n")
            
            if 'train_resampled' in class_dist:
                f.write("í›ˆë ¨ ì„¸íŠ¸ (ë¦¬ìƒ˜í”Œë§ í›„):\n")
                for target, dist in class_dist['train_resampled'].items():
                    f.write(f"  {target}: {dist}\n")
            
            # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í´ë˜ìŠ¤ ë¶„í¬
            if 'test' in class_dist:
                f.write("í…ŒìŠ¤íŠ¸ ì„¸íŠ¸:\n")
                for target, dist in class_dist['test'].items():
                    f.write(f"  {target}: {dist}\n")
        else:
            f.write("í´ë˜ìŠ¤ ë¶„í¬ ì •ë³´: N/A\n")
        f.write("\n")
        
        # === ê¸°ì¡´ ê²°ê³¼ ì •ë³´ ===
        f.write("=== ì‹¤í—˜ ê²°ê³¼ ===\n")
        if isinstance(result, tuple) and len(result) == 2:
            # ì¼ë°˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼
            best_params, best_score = result
            f.write(f"ìµœê³  ì„±ëŠ¥: {best_score}\n")
            f.write("ìµœì  íŒŒë¼ë¯¸í„°:\n")
            for param, value in best_params.items():
                f.write(f"  {param}: {value}\n")
            
            # ìƒì„¸ ë©”íŠ¸ë¦­ ì •ë³´ ì¶”ê°€ (MLflowì—ì„œ ê°€ì ¸ì˜¨ ì •ë³´ê°€ ìˆë‹¤ë©´)
            f.write("\n=== ìƒì„¸ ì„±ëŠ¥ ì§€í‘œ ===\n")
            f.write("(MLflowì—ì„œ í™•ì¸ ê°€ëŠ¥í•œ ìƒì„¸ ë©”íŠ¸ë¦­ë“¤)\n")
            f.write("- ê¸°ë³¸ ì§€í‘œ: precision, recall, f1, accuracy, balanced_accuracy\n")
            f.write("- í™•ì¥ ì§€í‘œ: f1_beta, mcc, kappa, specificity, sensitivity\n")
            f.write("- ì˜ˆì¸¡ ì§€í‘œ: ppv, npv, fpr, fnr, tpr, tnr\n")
            f.write("- ê³¡ì„  ì§€í‘œ: roc_auc, pr_auc\n")
            f.write("- ë°ì´í„° ì§€í‘œ: positive_samples, negative_samples, positive_ratio\n")
        else:
            # ë¦¬ìƒ˜í”Œë§ ë¹„êµ ê²°ê³¼
            f.write("ë¦¬ìƒ˜í”Œë§ ë¹„êµ ê²°ê³¼:\n")
            f.write(f"ìµœê³  ì„±ëŠ¥ ê¸°ë²•: {result.get('best_method', 'none')}\n")
            f.write(f"ìµœê³  ì„±ëŠ¥: {result.get('best_score', 0.0)}\n")
            f.write("\nê° ê¸°ë²•ë³„ ì„±ëŠ¥:\n")
            for method, method_result in result.get('methods', {}).items():
                f.write(f"  {method}: {method_result['best_score']}\n")
    
    logger.info(f"ì‹¤í—˜ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    logger.info(f"MLflow Experiment ID: {experiment_id}")
    logger.info(f"MLflow Run ID: {run_id}")
    logger.info(f"MLflow ë§í¬: {mlflow_link}")
    return filename


def collect_data_info(df: pd.DataFrame, config: Dict[str, Any], train_val_df: pd.DataFrame = None, test_df: pd.DataFrame = None) -> Dict[str, Any]:
    """
    ì‹¤í—˜ì— ì‚¬ìš©ëœ ë°ì´í„°ì˜ ìƒì„¸ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    
    Args:
        df: ì „ì²´ ë°ì´í„°í”„ë ˆì„
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        train_val_df: í›ˆë ¨/ê²€ì¦ ë°ì´í„°í”„ë ˆì„ (ì„ íƒì‚¬í•­)
        test_df: í…ŒìŠ¤íŠ¸ ë°ì´í„°í”„ë ˆì„ (ì„ íƒì‚¬í•­)
        
    Returns:
        ë°ì´í„° ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    data_info = {}
    
    # ê¸°ë³¸ ë°ì´í„° ì •ë³´
    data_info['total_rows'] = len(df)
    data_info['total_columns'] = len(df.columns)
    
    # ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    if 'data' in config:
        data_info['data_path'] = config['data'].get('data_path', 'N/A')
    
    # ê°œì¸ ìˆ˜ (ID ì»¬ëŸ¼ ê¸°ì¤€)
    id_column = config.get('time_series', {}).get('id_column', 'id')
    if id_column in df.columns:
        data_info['unique_ids'] = df[id_column].nunique()
    
    # ë°ì´í„° ê¸°ê°„
    date_column = config.get('time_series', {}).get('date_column', 'dov')
    year_column = config.get('time_series', {}).get('year_column', 'yov')
    
    if date_column in df.columns:
        try:
            df[date_column] = pd.to_datetime(df[date_column])
            date_range = f"{df[date_column].min().strftime('%Y-%m-%d')} ~ {df[date_column].max().strftime('%Y-%m-%d')}"
            data_info['date_range'] = date_range
        except:
            data_info['date_range'] = 'N/A'
    elif year_column in df.columns:
        try:
            year_range = f"{df[year_column].min()} ~ {df[year_column].max()}"
            data_info['date_range'] = year_range
        except:
            data_info['date_range'] = 'N/A'
    else:
        data_info['date_range'] = 'N/A'
    
    # í´ë˜ìŠ¤ ë¶„í¬ ì •ë³´
    class_distributions = {}
    target_columns = config.get('features', {}).get('target_columns', [])
    
    # ì „ì²´ ë°ì´í„°ì˜ í´ë˜ìŠ¤ ë¶„í¬
    for target in target_columns:
        if target in df.columns:
            class_distributions[f'full_{target}'] = df[target].value_counts().to_dict()
    
    # í›ˆë ¨/ê²€ì¦ ë°ì´í„°ì˜ í´ë˜ìŠ¤ ë¶„í¬
    if train_val_df is not None:
        train_original = {}
        for target in target_columns:
            if target in train_val_df.columns:
                train_original[target] = train_val_df[target].value_counts().to_dict()
        class_distributions['train_original'] = train_original
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ í´ë˜ìŠ¤ ë¶„í¬
    if test_df is not None:
        test_dist = {}
        for target in target_columns:
            if target in test_df.columns:
                test_dist[target] = test_df[target].value_counts().to_dict()
        class_distributions['test'] = test_dist
    
    data_info['class_distributions'] = class_distributions
    
    return data_info


def update_class_distributions_after_resampling(data_info: Dict[str, Any], X_resampled: pd.DataFrame, y_resampled: pd.Series, config: Dict[str, Any]) -> Dict[str, Any]:
    """
    ë¦¬ìƒ˜í”Œë§ í›„ í´ë˜ìŠ¤ ë¶„í¬ ì •ë³´ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    
    Args:
        data_info: ê¸°ì¡´ ë°ì´í„° ì •ë³´
        X_resampled: ë¦¬ìƒ˜í”Œë§ëœ í”¼ì²˜ ë°ì´í„°
        y_resampled: ë¦¬ìƒ˜í”Œë§ëœ íƒ€ê²Ÿ ë°ì´í„°
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        
    Returns:
        ì—…ë°ì´íŠ¸ëœ ë°ì´í„° ì •ë³´
    """
    if 'class_distributions' not in data_info:
        data_info['class_distributions'] = {}
    
    # ë¦¬ìƒ˜í”Œë§ í›„ í´ë˜ìŠ¤ ë¶„í¬ ê³„ì‚°
    target_columns = config.get('features', {}).get('target_columns', [])
    train_resampled = {}
    
    for target in target_columns:
        if target in y_resampled.columns:
            train_resampled[target] = y_resampled[target].value_counts().to_dict()
    
    data_info['class_distributions']['train_resampled'] = train_resampled
    
    return data_info


def main():
    """ë©”ì¸ í•¨ìˆ˜ (ConfigManager ê¸°ë°˜ ê³„ì¸µì  config ë³‘í•© ì§€ì›)"""
    parser = argparse.ArgumentParser(
        description="í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í–‰ (ConfigManager ì§€ì›)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ê³„ì¸µì  configë¡œ íŠœë‹ ì‹¤í–‰
  python scripts/run_hyperparameter_tuning.py --model-type xgboost --experiment-type hyperparameter_tuning
  # legacy ë‹¨ì¼ íŒŒì¼ configë¡œ íŠœë‹ ì‹¤í–‰
  python scripts/run_hyperparameter_tuning.py --tuning_config configs/hyperparameter_tuning.yaml --base_config configs/default_config.yaml
        """
    )
    parser.add_argument("--model-type", type=str, choices=['xgboost', 'lightgbm', 'random_forest', 'catboost'], default=None, help="ì‚¬ìš©í•  ëª¨ë¸ íƒ€ì… (ê³„ì¸µì  config ë³‘í•©)")
    parser.add_argument("--experiment-type", type=str, default=None, help="ì‹¤í—˜ íƒ€ì… (focal_loss, resampling, hyperparameter_tuning ë“±)")
    parser.add_argument("--tuning_config", type=str, default=None, help="(legacy) íŠœë‹ ì„¤ì • íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--base_config", type=str, default=None, help="(legacy) ê¸°ë³¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--data_path", type=str, default=None, help="ë°ì´í„° íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--nrows", type=int, default=None, help="ì‚¬ìš©í•  ë°ì´í„° í–‰ ìˆ˜")
    parser.add_argument("--resampling-comparison", action="store_true", help="ë¦¬ìƒ˜í”Œë§ ê¸°ë²• ë¹„êµ ì‹¤í—˜ ì‹¤í–‰")
    parser.add_argument("--resampling-methods", nargs="+", choices=['none', 'smote', 'borderline_smote', 'adasyn', 'under_sampling', 'hybrid'], default=None, help="ë¹„êµí•  ë¦¬ìƒ˜í”Œë§ ê¸°ë²•ë“¤")
    parser.add_argument("--mlflow_ui", action="store_true", help="íŠœë‹ ì™„ë£Œ í›„ MLflow UI ì‹¤í–‰")
    
    # ìë™í™”ëœ ì‹¤í—˜ì„ ìœ„í•œ ì¶”ê°€ ì¸ìë“¤
    parser.add_argument("--split-strategy", type=str, choices=['group_kfold', 'time_series_walk_forward', 'time_series_group_kfold'], default=None, help="ë°ì´í„° ë¶„í•  ì „ëµ")
    parser.add_argument("--cv-folds", type=int, default=None, help="êµì°¨ ê²€ì¦ í´ë“œ ìˆ˜")
    parser.add_argument("--n-trials", type=int, default=None, help="í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œë„ íšŸìˆ˜")
    parser.add_argument("--tuning-direction", type=str, choices=['maximize', 'minimize'], default=None, help="íŠœë‹ ë°©í–¥ (maximize/minimize)")
    parser.add_argument("--primary-metric", type=str, default=None, help="ì£¼ìš” í‰ê°€ ì§€í‘œ (f1, precision, recall, mcc, roc_auc, pr_auc ë“±)")
    parser.add_argument("--test-size", type=float, default=None, help="í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¹„ìœ¨ (0.0-1.0)")
    parser.add_argument("--random-state", type=int, default=None, help="ëœë¤ ì‹œë“œ")
    parser.add_argument("--n-jobs", type=int, default=None, help="ë³‘ë ¬ ì²˜ë¦¬ ì‘ì—… ìˆ˜")
    parser.add_argument("--timeout", type=int, default=None, help="íŠœë‹ íƒ€ì„ì•„ì›ƒ (ì´ˆ)")
    parser.add_argument("--early-stopping", action="store_true", help="Early stopping í™œì„±í™”")
    parser.add_argument("--early-stopping-rounds", type=int, default=None, help="Early stopping ë¼ìš´ë“œ ìˆ˜")
    parser.add_argument("--feature-selection", action="store_true", help="í”¼ì²˜ ì„ íƒ í™œì„±í™”")
    parser.add_argument("--feature-selection-method", type=str, choices=['mutual_info', 'chi2', 'f_classif', 'recursive'], default=None, help="í”¼ì²˜ ì„ íƒ ë°©ë²•")
    parser.add_argument("--feature-selection-k", type=int, default=None, help="ì„ íƒí•  í”¼ì²˜ ìˆ˜")
    parser.add_argument("--resampling-enabled", action="store_true", help="ë¦¬ìƒ˜í”Œë§ í™œì„±í™”")
    parser.add_argument("--resampling-method", type=str, choices=['smote', 'borderline_smote', 'adasyn', 'under_sampling', 'hybrid'], default=None, help="ë¦¬ìƒ˜í”Œë§ ë°©ë²•")
    parser.add_argument("--resampling-ratio", type=float, default=None, help="ë¦¬ìƒ˜í”Œë§ í›„ ì–‘ì„± í´ë˜ìŠ¤ ë¹„ìœ¨")
    parser.add_argument("--experiment-name", type=str, default=None, help="MLflow ì‹¤í—˜ ì´ë¦„ (ê¸°ë³¸ê°’: model_type_experiment_type)")
    parser.add_argument("--save-model", action="store_true", help="ìµœì  ëª¨ë¸ ì €ì¥")
    parser.add_argument("--save-predictions", action="store_true", help="ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥")
    parser.add_argument("--verbose", type=int, choices=[0, 1, 2], default=1, help="ë¡œê·¸ ë ˆë²¨ (0: ìµœì†Œ, 1: ê¸°ë³¸, 2: ìƒì„¸)")
    args = parser.parse_args()
    try:
        if args.model_type:
            # ConfigManager ê¸°ë°˜ ê³„ì¸µì  config ë³‘í•©
            config_manager = ConfigManager()
            config = config_manager.create_experiment_config(args.model_type, args.experiment_type)
            
            # ëª…ë ¹í–‰ ì¸ìë¥¼ configì— ì ìš©
            config = config_manager.apply_command_line_args(config, args)
            
            # ì„¤ì • ìš”ì•½ ì¶œë ¥
            config_manager.print_config_summary(config)
            
            result = run_hyperparameter_tuning_with_config(
                config,
                data_path=args.data_path,
                nrows=args.nrows,
                resampling_comparison=args.resampling_comparison,
                resampling_methods=args.resampling_methods
            )
            # ì‹¤í—˜ ê²°ê³¼ ì €ì¥ì€ run_hyperparameter_tuning_with_config ë‚´ë¶€ì—ì„œ ì²˜ë¦¬ë¨
        elif args.tuning_config and args.base_config:
            # legacy: ë‹¨ì¼ íŒŒì¼ ê¸°ë°˜
            result = run_resampling_tuning_comparison(
                tuning_config_path=args.tuning_config,
                base_config_path=args.base_config,
                data_path=args.data_path,
                resampling_methods=args.resampling_methods,
                nrows=args.nrows
            )
            save_tuning_log(result, 'legacy', 'resampling', args.nrows)
        else:
            raise ValueError("model-type ë˜ëŠ” tuning_config+base_config ì¤‘ í•˜ë‚˜ëŠ” ë°˜ë“œì‹œ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
        if args.resampling_comparison:
            print(f"\nğŸ‰ ë¦¬ìƒ˜í”Œë§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë¹„êµ ì‹¤í—˜ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            print(f"ğŸ“Š ìµœê³  ì„±ëŠ¥ ê¸°ë²•: {result.get('best_method', 'none')}")
            print(f"âš™ï¸  ìµœê³  ì„±ëŠ¥: {result.get('best_score', 0.0):.4f}")
            print(f"\nğŸ“ˆ ê° ê¸°ë²•ë³„ ì„±ëŠ¥:")
            for method, method_result in result.get('methods', {}).items():
                print(f"  {method}: {method_result['best_score']:.4f}")
        else:
            best_params, best_score, _, _ = result
            print(f"\nğŸ‰ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            print(f"ğŸ“Š ìµœê³  ì„±ëŠ¥: {best_score:.4f}")
            print(f"âš™ï¸  ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")
        if args.mlflow_ui:
            print(f"\nğŸŒ MLflow UIë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤...")
            print(f"   ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:5000 ìœ¼ë¡œ ì ‘ì†í•˜ì„¸ìš”.")
            print(f"   ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”.")
            import subprocess
            try:
                subprocess.run(["mlflow", "ui"], check=True)
            except KeyboardInterrupt:
                print(f"\nğŸ‘‹ MLflow UIë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            except Exception as e:
                print(f"âŒ MLflow UI ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
    except Exception as e:
        logger.error(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤íŒ¨: {str(e)}")
        # robust error logging
        save_tuning_log(None, args.model_type if 'args' in locals() and args.model_type else 'unknown', args.experiment_type if 'args' in locals() and args.experiment_type else 'unknown', getattr(args, 'nrows', None), error_msg=str(e))
        sys.exit(1)


if __name__ == "__main__":
    main() 