#!/usr/bin/env python3
"""
ë¦¬ìƒ˜í”Œë§ ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸

í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ê³¼ ë™ì¼í•œ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë©´ì„œ ë¦¬ìƒ˜í”Œë§ ë°©ë²•ì„ íŠœë‹ ë²”ìœ„ì— í¬í•¨í•˜ëŠ” ì‹¤í—˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import argparse
import logging
import os
import sys
import time
from typing import Dict, Any, List, Optional, Tuple

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import numpy as np
import mlflow
import optuna
from datetime import datetime

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import
from src.utils.config_manager import ConfigManager
from src.utils import setup_logging, setup_experiment_logging, log_experiment_summary, experiment_logging_context
from src.utils.experiment_results import save_experiment_results
from src.hyperparameter_tuning import HyperparameterTuner
from src.utils.config_manager import ConfigManager
from src.splits import split_test_set, validate_splits


# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


def setup_mlflow_experiment(experiment_name: str):
    """
    MLflow ì‹¤í—˜ì„ ì„¤ì •í•©ë‹ˆë‹¤.
    
    Args:
        experiment_name: ì‹¤í—˜ ì´ë¦„
        
    Returns:
        ì‹¤í—˜ ID
    """
    from src.utils.mlflow_manager import setup_mlflow_experiment_safely
    
    experiment_id = setup_mlflow_experiment_safely(experiment_name)
    logger.info(f"MLflow ì‹¤í—˜ ì„¤ì •: {experiment_name} (ID: {experiment_id})")
    return experiment_id


def log_tuning_params(config: Dict[str, Any], base_config: Dict[str, Any]):
    """
    íŠœë‹ íŒŒë¼ë¯¸í„°ë¥¼ MLflowì— ë¡œê¹…í•©ë‹ˆë‹¤.
    
    Args:
        config: íŠœë‹ ì„¤ì •
        base_config: ê¸°ë³¸ ì„¤ì •
    """
    # ë¦¬ìƒ˜í”Œë§ ê´€ë ¨ íŒŒë¼ë¯¸í„° ë¡œê¹…
    resampling_config = config.get('imbalanced_data', {}).get('resampling', {})
    
    # ë¦¬ìƒ˜í”Œë§ í™œì„±í™” ì—¬ë¶€ (ì•ˆì „í•œ ë¡œê¹…)
    from src.utils.mlflow_manager import safe_log_param
    
    safe_log_param("resampling_enabled", resampling_config.get('enabled', False), logger)
    
    if resampling_config.get('enabled', False):
        # ë¦¬ìƒ˜í”Œë§ ë°©ë²• (ì•ˆì „í•œ ë¡œê¹…)
        safe_log_param("resampling_method", resampling_config.get('method', 'none'), logger)
        
        # SMOTE ê´€ë ¨ íŒŒë¼ë¯¸í„°
        smote_config = resampling_config.get('smote', {})
        mlflow.log_param("smote_k_neighbors", smote_config.get('k_neighbors', 5))
        mlflow.log_param("smote_sampling_strategy", smote_config.get('sampling_strategy', 'auto'))
        
        # Borderline SMOTE ê´€ë ¨ íŒŒë¼ë¯¸í„°
        borderline_smote_config = resampling_config.get('borderline_smote', {})
        mlflow.log_param("borderline_smote_k_neighbors", borderline_smote_config.get('k_neighbors', 5))
        mlflow.log_param("borderline_smote_m_neighbors", borderline_smote_config.get('m_neighbors', 10))
        mlflow.log_param("borderline_smote_sampling_strategy", borderline_smote_config.get('sampling_strategy', 'auto'))
        
        # ADASYN ê´€ë ¨ íŒŒë¼ë¯¸í„°
        adasyn_config = resampling_config.get('adasyn', {})
        mlflow.log_param("adasyn_k_neighbors", adasyn_config.get('k_neighbors', 5))
        mlflow.log_param("adasyn_n_neighbors", adasyn_config.get('n_neighbors', 5))
        mlflow.log_param("adasyn_sampling_strategy", adasyn_config.get('sampling_strategy', 'auto'))
        
        # Under Sampling ê´€ë ¨ íŒŒë¼ë¯¸í„°
        under_sampling_config = resampling_config.get('under_sampling', {})
        mlflow.log_param("under_sampling_strategy", under_sampling_config.get('strategy', 'random'))
        mlflow.log_param("under_sampling_sampling_strategy", under_sampling_config.get('sampling_strategy', 'auto'))
    
    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê´€ë ¨ íŒŒë¼ë¯¸í„°
    mlflow.log_param("class_weights", config.get('imbalanced_data', {}).get('class_weights', 'Balanced'))
    mlflow.log_param("auto_class_weights", config.get('imbalanced_data', {}).get('auto_class_weights', 'Balanced'))
    mlflow.log_param("scale_pos_weight", config.get('imbalanced_data', {}).get('scale_pos_weight', 1.0))
    
    logger.info(f"íŠœë‹ íŒŒë¼ë¯¸í„° ë¡œê¹… ì™„ë£Œ: {len(resampling_config)}ê°œ íŒŒë¼ë¯¸í„°")


def run_resampling_experiment_with_config(
    config: Dict[str, Any], 
    data_path: str = None, 
    nrows: int = None
) -> Tuple[Dict[str, Any], float, Any, Any]:
    """
    ì„¤ì • ê¸°ë°˜ ë¦¬ìƒ˜í”Œë§ ì‹¤í—˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    
    Args:
        config: ì‹¤í—˜ ì„¤ì •
        data_path: ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        nrows: ì‚¬ìš©í•  ë°ì´í„° í–‰ ìˆ˜ (Noneì´ë©´ ì „ì²´)
        
    Returns:
        (best_params, best_score, experiment_id, run_id) íŠœí”Œ
    """
    start_time = time.time()
    
    # ìƒˆë¡œìš´ ë¡œê¹… ì‹œìŠ¤í…œ ì ìš©
    experiment_type = config.get('experiment', {}).get('type', 'resampling_experiment')
    model_type = config.get('model', {}).get('model_type', 'xgboost')
    
    with experiment_logging_context(
        experiment_type=experiment_type,
        model_type=model_type,
        log_level="INFO",
        capture_console=True
    ) as log_file_path:
        
        logger.info(f"=== ë¦¬ìƒ˜í”Œë§ ì‹¤í—˜ ì‹œì‘ ===")
        logger.info(f"ëª¨ë¸ íƒ€ì…: {model_type}")
        logger.info(f"ì‹¤í—˜ íƒ€ì…: {experiment_type}")
        logger.info(f"ë¡œê·¸ íŒŒì¼: {log_file_path}")
        
        try:
            # MLflow ì‹¤í—˜ ë¬´ê²°ì„± ê²€ì¦ ë° ë³µêµ¬
            from src.utils.mlflow_manager import MLflowExperimentManager
            manager = MLflowExperimentManager()
            
            # í˜„ì¬ ì‹¤í—˜ ìƒíƒœ í™•ì¸
            logger.info("MLflow ì‹¤í—˜ ìƒíƒœ í™•ì¸ ì¤‘...")
            manager.print_experiment_summary()
            
            # Orphaned ì‹¤í—˜ ì •ë¦¬
            logger.info("Orphaned ì‹¤í—˜ ì •ë¦¬ ì¤‘...")
            deleted_experiments = manager.cleanup_orphaned_experiments(backup=True)
            if deleted_experiments:
                logger.info(f"ì •ë¦¬ëœ orphaned ì‹¤í—˜: {deleted_experiments}")
            
            # ë°ì´í„° ê²½ë¡œ ì„¤ì •
            if data_path is None:
                data_path = config['data'].get('file_path', 'data/processed/processed_data_with_features.csv')
            
            # ë°ì´í„° ë¡œë“œ
            logger.info(f"ë°ì´í„° ë¡œë“œ ì¤‘: {data_path}")
            if nrows:
                df = pd.read_csv(data_path, nrows=nrows)
                logger.info(f"í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë¡œë“œ: {len(df):,} í–‰")
            else:
                df = pd.read_csv(data_path)
                logger.info(f"ì „ì²´ ë°ì´í„° ë¡œë“œ: {len(df):,} í–‰")
            
            # ë°ì´í„° ì •ë³´ ìˆ˜ì§‘
            data_info = collect_data_info(df, config)
            logger.info(f"ë°ì´í„° ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {data_info['total_rows']} í–‰, {data_info['total_columns']} ì—´")
            
            # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„ë¦¬
            logger.info("=== í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„ë¦¬ ===")
            train_val_df, test_df, test_ids = split_test_set(df, config)
            
            # ë°ì´í„° ì •ë³´ ì—…ë°ì´íŠ¸ (í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì •ë³´ ì¶”ê°€)
            data_info = collect_data_info(df, config, train_val_df, test_df)
            
            # ë¶„í•  ê²€ì¦
            if not validate_splits(train_val_df, test_df, config):
                logger.error("ë¶„í•  ê²€ì¦ ì‹¤íŒ¨!")
                return None
            
            # MLflow ì‹¤í—˜ ì„¤ì •
            experiment_name = config['mlflow']['experiment_name']
            experiment_id = setup_mlflow_experiment(experiment_name)
            
            # ì‹¤í—˜ ë¬´ê²°ì„± ê²€ì¦
            if not manager.validate_experiment_integrity(experiment_id):
                logger.warning(f"ì‹¤í—˜ {experiment_id} ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨, ë³µêµ¬ ì‹œë„ ì¤‘...")
                if manager.repair_experiment(experiment_id):
                    logger.info(f"ì‹¤í—˜ {experiment_id} ë³µêµ¬ ì™„ë£Œ")
                else:
                    logger.error(f"ì‹¤í—˜ {experiment_id} ë³µêµ¬ ì‹¤íŒ¨")
            
            # íŠœë„ˆ ìƒì„± (ë¦¬ìƒ˜í”Œë§ ì‹¤í—˜ìš©)
            tuner = ResamplingHyperparameterTuner(config, train_val_df, nrows=nrows)
            
            # íŠœë‹ ì‹¤í–‰ (ì•ˆì „í•œ MLflow run ê´€ë¦¬)
            from src.utils.mlflow_manager import safe_mlflow_run
            
            with safe_mlflow_run(experiment_id=experiment_id) as run:
                logger.info(f"MLflow Run ì‹œì‘: {run.info.run_id}")
                
                # ì„¤ì • ë¡œê¹…
                log_tuning_params(config, config)  # configë¥¼ base_configë¡œë„ ì‚¬ìš©
                
                # íŠœë‹ ì‹¤í–‰
                result = tuner.optimize(start_mlflow_run=False)  # ì´ë¯¸ runì´ ì‹œì‘ë˜ì–´ ìˆìœ¼ë¯€ë¡œ False
                
                # ê²°ê³¼ ì €ì¥ (ResamplingHyperparameterTunerì—ëŠ” save_resultsê°€ ì—†ìœ¼ë¯€ë¡œ ì œê±°)
                logger.info("=== ë¦¬ìƒ˜í”Œë§ ì‹¤í—˜ ì™„ë£Œ ===")
                
                # ê²°ê³¼ ì¶”ì¶œ (ì•ˆì „í•˜ê²Œ)
                if result is None:
                    best_params = {}
                    best_score = 0.0
                    logger.warning("íŠœë‹ ê²°ê³¼ê°€ Noneì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                elif isinstance(result, tuple) and len(result) >= 2:
                    best_params = result[0] if result[0] is not None else {}
                    best_score = result[1] if result[1] is not None else 0.0
                elif isinstance(result, dict):
                    best_params = result.get('best_params', {})
                    best_score = result.get('best_score', 0.0)
                else:
                    best_params = {}
                    best_score = 0.0
                    logger.warning(f"ì˜ˆìƒì¹˜ ëª»í•œ ê²°ê³¼ íƒ€ì…: {type(result)}")
                
                logger.info(f"ìµœê³  ì„±ëŠ¥: {best_score:.4f}")
                logger.info("ìµœì  íŒŒë¼ë¯¸í„°:")
                for param, value in best_params.items():
                    logger.info(f"  {param}: {value}")
                
                # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
                execution_time = time.time() - start_time
                
                # ì‹¤í—˜ ìš”ì•½ ë¡œê¹…
                n_trials = len(tuner.study.trials) if hasattr(tuner, 'study') and tuner.study else 0
                log_experiment_summary(
                    experiment_type=experiment_type,
                    model_type=model_type,
                    best_score=best_score,
                    best_params=best_params,
                    execution_time=execution_time,
                    n_trials=n_trials,
                    data_info=data_info,
                    log_file_path=log_file_path
                )
                
                logger.info(f"=== ë¦¬ìƒ˜í”Œë§ ì‹¤í—˜ ì™„ë£Œ ===")
                logger.info(f"ìµœê³  ì„±ëŠ¥: {best_score:.6f}")
                logger.info(f"ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")
                logger.info(f"ì‹œë„ íšŸìˆ˜: {n_trials}")
                
                # ì‹¤í—˜ ê²°ê³¼ ì €ì¥
                try:
                    save_experiment_results(
                        result=(best_params, best_score),
                        model_type=model_type,
                        experiment_type=experiment_type,
                        nrows=nrows,
                        experiment_id=run.info.experiment_id,
                        run_id=run.info.run_id,
                        config=config,
                        data_info=data_info
                    )
                    logger.info("ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
                
                return best_params, best_score, run.info.experiment_id, run.info.run_id
                
        except Exception as e:
            logger.error(f"ë¦¬ìƒ˜í”Œë§ ì‹¤í—˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise


def add_resampling_hyperparameters_to_tuning_config(tuning_config: Dict[str, Any], resampling_method: str) -> Dict[str, Any]:
    """
    ë¦¬ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°ë¥¼ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì„¤ì •ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    
    Args:
        tuning_config: ê¸°ì¡´ íŠœë‹ ì„¤ì •
        resampling_method: ë¦¬ìƒ˜í”Œë§ ê¸°ë²•
        
    Returns:
        ë¦¬ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°ê°€ ì¶”ê°€ëœ íŠœë‹ ì„¤ì •
    """
    # ê¸°ì¡´ ì„¤ì • ë³µì‚¬
    updated_config = tuning_config.copy()
    
    # ë¦¬ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°ë¥¼ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì— ì¶”ê°€
    if 'resampling_params' not in updated_config:
        updated_config['resampling_params'] = {}
    
    resampling_params = updated_config['resampling_params']
    
    if resampling_method == 'smote':
        resampling_params.update({
            'smote_k_neighbors': {
                'type': 'int',
                'low': 3,
                'high': 10,
                'log': False
            },
            'smote_sampling_strategy': {
                'type': 'float',
                'low': 0.05,
                'high': 0.3,
                'log': False
            }
        })
    elif resampling_method == 'borderline_smote':
        resampling_params.update({
            'borderline_smote_k_neighbors': {
                'type': 'int',
                'low': 3,
                'high': 10,
                'log': False
            },
            'borderline_smote_sampling_strategy': {
                'type': 'float',
                'low': 0.05,
                'high': 0.3,
                'log': False
            },
            'borderline_smote_m_neighbors': {
                'type': 'int',
                'low': 5,
                'high': 15,
                'log': False
            }
        })
    elif resampling_method == 'adasyn':
        resampling_params.update({
            'adasyn_k_neighbors': {
                'type': 'int',
                'low': 3,
                'high': 10,
                'log': False
            },
            'adasyn_sampling_strategy': {
                'type': 'float',
                'low': 0.05,
                'high': 0.3,
                'log': False
            }
        })
    elif resampling_method == 'time_series_adapted':
        resampling_params.update({
            'time_weight': {
                'type': 'float',
                'low': 0.1,
                'high': 0.8,
                'log': False
            },
            'temporal_window': {
                'type': 'int',
                'low': 1,
                'high': 6,
                'log': False
            },
            'seasonality_weight': {
                'type': 'float',
                'low': 0.0,
                'high': 0.5,
                'log': False
            },
            'pattern_preservation': {
                'type': 'categorical',
                'choices': [True, False]
            },
            'trend_preservation': {
                'type': 'categorical',
                'choices': [True, False]
            }
        })
    
    return updated_config


def apply_resampling_hyperparameters_to_config(config: Dict[str, Any], trial_params: Dict[str, Any]) -> Dict[str, Any]:
    """
    Optuna trialì—ì„œ ìƒì„±ëœ ë¦¬ìƒ˜í”Œë§ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ configì— ì ìš©í•©ë‹ˆë‹¤.
    
    Args:
        config: ê¸°ì¡´ ì„¤ì •
        trial_params: Optuna trialì—ì„œ ìƒì„±ëœ íŒŒë¼ë¯¸í„°
        
    Returns:
        ë¦¬ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°ê°€ ì ìš©ëœ ì„¤ì •
    """
    updated_config = config.copy()
    resampling_config = updated_config.get('resampling', {})
    method = resampling_config.get('method', 'none')
    
    # ë¦¬ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° ì ìš©
    if method == 'smote':
        if 'smote_k_neighbors' in trial_params:
            resampling_config['smote_k_neighbors'] = trial_params['smote_k_neighbors']
        if 'smote_sampling_strategy' in trial_params:
            resampling_config['smote_sampling_strategy'] = trial_params['smote_sampling_strategy']
    elif method == 'borderline_smote':
        if 'borderline_smote_k_neighbors' in trial_params:
            resampling_config['borderline_smote_k_neighbors'] = trial_params['borderline_smote_k_neighbors']
        if 'borderline_smote_sampling_strategy' in trial_params:
            resampling_config['borderline_smote_sampling_strategy'] = trial_params['borderline_smote_sampling_strategy']
        if 'borderline_smote_m_neighbors' in trial_params:
            resampling_config['borderline_smote_m_neighbors'] = trial_params['borderline_smote_m_neighbors']
    elif method == 'adasyn':
        if 'adasyn_k_neighbors' in trial_params:
            resampling_config['adasyn_k_neighbors'] = trial_params['adasyn_k_neighbors']
        if 'adasyn_sampling_strategy' in trial_params:
            resampling_config['adasyn_sampling_strategy'] = trial_params['adasyn_sampling_strategy']
    elif method == 'time_series_adapted':
        time_series_config = resampling_config.get('time_series_adapted', {})
        if 'time_weight' in trial_params:
            time_series_config['time_weight'] = trial_params['time_weight']
        if 'temporal_window' in trial_params:
            time_series_config['temporal_window'] = trial_params['temporal_window']
        if 'seasonality_weight' in trial_params:
            time_series_config['seasonality_weight'] = trial_params['seasonality_weight']
        if 'pattern_preservation' in trial_params:
            time_series_config['pattern_preservation'] = trial_params['pattern_preservation']
        if 'trend_preservation' in trial_params:
            time_series_config['trend_preservation'] = trial_params['trend_preservation']
        resampling_config['time_series_adapted'] = time_series_config
    
    updated_config['resampling'] = resampling_config
    return updated_config


def update_class_distributions_after_resampling(data_info: Dict[str, Any], X_resampled: pd.DataFrame, y_resampled: pd.Series, config: Dict[str, Any]) -> Dict[str, Any]:
    """
    ë¦¬ìƒ˜í”Œë§ í›„ í´ë˜ìŠ¤ ë¶„í¬ ì •ë³´ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    
    Args:
        data_info: ê¸°ì¡´ ë°ì´í„° ì •ë³´
        X_resampled: ë¦¬ìƒ˜í”Œë§ëœ í”¼ì²˜ ë°ì´í„°
        y_resampled: ë¦¬ìƒ˜í”Œë§ëœ íƒ€ê²Ÿ ë°ì´í„°
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        
    Returns:
        ì—…ë°ì´íŠ¸ëœ ë°ì´í„° ì •ë³´
    """
    if 'class_distributions' not in data_info:
        data_info['class_distributions'] = {}
    
    # ë¦¬ìƒ˜í”Œë§ í›„ í´ë˜ìŠ¤ ë¶„í¬ ê³„ì‚°
    target_columns = config.get('features', {}).get('target_columns', [])
    train_resampled = {}
    
    for target in target_columns:
        if target in y_resampled.columns:
            train_resampled[target] = y_resampled[target].value_counts().to_dict()
    
    data_info['class_distributions']['train_resampled'] = train_resampled
    
    return data_info


def run_resampling_comparison_experiment(
    model_type: str,
    experiment_type: str,
    data_path: str,
    resampling_methods: List[str],
    nrows: int = None
) -> Dict[str, Any]:
    """
    ConfigManager ê¸°ë°˜ ë‹¤ì–‘í•œ ë¦¬ìƒ˜í”Œë§ ê¸°ë²•ì— ëŒ€í•´ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ë¹„êµí•˜ëŠ” ì‹¤í—˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    logger.info("=== [ConfigManager] ë¦¬ìƒ˜í”Œë§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë¹„êµ ì‹¤í—˜ ì‹œì‘ ===")
    config_manager = ConfigManager()

    # ë°ì´í„° ë¡œë“œ
    logger.info(f"ë°ì´í„° ë¡œë“œ ì¤‘: {data_path}")
    if nrows:
        df = pd.read_csv(data_path, nrows=nrows)
        logger.info(f"í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë¡œë“œ: {len(df):,} í–‰")
    else:
        df = pd.read_csv(data_path)
        logger.info(f"ì „ì²´ ë°ì´í„° ë¡œë“œ: {len(df):,} í–‰")

    # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„ë¦¬
    logger.info("=== í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„ë¦¬ ===")
    # configëŠ” ë¦¬ìƒ˜í”Œë§ê³¼ ë¬´ê´€í•˜ê²Œ ë¶„í• ì—ë§Œ ì‚¬ìš©
    base_config = config_manager.create_experiment_config(model_type, experiment_type)
    train_val_df, test_df, test_ids = split_test_set(df, base_config)

    # ë¶„í•  ê²€ì¦
    if not validate_splits(train_val_df, test_df, base_config):
        logger.error("ë¶„í•  ê²€ì¦ ì‹¤íŒ¨!")
        return {}

    # ë°ì´í„° ì •ë³´ ìˆ˜ì§‘
    data_info = collect_data_info(df, base_config, train_val_df, test_df)
    logger.info(f"ë°ì´í„° ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {data_info['total_rows']} í–‰, {data_info['total_columns']} ì—´")

    comparison_results = {
        'methods': {},
        'best_method': None,
        'best_score': -np.inf,
        'summary': {}
    }

    # ìƒìœ„ MLflow run ì‹œì‘ (ì•ˆì „í•œ MLflow run ê´€ë¦¬)
    from src.utils.mlflow_manager import safe_mlflow_run
    
    with safe_mlflow_run(run_name=f"resampling_comparison_{model_type}"):
        for method in resampling_methods:
            logger.info(f"\n--- {method.upper()} í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘ ---")
            # config ìƒì„± ë° ë¦¬ìƒ˜í”Œë§ ì„¤ì • ìˆ˜ì •
            config = config_manager.create_experiment_config(model_type, experiment_type)
            config['resampling'] = {
                'enabled': method != 'none',
                'method': method if method != 'none' else 'smote',
                'random_state': 42
            }
            # ë¦¬ìƒ˜í”Œë§ ê¸°ë²•ë³„ ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì„¤ì • (í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ëŒ€ìƒ)
            if method == 'smote':
                config['resampling']['smote_k_neighbors'] = 5  # ê¸°ë³¸ê°’, íŠœë‹ ëŒ€ìƒ
                config['resampling']['smote_sampling_strategy'] = 0.1  # ê¸°ë³¸ê°’, íŠœë‹ ëŒ€ìƒ
            elif method == 'borderline_smote':
                config['resampling']['borderline_smote_k_neighbors'] = 5  # ê¸°ë³¸ê°’, íŠœë‹ ëŒ€ìƒ
                config['resampling']['borderline_smote_sampling_strategy'] = 0.1  # ê¸°ë³¸ê°’, íŠœë‹ ëŒ€ìƒ
                config['resampling']['borderline_smote_m_neighbors'] = 10  # ê¸°ë³¸ê°’, íŠœë‹ ëŒ€ìƒ
            elif method == 'adasyn':
                config['resampling']['adasyn_k_neighbors'] = 5  # ê¸°ë³¸ê°’, íŠœë‹ ëŒ€ìƒ
                config['resampling']['adasyn_sampling_strategy'] = 0.1  # ê¸°ë³¸ê°’, íŠœë‹ ëŒ€ìƒ
            elif method == 'under_sampling':
                config['resampling']['under_sampling_strategy'] = 'random'  # ê¸°ë³¸ê°’, íŠœë‹ ëŒ€ìƒ
            elif method == 'hybrid':
                config['resampling']['hybrid_strategy'] = 'smote_tomek'  # ê¸°ë³¸ê°’, íŠœë‹ ëŒ€ìƒ
            elif method == 'time_series_adapted':
                # ì‹œê³„ì—´ íŠ¹í™” ë¦¬ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°
                config['resampling']['time_series_adapted'] = {
                    'enabled': True,
                    'time_weight': 0.3,  # ê¸°ë³¸ê°’, íŠœë‹ ëŒ€ìƒ
                    'pattern_preservation': True,  # ê¸°ë³¸ê°’, íŠœë‹ ëŒ€ìƒ
                    'temporal_window': 3,  # ê¸°ë³¸ê°’, íŠœë‹ ëŒ€ìƒ
                    'seasonality_weight': 0.2,  # ê¸°ë³¸ê°’, íŠœë‹ ëŒ€ìƒ
                    'trend_preservation': True  # ê¸°ë³¸ê°’, íŠœë‹ ëŒ€ìƒ
                }
            # ë°ì´í„° ê²½ë¡œ ë°˜ì˜
            config['data']['data_path'] = data_path
            # MLflow ì¤‘ì²© ì‹¤í–‰
            with mlflow.start_run(run_name=f"resampling_tuning_{method}", nested=True):
                # ë¦¬ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°ë¥¼ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì— ì¶”ê°€
                if 'tuning' in config:
                    config['tuning'] = add_resampling_hyperparameters_to_tuning_config(
                        config['tuning'], method
                    )
                
                # íŠœë‹ íŒŒë¼ë¯¸í„° ë¡œê¹… 
                log_tuning_params(config, config)
                
                # íŠœë„ˆ ìƒì„± ë° íŠœë‹ ì‹¤í–‰
                tuner = HyperparameterTuner(config, train_val_df, nrows=nrows)
                result = tuner.optimize(start_mlflow_run=False)
                
                # ê²°ê³¼ ì¶”ì¶œ
                if result is None:
                    best_params = {}
                    best_score = 0.0
                    logger.warning(f"{method} íŠœë‹ ê²°ê³¼ê°€ Noneì…ë‹ˆë‹¤.")
                elif isinstance(result, tuple) and len(result) >= 2:
                    best_params = result[0] if result[0] is not None else {}
                    best_score = result[1] if result[1] is not None else 0.0
                else:
                    best_params = {}
                    best_score = 0.0
                    logger.warning(f"{method} ì˜ˆìƒì¹˜ ëª»í•œ ê²°ê³¼ íƒ€ì…: {type(result)}")
                
                # ê²°ê³¼ ì €ì¥
                comparison_results['methods'][method] = {
                    'best_params': best_params,
                    'best_score': best_score
                }
                
                # ìµœê³  ì„±ëŠ¥ ì—…ë°ì´íŠ¸
                if best_score > comparison_results['best_score']:
                    comparison_results['best_score'] = best_score
                    comparison_results['best_method'] = method
                
                logger.info(f"{method} ìµœê³  ì„±ëŠ¥: {best_score:.4f}")
                logger.info(f"{method} ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")
                
                # íŠœë‹ ë¡œê·¸ ì €ì¥
                try:
                    from src.hyperparameter_tuning import save_tuning_log
                    save_tuning_log(
                        result=(best_params, best_score),
                        model_type=model_type,
                        experiment_type=experiment_type,
                        nrows=nrows,
                        experiment_id=mlflow.active_run().info.experiment_id,
                        run_id=mlflow.active_run().info.run_id,
                        log_file_path=getattr(tuner, 'log_file_path', None)
                    )
                except Exception as e:
                    logger.error(f"{method} íŠœë‹ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")

    # ìµœì¢… ê²°ê³¼ ìš”ì•½
    logger.info("\n=== ë¦¬ìƒ˜í”Œë§ ë¹„êµ ì‹¤í—˜ ì™„ë£Œ ===")
    logger.info(f"ìµœê³  ì„±ëŠ¥ ê¸°ë²•: {comparison_results['best_method']}")
    logger.info(f"ìµœê³  ì„±ëŠ¥: {comparison_results['best_score']:.4f}")
    logger.info("\nê° ê¸°ë²•ë³„ ì„±ëŠ¥:")
    for method, method_result in comparison_results['methods'].items():
        logger.info(f"  {method}: {method_result['best_score']:.4f}")

    # ì‹¤í—˜ ê²°ê³¼ ì €ì¥
    from src.hyperparameter_tuning import save_experiment_results
    save_experiment_results(
        comparison_results,
        model_type,
        experiment_type,
        nrows,
        config=base_config,
        data_info=data_info
    )

    return comparison_results


def collect_data_info(df: pd.DataFrame, config: Dict[str, Any], train_val_df: pd.DataFrame = None, test_df: pd.DataFrame = None) -> Dict[str, Any]:
    """
    ì‹¤í—˜ì— ì‚¬ìš©ëœ ë°ì´í„°ì˜ ìƒì„¸ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    
    Args:
        df: ì „ì²´ ë°ì´í„°í”„ë ˆì„
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        train_val_df: í›ˆë ¨/ê²€ì¦ ë°ì´í„°í”„ë ˆì„ (ì„ íƒì‚¬í•­)
        test_df: í…ŒìŠ¤íŠ¸ ë°ì´í„°í”„ë ˆì„ (ì„ íƒì‚¬í•­)
        
    Returns:
        ë°ì´í„° ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    data_info = {}
    
    # ê¸°ë³¸ ë°ì´í„° ì •ë³´
    data_info['total_rows'] = len(df)
    data_info['total_columns'] = len(df.columns)
    
    # ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    if 'data' in config:
        data_info['data_path'] = config['data'].get('data_path', 'N/A')
    
    # ê°œì¸ ìˆ˜ (ID ì»¬ëŸ¼ ê¸°ì¤€)
    id_column = config.get('time_series', {}).get('id_column', 'id')
    if id_column in df.columns:
        data_info['unique_ids'] = df[id_column].nunique()
    
    # ë°ì´í„° ê¸°ê°„
    date_column = config.get('time_series', {}).get('date_column', 'dov')
    year_column = config.get('time_series', {}).get('year_column', 'yov')
    
    if date_column in df.columns:
        try:
            df[date_column] = pd.to_datetime(df[date_column])
            date_range = f"{df[date_column].min().strftime('%Y-%m-%d')} ~ {df[date_column].max().strftime('%Y-%m-%d')}"
            data_info['date_range'] = date_range
        except:
            data_info['date_range'] = 'N/A'
    elif year_column in df.columns:
        try:
            year_range = f"{df[year_column].min()} ~ {df[year_column].max()}"
            data_info['date_range'] = year_range
        except:
            data_info['date_range'] = 'N/A'
    else:
        data_info['date_range'] = 'N/A'
    
    # í´ë˜ìŠ¤ ë¶„í¬ ì •ë³´
    class_distributions = {}
    target_columns = config.get('features', {}).get('target_columns', [])
    
    # ì „ì²´ ë°ì´í„°ì˜ í´ë˜ìŠ¤ ë¶„í¬
    for target in target_columns:
        if target in df.columns:
            class_distributions[f'full_{target}'] = df[target].value_counts().to_dict()
    
    # í›ˆë ¨/ê²€ì¦ ë°ì´í„°ì˜ í´ë˜ìŠ¤ ë¶„í¬
    if train_val_df is not None:
        train_original = {}
        for target in target_columns:
            if target in train_val_df.columns:
                train_original[target] = train_val_df[target].value_counts().to_dict()
        class_distributions['train_original'] = train_original
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ í´ë˜ìŠ¤ ë¶„í¬
    if test_df is not None:
        test_dist = {}
        for target in target_columns:
            if target in test_df.columns:
                test_dist[target] = test_df[target].value_counts().to_dict()
        class_distributions['test'] = test_dist
    
    data_info['class_distributions'] = class_distributions
    
    return data_info


class ResamplingHyperparameterTuner(HyperparameterTuner):
    """
    ë¦¬ìƒ˜í”Œë§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë„ˆ
    
    ê¸°ì¡´ HyperparameterTunerë¥¼ í™•ì¥í•˜ì—¬ ë¦¬ìƒ˜í”Œë§ ë°©ë²•ë„ íŠœë‹ ë²”ìœ„ì— í¬í•¨í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, config: Dict[str, Any], train_val_df: pd.DataFrame, nrows: int = None):
        """
        ë¦¬ìƒ˜í”Œë§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë„ˆ ì´ˆê¸°í™”
        
        Args:
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
            train_val_df: í›ˆë ¨/ê²€ì¦ ë°ì´í„°
            nrows: ì‚¬ìš©í•  ë°ì´í„° í–‰ ìˆ˜
        """
        super().__init__(config, train_val_df, nrows=nrows)
        
        # ë¦¬ìƒ˜í”Œë§ ë°©ë²• ëª©ë¡ (ì„¤ì •ì—ì„œ ì§€ì •ëœ ë°©ë²•ì´ ìˆìœ¼ë©´ í•´ë‹¹ ë°©ë²•ë§Œ ì‚¬ìš©)
        config_resampling_method = config.get('imbalanced_data', {}).get('resampling', {}).get('method')
        if config_resampling_method and config_resampling_method != 'auto':
            self.resampling_methods = [config_resampling_method]
            logger.info(f"ì§€ì •ëœ ë¦¬ìƒ˜í”Œë§ ë°©ë²•ë§Œ ì‚¬ìš©: {config_resampling_method}")
        else:
            self.resampling_methods = [
                'none', 'smote', 'borderline_smote', 'adasyn', 'under_sampling', 'hybrid'
            ]
        
        logger.info(f"ë¦¬ìƒ˜í”Œë§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë„ˆ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"  - ì§€ì›í•˜ëŠ” ë¦¬ìƒ˜í”Œë§ ë°©ë²•: {self.resampling_methods}")
    
    def suggest_hyperparameters(self, trial: optuna.Trial) -> Dict[str, Any]:
        """
        í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤ (ë¦¬ìƒ˜í”Œë§ ë°©ë²• í¬í•¨).
        
        Args:
            trial: Optuna trial ê°ì²´
            
        Returns:
            ì œì•ˆëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
        """
        # ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì œì•ˆ
        params = super().suggest_hyperparameters(trial)
        
        # ë¦¬ìƒ˜í”Œë§ ë°©ë²• ì œì•ˆ
        resampling_method = trial.suggest_categorical('resampling_method', self.resampling_methods)
        params['resampling_method'] = resampling_method
        
        # ë¦¬ìƒ˜í”Œë§ ë°©ë²•ë³„ íŒŒë¼ë¯¸í„° ì œì•ˆ
        if resampling_method == 'smote':
            params['smote_k_neighbors'] = trial.suggest_int('smote_k_neighbors', 3, 10)
            params['smote_sampling_strategy'] = trial.suggest_float('smote_sampling_strategy', 0.05, 0.3)
        
        elif resampling_method == 'borderline_smote':
            params['borderline_smote_k_neighbors'] = trial.suggest_int('borderline_smote_k_neighbors', 3, 10)
            params['borderline_smote_m_neighbors'] = trial.suggest_int('borderline_smote_m_neighbors', 5, 15)
            params['borderline_smote_sampling_strategy'] = trial.suggest_float('borderline_smote_sampling_strategy', 0.05, 0.3)
        
        elif resampling_method == 'adasyn':
            params['adasyn_k_neighbors'] = trial.suggest_int('adasyn_k_neighbors', 3, 10)
            params['adasyn_n_neighbors'] = trial.suggest_int('adasyn_n_neighbors', 3, 10)
            params['adasyn_sampling_strategy'] = trial.suggest_float('adasyn_sampling_strategy', 0.05, 0.3)
        
        elif resampling_method == 'under_sampling':
            params['under_sampling_strategy'] = trial.suggest_categorical('under_sampling_strategy', ['random', 'tomek_links', 'edited_nearest_neighbours'])
            params['under_sampling_sampling_strategy'] = trial.suggest_float('under_sampling_sampling_strategy', 0.1, 0.5)
        
        elif resampling_method == 'hybrid':
            # HybridëŠ” SMOTE + Under Samplingì˜ ì¡°í•©
            params['hybrid_smote_k_neighbors'] = trial.suggest_int('hybrid_smote_k_neighbors', 3, 10)
            params['hybrid_smote_sampling_strategy'] = trial.suggest_float('hybrid_smote_sampling_strategy', 0.05, 0.3)
            params['hybrid_under_sampling_strategy'] = trial.suggest_categorical('hybrid_under_sampling_strategy', ['random', 'tomek_links'])
            params['hybrid_under_sampling_sampling_strategy'] = trial.suggest_float('hybrid_under_sampling_sampling_strategy', 0.1, 0.5)
        
        return params
    
    def apply_resampling_parameters_to_config(self, config: Dict[str, Any], trial_params: Dict[str, Any]) -> Dict[str, Any]:
        """
        ë¦¬ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •ì— ì ìš©í•©ë‹ˆë‹¤.
        
        Args:
            config: ì›ë³¸ ì„¤ì •
            trial_params: trialì—ì„œ ì œì•ˆëœ íŒŒë¼ë¯¸í„°
            
        Returns:
            ë¦¬ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°ê°€ ì ìš©ëœ ì„¤ì •
        """
        # ì„¤ì • ë³µì‚¬
        updated_config = config.copy()
        
        # ë¦¬ìƒ˜í”Œë§ í™œì„±í™”
        resampling_method = trial_params.get('resampling_method', 'none')
        
        if resampling_method != 'none':
            # ë¦¬ìƒ˜í”Œë§ í™œì„±í™”
            if 'imbalanced_data' not in updated_config:
                updated_config['imbalanced_data'] = {}
            if 'resampling' not in updated_config['imbalanced_data']:
                updated_config['imbalanced_data']['resampling'] = {}
            
            updated_config['imbalanced_data']['resampling']['enabled'] = True
            updated_config['imbalanced_data']['resampling']['method'] = resampling_method
            
            # ë¦¬ìƒ˜í”Œë§ ë°©ë²•ë³„ íŒŒë¼ë¯¸í„° ì ìš©
            if resampling_method == 'smote':
                if 'smote' not in updated_config['imbalanced_data']['resampling']:
                    updated_config['imbalanced_data']['resampling']['smote'] = {}
                
                updated_config['imbalanced_data']['resampling']['smote']['k_neighbors'] = trial_params.get('smote_k_neighbors', 5)
                updated_config['imbalanced_data']['resampling']['smote']['sampling_strategy'] = trial_params.get('smote_sampling_strategy', 'auto')
            
            elif resampling_method == 'borderline_smote':
                if 'borderline_smote' not in updated_config['imbalanced_data']['resampling']:
                    updated_config['imbalanced_data']['resampling']['borderline_smote'] = {}
                
                updated_config['imbalanced_data']['resampling']['borderline_smote']['k_neighbors'] = trial_params.get('borderline_smote_k_neighbors', 5)
                updated_config['imbalanced_data']['resampling']['borderline_smote']['m_neighbors'] = trial_params.get('borderline_smote_m_neighbors', 10)
                updated_config['imbalanced_data']['resampling']['borderline_smote']['sampling_strategy'] = trial_params.get('borderline_smote_sampling_strategy', 'auto')
            
            elif resampling_method == 'adasyn':
                if 'adasyn' not in updated_config['imbalanced_data']['resampling']:
                    updated_config['imbalanced_data']['resampling']['adasyn'] = {}
                
                updated_config['imbalanced_data']['resampling']['adasyn']['k_neighbors'] = trial_params.get('adasyn_k_neighbors', 5)
                updated_config['imbalanced_data']['resampling']['adasyn']['n_neighbors'] = trial_params.get('adasyn_n_neighbors', 5)
                updated_config['imbalanced_data']['resampling']['adasyn']['sampling_strategy'] = trial_params.get('adasyn_sampling_strategy', 'auto')
            
            elif resampling_method == 'under_sampling':
                if 'under_sampling' not in updated_config['imbalanced_data']['resampling']:
                    updated_config['imbalanced_data']['resampling']['under_sampling'] = {}
                
                updated_config['imbalanced_data']['resampling']['under_sampling']['strategy'] = trial_params.get('under_sampling_strategy', 'random')
                updated_config['imbalanced_data']['resampling']['under_sampling']['sampling_strategy'] = trial_params.get('under_sampling_sampling_strategy', 'auto')
            
            elif resampling_method == 'hybrid':
                # HybridëŠ” SMOTE + Under Samplingì˜ ì¡°í•©
                if 'hybrid' not in updated_config['imbalanced_data']['resampling']:
                    updated_config['imbalanced_data']['resampling']['hybrid'] = {}
                
                updated_config['imbalanced_data']['resampling']['hybrid']['smote_k_neighbors'] = trial_params.get('hybrid_smote_k_neighbors', 5)
                updated_config['imbalanced_data']['resampling']['hybrid']['smote_sampling_strategy'] = trial_params.get('hybrid_smote_sampling_strategy', 'auto')
                updated_config['imbalanced_data']['resampling']['hybrid']['under_sampling_strategy'] = trial_params.get('hybrid_under_sampling_strategy', 'random')
                updated_config['imbalanced_data']['resampling']['hybrid']['under_sampling_sampling_strategy'] = trial_params.get('hybrid_under_sampling_sampling_strategy', 'auto')
        else:
            # ë¦¬ìƒ˜í”Œë§ ë¹„í™œì„±í™”
            if 'imbalanced_data' in updated_config and 'resampling' in updated_config['imbalanced_data']:
                updated_config['imbalanced_data']['resampling']['enabled'] = False
        
        return updated_config
    
    def objective(self, trial: optuna.Trial) -> float:
        """
        Optuna objective í•¨ìˆ˜ (ë¦¬ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° í¬í•¨).
        
        Args:
            trial: Optuna trial ê°ì²´
            
        Returns:
            í‰ê°€ ì ìˆ˜
        """
        try:
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì œì•ˆ (ë¦¬ìƒ˜í”Œë§ í¬í•¨)
            trial_params = self.suggest_hyperparameters(trial)
            
            # ë¦¬ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •ì— ì ìš©
            updated_config = self.apply_resampling_parameters_to_config(self.config, trial_params)
            
            # ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶”ì¶œ
            model_params = {}
            for key, value in trial_params.items():
                if not key.startswith(('resampling_', 'smote_', 'borderline_smote_', 'adasyn_', 'under_sampling_', 'hybrid_')):
                    model_params[key] = value
            
            # ëª¨ë¸ íƒ€ì… í™•ì¸
            model_type = self.config.get('model', {}).get('model_type', 'catboost')
            
            # MLflow íŒŒë¼ë¯¸í„° ë¡œê¹… (ì•ˆì „í•œ ë¡œê¹…)
            from src.utils.mlflow_manager import safe_log_param
            
            for param, value in trial_params.items():
                safe_log_param(param, value, logger)
            
            # êµì°¨ ê²€ì¦ ìˆ˜í–‰
            from src.training import cross_validate_model
            
            # êµì°¨ ê²€ì¦ ì‹¤í–‰
            cv_results = cross_validate_model(
                data=self.data,
                config=updated_config,
                model_params=model_params,
                model_type=model_type,
                n_splits=self.config.get('validation', {}).get('num_cv_folds', 5),
                random_state=self.config.get('validation', {}).get('random_state', 42)
            )
            
            # ì£¼ìš” ë©”íŠ¸ë¦­ ì¶”ì¶œ
            primary_metric = self.config.get('evaluation', {}).get('primary_metric', 'f1')
            if primary_metric in cv_results:
                score = cv_results[primary_metric]
            else:
                # ê¸°ë³¸ê°’ìœ¼ë¡œ f1 ì‚¬ìš©
                score = cv_results.get('f1', 0.0)
            
            # MLflow ë©”íŠ¸ë¦­ ë¡œê¹…
            for metric, value in cv_results.items():
                if isinstance(value, (int, float)) and not np.isnan(value) and not np.isinf(value):
                    mlflow.log_metric(metric, value)
            
            logger.info(f"Trial {trial.number} ì™„ë£Œ: {primary_metric} = {score:.4f}")
            return score
            
        except Exception as e:
            logger.error(f"Trial {trial.number} ì‹¤íŒ¨: {e}")
            return 0.0
        finally:
            # === ë¡œê·¸ í•¸ë“¤ëŸ¬ ë° í‘œì¤€ì¶œë ¥ flush ì¶”ê°€ ===
            import sys
            for handler in logger.handlers:
                try:
                    handler.flush()
                except Exception:
                    pass
            try:
                sys.stdout.flush()
            except Exception:
                pass


def main():
    """ë©”ì¸ í•¨ìˆ˜ (ConfigManager ê¸°ë°˜ ê³„ì¸µì  config ë³‘í•© ì§€ì›)"""
    parser = argparse.ArgumentParser(
        description="ë¦¬ìƒ˜í”Œë§ ì‹¤í—˜ ì‹¤í–‰ (ConfigManager ì§€ì›)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ê³„ì¸µì  configë¡œ ë‹¨ì¼ ë¦¬ìƒ˜í”Œë§ ì‹¤í—˜ ì‹¤í–‰
  python scripts/run_resampling_experiment.py --model-type xgboost --experiment-type resampling_experiment
  
  # ë¦¬ìƒ˜í”Œë§ ê¸°ë²• ë¹„êµ ì‹¤í—˜ ì‹¤í–‰
  python scripts/run_resampling_experiment.py --model-type xgboost --resampling-comparison --resampling-methods smote adasyn
  
  # íŠ¹ì • ë¦¬ìƒ˜í”Œë§ ë°©ë²• ì§€ì •
  python scripts/run_resampling_experiment.py --model-type catboost --resampling-method smote
  
  # legacy ë‹¨ì¼ íŒŒì¼ configë¡œ ë¦¬ìƒ˜í”Œë§ ì‹¤í—˜ ì‹¤í–‰
  python scripts/run_resampling_experiment.py --tuning_config configs/resampling_experiment.yaml --base_config configs/default_config.yaml
        """
    )
    parser.add_argument("--model-type", type=str, choices=['xgboost', 'lightgbm', 'random_forest', 'catboost'], default=None, help="ì‚¬ìš©í•  ëª¨ë¸ íƒ€ì… (ê³„ì¸µì  config ë³‘í•©)")
    parser.add_argument("--experiment-type", type=str, default=None, help="ì‹¤í—˜ íƒ€ì… (resampling_experiment ë“±)")
    parser.add_argument("--tuning_config", type=str, default=None, help="(legacy) íŠœë‹ ì„¤ì • íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--base_config", type=str, default=None, help="(legacy) ê¸°ë³¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--data_path", type=str, default=None, help="ë°ì´í„° íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--nrows", type=int, default=None, help="ì‚¬ìš©í•  ë°ì´í„° í–‰ ìˆ˜")
    parser.add_argument("--mlflow_ui", action="store_true", help="ì‹¤í—˜ ì™„ë£Œ í›„ MLflow UI ì‹¤í–‰")
    
    # ìë™í™”ëœ ì‹¤í—˜ì„ ìœ„í•œ ì¶”ê°€ ì¸ìë“¤
    parser.add_argument("--split-strategy", type=str, choices=['group_kfold', 'time_series_walk_forward', 'time_series_group_kfold'], default=None, help="ë°ì´í„° ë¶„í•  ì „ëµ")
    parser.add_argument("--cv-folds", type=int, default=None, help="êµì°¨ ê²€ì¦ í´ë“œ ìˆ˜")
    parser.add_argument("--n-trials", type=int, default=None, help="í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œë„ íšŸìˆ˜")
    parser.add_argument("--tuning-direction", type=str, choices=['maximize', 'minimize'], default=None, help="íŠœë‹ ë°©í–¥ (maximize/minimize)")
    parser.add_argument("--primary-metric", type=str, default=None, help="ì£¼ìš” í‰ê°€ ì§€í‘œ (f1, precision, recall, mcc, roc_auc, pr_auc ë“±)")
    parser.add_argument("--test-size", type=float, default=None, help="í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¹„ìœ¨ (0.0-1.0)")
    parser.add_argument("--random-state", type=int, default=None, help="ëœë¤ ì‹œë“œ")
    parser.add_argument("--n-jobs", type=int, default=None, help="ë³‘ë ¬ ì²˜ë¦¬ ì‘ì—… ìˆ˜")
    parser.add_argument("--timeout", type=int, default=None, help="íŠœë‹ íƒ€ì„ì•„ì›ƒ (ì´ˆ)")
    parser.add_argument("--early-stopping", action="store_true", help="Early stopping í™œì„±í™”")
    parser.add_argument("--early-stopping-rounds", type=int, default=None, help="Early stopping ë¼ìš´ë“œ ìˆ˜")
    parser.add_argument("--feature-selection", action="store_true", help="í”¼ì²˜ ì„ íƒ í™œì„±í™”")
    parser.add_argument("--feature-selection-method", type=str, choices=['mutual_info', 'chi2', 'f_classif', 'recursive'], default=None, help="í”¼ì²˜ ì„ íƒ ë°©ë²•")
    parser.add_argument("--feature-selection-k", type=int, default=None, help="ì„ íƒí•  í”¼ì²˜ ìˆ˜")
    parser.add_argument("--experiment-name", type=str, default=None, help="MLflow ì‹¤í—˜ ì´ë¦„ (ê¸°ë³¸ê°’: model_type_experiment_type)")
    parser.add_argument("--save-model", action="store_true", help="ìµœì  ëª¨ë¸ ì €ì¥")
    parser.add_argument("--save-predictions", action="store_true", help="ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥")
    parser.add_argument("--verbose", type=int, choices=[0, 1, 2], default=1, help="ë¡œê·¸ ë ˆë²¨ (0: ìµœì†Œ, 1: ê¸°ë³¸, 2: ìƒì„¸)")
    parser.add_argument("--resampling-method", type=str, choices=['none', 'smote', 'borderline_smote', 'adasyn', 'under_sampling', 'hybrid'], default=None, help="íŠ¹ì • ë¦¬ìƒ˜í”Œë§ ë°©ë²• ì§€ì • (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ íŠœë‹ì—ì„œ ìë™ ì„ íƒ)")
    parser.add_argument("--resampling-comparison", action="store_true", help="ë¦¬ìƒ˜í”Œë§ ê¸°ë²• ë¹„êµ ì‹¤í—˜ ì‹¤í–‰")
    parser.add_argument("--resampling-methods", nargs="+", choices=['none', 'smote', 'borderline_smote', 'adasyn', 'under_sampling', 'hybrid', 'time_series_adapted'], default=None, help="ë¹„êµí•  ë¦¬ìƒ˜í”Œë§ ê¸°ë²•ë“¤")
    parser.add_argument("--resampling-enabled", action="store_true", help="ë¦¬ìƒ˜í”Œë§ í™œì„±í™”")
    parser.add_argument("--resampling-ratio", type=float, default=None, help="ë¦¬ìƒ˜í”Œë§ í›„ ì–‘ì„± í´ë˜ìŠ¤ ë¹„ìœ¨")
    args = parser.parse_args()
    
    try:
        if args.model_type:
            # ConfigManager ê¸°ë°˜ ê³„ì¸µì  config ë³‘í•©
            config_manager = ConfigManager()
            config = config_manager.create_experiment_config(args.model_type, args.experiment_type or 'resampling_experiment')
            
            # ëª…ë ¹í–‰ ì¸ìë¥¼ configì— ì ìš©
            config = config_manager.apply_command_line_args(config, args)
            
            # ë¦¬ìƒ˜í”Œë§ ë°©ë²•ì´ ì§€ì •ëœ ê²½ìš° ì„¤ì •ì— ì ìš©
            if args.resampling_method:
                if 'imbalanced_data' not in config:
                    config['imbalanced_data'] = {}
                if 'resampling' not in config['imbalanced_data']:
                    config['imbalanced_data']['resampling'] = {}
                config['imbalanced_data']['resampling']['method'] = args.resampling_method
                logger.info(f"ì§€ì •ëœ ë¦¬ìƒ˜í”Œë§ ë°©ë²•: {args.resampling_method}")
            
            # ì„¤ì • ìš”ì•½ ì¶œë ¥
            config_manager.print_config_summary(config)
            
            # ë¦¬ìƒ˜í”Œë§ ë¹„êµ ì‹¤í—˜ì¸ì§€ í™•ì¸
            if args.resampling_comparison and args.resampling_methods:
                # ë¦¬ìƒ˜í”Œë§ ë¹„êµ ì‹¤í—˜ ì‹¤í–‰
                result = run_resampling_comparison_experiment(
                    model_type=args.model_type,
                    experiment_type=args.experiment_type or 'resampling_experiment',
                    data_path=args.data_path or config.get('data', {}).get('file_path', 'data/processed/processed_data_with_features.csv'),
                    resampling_methods=args.resampling_methods,
                    nrows=args.nrows
                )
            else:
                # ë‹¨ì¼ ë¦¬ìƒ˜í”Œë§ ì‹¤í—˜ ì‹¤í–‰
                result = run_resampling_experiment_with_config(
                    config,
                    data_path=args.data_path,
                    nrows=args.nrows
                )
            # ì‹¤í—˜ ê²°ê³¼ ì €ì¥ì€ ê° í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì²˜ë¦¬ë¨
        elif args.tuning_config and args.base_config:
            # legacy: ë‹¨ì¼ íŒŒì¼ ê¸°ë°˜
            raise NotImplementedError("Legacy ëª¨ë“œëŠ” ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ConfigManagerë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")
        else:
            raise ValueError("model-type ë˜ëŠ” tuning_config+base_config ì¤‘ í•˜ë‚˜ëŠ” ë°˜ë“œì‹œ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        # ê²°ê³¼ ì¶œë ¥ (ë¹„êµ ì‹¤í—˜ì¸ì§€ ë‹¨ì¼ ì‹¤í—˜ì¸ì§€ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì²˜ë¦¬)
        if args.resampling_comparison and args.resampling_methods:
            print(f"\nğŸ‰ ë¦¬ìƒ˜í”Œë§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë¹„êµ ì‹¤í—˜ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            print(f"ğŸ“Š ìµœê³  ì„±ëŠ¥ ê¸°ë²•: {result.get('best_method', 'none')}")
            print(f"âš™ï¸  ìµœê³  ì„±ëŠ¥: {result.get('best_score', 0.0):.4f}")
            print(f"\nğŸ“ˆ ê° ê¸°ë²•ë³„ ì„±ëŠ¥:")
            for method, method_result in result.get('methods', {}).items():
                print(f"  {method}: {method_result['best_score']:.4f}")
        else:
            best_params, best_score, _, _ = result
            print(f"\nğŸ‰ ë¦¬ìƒ˜í”Œë§ ì‹¤í—˜ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            print(f"ğŸ“Š ìµœê³  ì„±ëŠ¥: {best_score:.4f}")
            print(f"âš™ï¸  ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")
        
        if args.mlflow_ui:
            print(f"\nğŸŒ MLflow UIë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤...")
            print(f"   ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:5000 ìœ¼ë¡œ ì ‘ì†í•˜ì„¸ìš”.")
            print(f"   ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”.")
            import subprocess
            try:
                subprocess.run(["mlflow", "ui"], check=True)
            except KeyboardInterrupt:
                print(f"\nğŸ‘‹ MLflow UIë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                
    except Exception as e:
        logger.error(f"ë¦¬ìƒ˜í”Œë§ ì‹¤í—˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


if __name__ == "__main__":
    main() 