#!/usr/bin/env python3
"""
ë¦¬ìƒ˜í”Œë§ ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸

í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ê³¼ ë™ì¼í•œ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë©´ì„œ ë¦¬ìƒ˜í”Œë§ ë°©ë²•ì„ íŠœë‹ ë²”ìœ„ì— í¬í•¨í•˜ëŠ” ì‹¤í—˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import argparse
import logging
import os
import sys
import time
from typing import Dict, Any, List, Optional, Tuple

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import numpy as np
import mlflow
import optuna
from datetime import datetime

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import
from src.utils.config_manager import ConfigManager
from src.utils import setup_logging, setup_experiment_logging, log_experiment_summary, experiment_logging_context
from src.hyperparameter_tuning import HyperparameterTuner
from src.splits import split_test_set, validate_splits


# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


def setup_mlflow_experiment(experiment_name: str):
    """
    MLflow ì‹¤í—˜ì„ ì„¤ì •í•©ë‹ˆë‹¤.
    
    Args:
        experiment_name: ì‹¤í—˜ ì´ë¦„
        
    Returns:
        ì‹¤í—˜ ID
    """
    from src.utils.mlflow_manager import setup_mlflow_experiment_safely
    
    experiment_id = setup_mlflow_experiment_safely(experiment_name)
    logger.info(f"MLflow ì‹¤í—˜ ì„¤ì •: {experiment_name} (ID: {experiment_id})")
    return experiment_id


def log_tuning_params(config: Dict[str, Any], base_config: Dict[str, Any]):
    """
    íŠœë‹ íŒŒë¼ë¯¸í„°ë¥¼ MLflowì— ë¡œê¹…í•©ë‹ˆë‹¤.
    
    Args:
        config: íŠœë‹ ì„¤ì •
        base_config: ê¸°ë³¸ ì„¤ì •
    """
    # ë¦¬ìƒ˜í”Œë§ ê´€ë ¨ íŒŒë¼ë¯¸í„° ë¡œê¹…
    resampling_config = config.get('imbalanced_data', {}).get('resampling', {})
    
    # ë¦¬ìƒ˜í”Œë§ í™œì„±í™” ì—¬ë¶€
    mlflow.log_param("resampling_enabled", resampling_config.get('enabled', False))
    
    if resampling_config.get('enabled', False):
        # ë¦¬ìƒ˜í”Œë§ ë°©ë²•
        mlflow.log_param("resampling_method", resampling_config.get('method', 'none'))
        
        # SMOTE ê´€ë ¨ íŒŒë¼ë¯¸í„°
        smote_config = resampling_config.get('smote', {})
        mlflow.log_param("smote_k_neighbors", smote_config.get('k_neighbors', 5))
        mlflow.log_param("smote_sampling_strategy", smote_config.get('sampling_strategy', 'auto'))
        
        # Borderline SMOTE ê´€ë ¨ íŒŒë¼ë¯¸í„°
        borderline_smote_config = resampling_config.get('borderline_smote', {})
        mlflow.log_param("borderline_smote_k_neighbors", borderline_smote_config.get('k_neighbors', 5))
        mlflow.log_param("borderline_smote_m_neighbors", borderline_smote_config.get('m_neighbors', 10))
        mlflow.log_param("borderline_smote_sampling_strategy", borderline_smote_config.get('sampling_strategy', 'auto'))
        
        # ADASYN ê´€ë ¨ íŒŒë¼ë¯¸í„°
        adasyn_config = resampling_config.get('adasyn', {})
        mlflow.log_param("adasyn_k_neighbors", adasyn_config.get('k_neighbors', 5))
        mlflow.log_param("adasyn_n_neighbors", adasyn_config.get('n_neighbors', 5))
        mlflow.log_param("adasyn_sampling_strategy", adasyn_config.get('sampling_strategy', 'auto'))
        
        # Under Sampling ê´€ë ¨ íŒŒë¼ë¯¸í„°
        under_sampling_config = resampling_config.get('under_sampling', {})
        mlflow.log_param("under_sampling_strategy", under_sampling_config.get('strategy', 'random'))
        mlflow.log_param("under_sampling_sampling_strategy", under_sampling_config.get('sampling_strategy', 'auto'))
    
    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê´€ë ¨ íŒŒë¼ë¯¸í„°
    mlflow.log_param("class_weights", config.get('imbalanced_data', {}).get('class_weights', 'Balanced'))
    mlflow.log_param("auto_class_weights", config.get('imbalanced_data', {}).get('auto_class_weights', 'Balanced'))
    mlflow.log_param("scale_pos_weight", config.get('imbalanced_data', {}).get('scale_pos_weight', 1.0))
    
    logger.info(f"íŠœë‹ íŒŒë¼ë¯¸í„° ë¡œê¹… ì™„ë£Œ: {len(resampling_config)}ê°œ íŒŒë¼ë¯¸í„°")


def run_resampling_experiment_with_config(
    config: Dict[str, Any], 
    data_path: str = None, 
    nrows: int = None
) -> Tuple[Dict[str, Any], float, Any, Any]:
    """
    ì„¤ì • ê¸°ë°˜ ë¦¬ìƒ˜í”Œë§ ì‹¤í—˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    
    Args:
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        data_path: ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        nrows: ì‚¬ìš©í•  ë°ì´í„° í–‰ ìˆ˜
        
    Returns:
        íŠœë‹ ê²°ê³¼ íŠœí”Œ (best_params, best_score, study, tuner)
    """
    import time
    start_time = time.time()
    
    # ì‹¤í—˜ ì •ë³´ ì¶”ì¶œ
    model_type = config.get('model', {}).get('model_type', 'unknown')
    experiment_type = config.get('experiment_type', 'resampling_experiment')
    
    # ìƒˆë¡œìš´ ë¡œê¹… ì‹œìŠ¤í…œ ì ìš©
    with experiment_logging_context(
        experiment_type=experiment_type,
        model_type=model_type,
        log_level="INFO",
        capture_console=True
    ) as log_file_path:
        
        logger.info(f"=== ë¦¬ìƒ˜í”Œë§ ì‹¤í—˜ ì‹œì‘ ===")
        logger.info(f"ëª¨ë¸ íƒ€ì…: {model_type}")
        logger.info(f"ì‹¤í—˜ íƒ€ì…: {experiment_type}")
        logger.info(f"ë¡œê·¸ íŒŒì¼: {log_file_path}")
        
        try:
            # ë°ì´í„° ê²½ë¡œ ì„¤ì •
            if data_path is None:
                data_path = config['data'].get('file_path', 'data/processed/processed_data_with_features.csv')
            
            # ë°ì´í„° ë¡œë“œ
            logger.info(f"ë°ì´í„° ë¡œë“œ ì¤‘: {data_path}")
            if nrows:
                df = pd.read_csv(data_path, nrows=nrows)
                logger.info(f"í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë¡œë“œ: {len(df):,} í–‰")
            else:
                df = pd.read_csv(data_path)
                logger.info(f"ì „ì²´ ë°ì´í„° ë¡œë“œ: {len(df):,} í–‰")
            
            # ë°ì´í„° ì •ë³´ ìˆ˜ì§‘
            data_info = collect_data_info(df, config)
            logger.info(f"ë°ì´í„° ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {data_info['total_rows']} í–‰, {data_info['total_columns']} ì—´")
            
            # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„ë¦¬
            logger.info("=== í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„ë¦¬ ===")
            train_val_df, test_df, test_ids = split_test_set(df, config)
            
            # ë°ì´í„° ì •ë³´ ì—…ë°ì´íŠ¸ (í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì •ë³´ ì¶”ê°€)
            data_info = collect_data_info(df, config, train_val_df, test_df)
            
            # ë¶„í•  ê²€ì¦
            if not validate_splits(train_val_df, test_df, config):
                logger.error("ë¶„í•  ê²€ì¦ ì‹¤íŒ¨!")
                return None
            
            # MLflow ì‹¤í—˜ ì„¤ì •
            experiment_name = config['mlflow']['experiment_name']
            experiment_id = setup_mlflow_experiment(experiment_name)
            
            # íŠœë„ˆ ìƒì„± (ë¦¬ìƒ˜í”Œë§ ì‹¤í—˜ìš©)
            tuner = ResamplingHyperparameterTuner(config, train_val_df, nrows=nrows)
            
            # íŠœë‹ ì‹¤í–‰
            with mlflow.start_run(experiment_id=experiment_id) as run:
                logger.info(f"MLflow Run ì‹œì‘: {run.info.run_id}")
                
                # ì„¤ì • ë¡œê¹…
                log_tuning_params(config, config)  # configë¥¼ base_configë¡œë„ ì‚¬ìš©
                
                # íŠœë‹ ì‹¤í–‰
                result = tuner.optimize(start_mlflow_run=False)  # ì´ë¯¸ runì´ ì‹œì‘ë˜ì–´ ìˆìœ¼ë¯€ë¡œ False
                
                # ê²°ê³¼ ì €ì¥
                try:
                    tuner.save_results()
                    logger.info("=== ë¦¬ìƒ˜í”Œë§ ì‹¤í—˜ ì™„ë£Œ ===")
                except Exception as e:
                    logger.error(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
                    # ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                
                # ê²°ê³¼ ì¶”ì¶œ (ì•ˆì „í•˜ê²Œ)
                if result is None:
                    best_params = {}
                    best_score = 0.0
                    logger.warning("íŠœë‹ ê²°ê³¼ê°€ Noneì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                elif isinstance(result, tuple) and len(result) >= 2:
                    best_params = result[0] if result[0] is not None else {}
                    best_score = result[1] if result[1] is not None else 0.0
                elif isinstance(result, dict):
                    best_params = result.get('best_params', {})
                    best_score = result.get('best_score', 0.0)
                else:
                    best_params = {}
                    best_score = 0.0
                    logger.warning(f"ì˜ˆìƒì¹˜ ëª»í•œ ê²°ê³¼ íƒ€ì…: {type(result)}")
                
                logger.info(f"ìµœê³  ì„±ëŠ¥: {best_score:.4f}")
                logger.info("ìµœì  íŒŒë¼ë¯¸í„°:")
                for param, value in best_params.items():
                    logger.info(f"  {param}: {value}")
                
                # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
                execution_time = time.time() - start_time
                
                # ì‹¤í—˜ ìš”ì•½ ë¡œê¹…
                n_trials = len(tuner.study.trials) if hasattr(tuner, 'study') and tuner.study else 0
                log_experiment_summary(
                    experiment_type=experiment_type,
                    model_type=model_type,
                    best_score=best_score,
                    best_params=best_params,
                    execution_time=execution_time,
                    n_trials=n_trials,
                    data_info=data_info,
                    log_file_path=log_file_path
                )
                
                logger.info(f"=== ë¦¬ìƒ˜í”Œë§ ì‹¤í—˜ ì™„ë£Œ ===")
                logger.info(f"ìµœê³  ì„±ëŠ¥: {best_score:.6f}")
                logger.info(f"ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")
                logger.info(f"ì‹œë„ íšŸìˆ˜: {n_trials}")
                
                return best_params, best_score, run.info.experiment_id, run.info.run_id
                
        except Exception as e:
            logger.error(f"ë¦¬ìƒ˜í”Œë§ ì‹¤í—˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise


def collect_data_info(df: pd.DataFrame, config: Dict[str, Any], train_val_df: pd.DataFrame = None, test_df: pd.DataFrame = None) -> Dict[str, Any]:
    """
    ì‹¤í—˜ì— ì‚¬ìš©ëœ ë°ì´í„°ì˜ ìƒì„¸ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    
    Args:
        df: ì „ì²´ ë°ì´í„°í”„ë ˆì„
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        train_val_df: í›ˆë ¨/ê²€ì¦ ë°ì´í„°í”„ë ˆì„ (ì„ íƒì‚¬í•­)
        test_df: í…ŒìŠ¤íŠ¸ ë°ì´í„°í”„ë ˆì„ (ì„ íƒì‚¬í•­)
        
    Returns:
        ë°ì´í„° ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    data_info = {}
    
    # ê¸°ë³¸ ë°ì´í„° ì •ë³´
    data_info['total_rows'] = len(df)
    data_info['total_columns'] = len(df.columns)
    
    # ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    if 'data' in config:
        data_info['data_path'] = config['data'].get('data_path', 'N/A')
    
    # ê°œì¸ ìˆ˜ (ID ì»¬ëŸ¼ ê¸°ì¤€)
    id_column = config.get('time_series', {}).get('id_column', 'id')
    if id_column in df.columns:
        data_info['unique_ids'] = df[id_column].nunique()
    
    # ë°ì´í„° ê¸°ê°„
    date_column = config.get('time_series', {}).get('date_column', 'dov')
    year_column = config.get('time_series', {}).get('year_column', 'yov')
    
    if date_column in df.columns:
        try:
            df[date_column] = pd.to_datetime(df[date_column])
            date_range = f"{df[date_column].min().strftime('%Y-%m-%d')} ~ {df[date_column].max().strftime('%Y-%m-%d')}"
            data_info['date_range'] = date_range
        except:
            data_info['date_range'] = 'N/A'
    elif year_column in df.columns:
        try:
            year_range = f"{df[year_column].min()} ~ {df[year_column].max()}"
            data_info['date_range'] = year_range
        except:
            data_info['date_range'] = 'N/A'
    else:
        data_info['date_range'] = 'N/A'
    
    # í´ë˜ìŠ¤ ë¶„í¬ ì •ë³´
    class_distributions = {}
    target_columns = config.get('features', {}).get('target_columns', [])
    
    # ì „ì²´ ë°ì´í„°ì˜ í´ë˜ìŠ¤ ë¶„í¬
    for target in target_columns:
        if target in df.columns:
            class_distributions[f'full_{target}'] = df[target].value_counts().to_dict()
    
    # í›ˆë ¨/ê²€ì¦ ë°ì´í„°ì˜ í´ë˜ìŠ¤ ë¶„í¬
    if train_val_df is not None:
        train_original = {}
        for target in target_columns:
            if target in train_val_df.columns:
                train_original[target] = train_val_df[target].value_counts().to_dict()
        class_distributions['train_original'] = train_original
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ í´ë˜ìŠ¤ ë¶„í¬
    if test_df is not None:
        test_dist = {}
        for target in target_columns:
            if target in test_df.columns:
                test_dist[target] = test_df[target].value_counts().to_dict()
        class_distributions['test'] = test_dist
    
    data_info['class_distributions'] = class_distributions
    
    return data_info


class ResamplingHyperparameterTuner(HyperparameterTuner):
    """
    ë¦¬ìƒ˜í”Œë§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë„ˆ
    
    ê¸°ì¡´ HyperparameterTunerë¥¼ í™•ì¥í•˜ì—¬ ë¦¬ìƒ˜í”Œë§ ë°©ë²•ë„ íŠœë‹ ë²”ìœ„ì— í¬í•¨í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, config: Dict[str, Any], train_val_df: pd.DataFrame, nrows: int = None):
        """
        ë¦¬ìƒ˜í”Œë§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë„ˆ ì´ˆê¸°í™”
        
        Args:
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
            train_val_df: í›ˆë ¨/ê²€ì¦ ë°ì´í„°
            nrows: ì‚¬ìš©í•  ë°ì´í„° í–‰ ìˆ˜
        """
        super().__init__(config, train_val_df, nrows=nrows)
        
        # ë¦¬ìƒ˜í”Œë§ ë°©ë²• ëª©ë¡ (ì„¤ì •ì—ì„œ ì§€ì •ëœ ë°©ë²•ì´ ìˆìœ¼ë©´ í•´ë‹¹ ë°©ë²•ë§Œ ì‚¬ìš©)
        config_resampling_method = config.get('imbalanced_data', {}).get('resampling', {}).get('method')
        if config_resampling_method and config_resampling_method != 'auto':
            self.resampling_methods = [config_resampling_method]
            logger.info(f"ì§€ì •ëœ ë¦¬ìƒ˜í”Œë§ ë°©ë²•ë§Œ ì‚¬ìš©: {config_resampling_method}")
        else:
            self.resampling_methods = [
                'none', 'smote', 'borderline_smote', 'adasyn', 'under_sampling', 'hybrid'
            ]
        
        logger.info(f"ë¦¬ìƒ˜í”Œë§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë„ˆ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"  - ì§€ì›í•˜ëŠ” ë¦¬ìƒ˜í”Œë§ ë°©ë²•: {self.resampling_methods}")
    
    def suggest_hyperparameters(self, trial: optuna.Trial) -> Dict[str, Any]:
        """
        í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤ (ë¦¬ìƒ˜í”Œë§ ë°©ë²• í¬í•¨).
        
        Args:
            trial: Optuna trial ê°ì²´
            
        Returns:
            ì œì•ˆëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
        """
        # ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì œì•ˆ
        params = super().suggest_hyperparameters(trial)
        
        # ë¦¬ìƒ˜í”Œë§ ë°©ë²• ì œì•ˆ
        resampling_method = trial.suggest_categorical('resampling_method', self.resampling_methods)
        params['resampling_method'] = resampling_method
        
        # ë¦¬ìƒ˜í”Œë§ ë°©ë²•ë³„ íŒŒë¼ë¯¸í„° ì œì•ˆ
        if resampling_method == 'smote':
            params['smote_k_neighbors'] = trial.suggest_int('smote_k_neighbors', 3, 10)
            params['smote_sampling_strategy'] = trial.suggest_float('smote_sampling_strategy', 0.05, 0.3)
        
        elif resampling_method == 'borderline_smote':
            params['borderline_smote_k_neighbors'] = trial.suggest_int('borderline_smote_k_neighbors', 3, 10)
            params['borderline_smote_m_neighbors'] = trial.suggest_int('borderline_smote_m_neighbors', 5, 15)
            params['borderline_smote_sampling_strategy'] = trial.suggest_float('borderline_smote_sampling_strategy', 0.05, 0.3)
        
        elif resampling_method == 'adasyn':
            params['adasyn_k_neighbors'] = trial.suggest_int('adasyn_k_neighbors', 3, 10)
            params['adasyn_n_neighbors'] = trial.suggest_int('adasyn_n_neighbors', 3, 10)
            params['adasyn_sampling_strategy'] = trial.suggest_float('adasyn_sampling_strategy', 0.05, 0.3)
        
        elif resampling_method == 'under_sampling':
            params['under_sampling_strategy'] = trial.suggest_categorical('under_sampling_strategy', ['random', 'tomek_links', 'edited_nearest_neighbours'])
            params['under_sampling_sampling_strategy'] = trial.suggest_float('under_sampling_sampling_strategy', 0.1, 0.5)
        
        elif resampling_method == 'hybrid':
            # HybridëŠ” SMOTE + Under Samplingì˜ ì¡°í•©
            params['hybrid_smote_k_neighbors'] = trial.suggest_int('hybrid_smote_k_neighbors', 3, 10)
            params['hybrid_smote_sampling_strategy'] = trial.suggest_float('hybrid_smote_sampling_strategy', 0.05, 0.3)
            params['hybrid_under_sampling_strategy'] = trial.suggest_categorical('hybrid_under_sampling_strategy', ['random', 'tomek_links'])
            params['hybrid_under_sampling_sampling_strategy'] = trial.suggest_float('hybrid_under_sampling_sampling_strategy', 0.1, 0.5)
        
        return params
    
    def apply_resampling_parameters_to_config(self, config: Dict[str, Any], trial_params: Dict[str, Any]) -> Dict[str, Any]:
        """
        ë¦¬ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •ì— ì ìš©í•©ë‹ˆë‹¤.
        
        Args:
            config: ì›ë³¸ ì„¤ì •
            trial_params: trialì—ì„œ ì œì•ˆëœ íŒŒë¼ë¯¸í„°
            
        Returns:
            ë¦¬ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°ê°€ ì ìš©ëœ ì„¤ì •
        """
        # ì„¤ì • ë³µì‚¬
        updated_config = config.copy()
        
        # ë¦¬ìƒ˜í”Œë§ í™œì„±í™”
        resampling_method = trial_params.get('resampling_method', 'none')
        
        if resampling_method != 'none':
            # ë¦¬ìƒ˜í”Œë§ í™œì„±í™”
            if 'imbalanced_data' not in updated_config:
                updated_config['imbalanced_data'] = {}
            if 'resampling' not in updated_config['imbalanced_data']:
                updated_config['imbalanced_data']['resampling'] = {}
            
            updated_config['imbalanced_data']['resampling']['enabled'] = True
            updated_config['imbalanced_data']['resampling']['method'] = resampling_method
            
            # ë¦¬ìƒ˜í”Œë§ ë°©ë²•ë³„ íŒŒë¼ë¯¸í„° ì ìš©
            if resampling_method == 'smote':
                if 'smote' not in updated_config['imbalanced_data']['resampling']:
                    updated_config['imbalanced_data']['resampling']['smote'] = {}
                
                updated_config['imbalanced_data']['resampling']['smote']['k_neighbors'] = trial_params.get('smote_k_neighbors', 5)
                updated_config['imbalanced_data']['resampling']['smote']['sampling_strategy'] = trial_params.get('smote_sampling_strategy', 'auto')
            
            elif resampling_method == 'borderline_smote':
                if 'borderline_smote' not in updated_config['imbalanced_data']['resampling']:
                    updated_config['imbalanced_data']['resampling']['borderline_smote'] = {}
                
                updated_config['imbalanced_data']['resampling']['borderline_smote']['k_neighbors'] = trial_params.get('borderline_smote_k_neighbors', 5)
                updated_config['imbalanced_data']['resampling']['borderline_smote']['m_neighbors'] = trial_params.get('borderline_smote_m_neighbors', 10)
                updated_config['imbalanced_data']['resampling']['borderline_smote']['sampling_strategy'] = trial_params.get('borderline_smote_sampling_strategy', 'auto')
            
            elif resampling_method == 'adasyn':
                if 'adasyn' not in updated_config['imbalanced_data']['resampling']:
                    updated_config['imbalanced_data']['resampling']['adasyn'] = {}
                
                updated_config['imbalanced_data']['resampling']['adasyn']['k_neighbors'] = trial_params.get('adasyn_k_neighbors', 5)
                updated_config['imbalanced_data']['resampling']['adasyn']['n_neighbors'] = trial_params.get('adasyn_n_neighbors', 5)
                updated_config['imbalanced_data']['resampling']['adasyn']['sampling_strategy'] = trial_params.get('adasyn_sampling_strategy', 'auto')
            
            elif resampling_method == 'under_sampling':
                if 'under_sampling' not in updated_config['imbalanced_data']['resampling']:
                    updated_config['imbalanced_data']['resampling']['under_sampling'] = {}
                
                updated_config['imbalanced_data']['resampling']['under_sampling']['strategy'] = trial_params.get('under_sampling_strategy', 'random')
                updated_config['imbalanced_data']['resampling']['under_sampling']['sampling_strategy'] = trial_params.get('under_sampling_sampling_strategy', 'auto')
            
            elif resampling_method == 'hybrid':
                # HybridëŠ” SMOTE + Under Samplingì˜ ì¡°í•©
                if 'hybrid' not in updated_config['imbalanced_data']['resampling']:
                    updated_config['imbalanced_data']['resampling']['hybrid'] = {}
                
                updated_config['imbalanced_data']['resampling']['hybrid']['smote_k_neighbors'] = trial_params.get('hybrid_smote_k_neighbors', 5)
                updated_config['imbalanced_data']['resampling']['hybrid']['smote_sampling_strategy'] = trial_params.get('hybrid_smote_sampling_strategy', 'auto')
                updated_config['imbalanced_data']['resampling']['hybrid']['under_sampling_strategy'] = trial_params.get('hybrid_under_sampling_strategy', 'random')
                updated_config['imbalanced_data']['resampling']['hybrid']['under_sampling_sampling_strategy'] = trial_params.get('hybrid_under_sampling_sampling_strategy', 'auto')
        else:
            # ë¦¬ìƒ˜í”Œë§ ë¹„í™œì„±í™”
            if 'imbalanced_data' in updated_config and 'resampling' in updated_config['imbalanced_data']:
                updated_config['imbalanced_data']['resampling']['enabled'] = False
        
        return updated_config
    
    def objective(self, trial: optuna.Trial) -> float:
        """
        Optuna objective í•¨ìˆ˜ (ë¦¬ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° í¬í•¨).
        
        Args:
            trial: Optuna trial ê°ì²´
            
        Returns:
            í‰ê°€ ì ìˆ˜
        """
        try:
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì œì•ˆ (ë¦¬ìƒ˜í”Œë§ í¬í•¨)
            trial_params = self.suggest_hyperparameters(trial)
            
            # ë¦¬ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •ì— ì ìš©
            updated_config = self.apply_resampling_parameters_to_config(self.config, trial_params)
            
            # ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶”ì¶œ
            model_params = {}
            for key, value in trial_params.items():
                if not key.startswith(('resampling_', 'smote_', 'borderline_smote_', 'adasyn_', 'under_sampling_', 'hybrid_')):
                    model_params[key] = value
            
            # ëª¨ë¸ íƒ€ì… í™•ì¸
            model_type = self.config.get('model', {}).get('model_type', 'catboost')
            
            # MLflow íŒŒë¼ë¯¸í„° ë¡œê¹…
            for param, value in trial_params.items():
                mlflow.log_param(param, value)
            
            # êµì°¨ ê²€ì¦ ìˆ˜í–‰
            from src.training import cross_validate_model
            
            # êµì°¨ ê²€ì¦ ì‹¤í–‰
            cv_results = cross_validate_model(
                data=self.data,
                config=updated_config,
                model_params=model_params,
                model_type=model_type,
                n_splits=self.config.get('validation', {}).get('num_cv_folds', 5),
                random_state=self.config.get('validation', {}).get('random_state', 42)
            )
            
            # ì£¼ìš” ë©”íŠ¸ë¦­ ì¶”ì¶œ
            primary_metric = self.config.get('evaluation', {}).get('primary_metric', 'f1')
            if primary_metric in cv_results:
                score = cv_results[primary_metric]
            else:
                # ê¸°ë³¸ê°’ìœ¼ë¡œ f1 ì‚¬ìš©
                score = cv_results.get('f1', 0.0)
            
            # MLflow ë©”íŠ¸ë¦­ ë¡œê¹…
            for metric, value in cv_results.items():
                if isinstance(value, (int, float)) and not np.isnan(value) and not np.isinf(value):
                    mlflow.log_metric(metric, value)
            
            logger.info(f"Trial {trial.number} ì™„ë£Œ: {primary_metric} = {score:.4f}")
            return score
            
        except Exception as e:
            logger.error(f"Trial {trial.number} ì‹¤íŒ¨: {e}")
            return 0.0


def main():
    """ë©”ì¸ í•¨ìˆ˜ (ConfigManager ê¸°ë°˜ ê³„ì¸µì  config ë³‘í•© ì§€ì›)"""
    parser = argparse.ArgumentParser(
        description="ë¦¬ìƒ˜í”Œë§ ì‹¤í—˜ ì‹¤í–‰ (ConfigManager ì§€ì›)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ê³„ì¸µì  configë¡œ ë¦¬ìƒ˜í”Œë§ ì‹¤í—˜ ì‹¤í–‰
  python scripts/run_resampling_experiment.py --model-type xgboost --experiment-type resampling_experiment
  # legacy ë‹¨ì¼ íŒŒì¼ configë¡œ ë¦¬ìƒ˜í”Œë§ ì‹¤í—˜ ì‹¤í–‰
  python scripts/run_resampling_experiment.py --tuning_config configs/resampling_experiment.yaml --base_config configs/default_config.yaml
        """
    )
    parser.add_argument("--model-type", type=str, choices=['xgboost', 'lightgbm', 'random_forest', 'catboost'], default=None, help="ì‚¬ìš©í•  ëª¨ë¸ íƒ€ì… (ê³„ì¸µì  config ë³‘í•©)")
    parser.add_argument("--experiment-type", type=str, default=None, help="ì‹¤í—˜ íƒ€ì… (resampling_experiment ë“±)")
    parser.add_argument("--tuning_config", type=str, default=None, help="(legacy) íŠœë‹ ì„¤ì • íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--base_config", type=str, default=None, help="(legacy) ê¸°ë³¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--data_path", type=str, default=None, help="ë°ì´í„° íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--nrows", type=int, default=None, help="ì‚¬ìš©í•  ë°ì´í„° í–‰ ìˆ˜")
    parser.add_argument("--mlflow_ui", action="store_true", help="ì‹¤í—˜ ì™„ë£Œ í›„ MLflow UI ì‹¤í–‰")
    
    # ìë™í™”ëœ ì‹¤í—˜ì„ ìœ„í•œ ì¶”ê°€ ì¸ìë“¤
    parser.add_argument("--split-strategy", type=str, choices=['group_kfold', 'time_series_walk_forward', 'time_series_group_kfold'], default=None, help="ë°ì´í„° ë¶„í•  ì „ëµ")
    parser.add_argument("--cv-folds", type=int, default=None, help="êµì°¨ ê²€ì¦ í´ë“œ ìˆ˜")
    parser.add_argument("--n-trials", type=int, default=None, help="í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œë„ íšŸìˆ˜")
    parser.add_argument("--tuning-direction", type=str, choices=['maximize', 'minimize'], default=None, help="íŠœë‹ ë°©í–¥ (maximize/minimize)")
    parser.add_argument("--primary-metric", type=str, default=None, help="ì£¼ìš” í‰ê°€ ì§€í‘œ (f1, precision, recall, mcc, roc_auc, pr_auc ë“±)")
    parser.add_argument("--test-size", type=float, default=None, help="í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¹„ìœ¨ (0.0-1.0)")
    parser.add_argument("--random-state", type=int, default=None, help="ëœë¤ ì‹œë“œ")
    parser.add_argument("--n-jobs", type=int, default=None, help="ë³‘ë ¬ ì²˜ë¦¬ ì‘ì—… ìˆ˜")
    parser.add_argument("--timeout", type=int, default=None, help="íŠœë‹ íƒ€ì„ì•„ì›ƒ (ì´ˆ)")
    parser.add_argument("--early-stopping", action="store_true", help="Early stopping í™œì„±í™”")
    parser.add_argument("--early-stopping-rounds", type=int, default=None, help="Early stopping ë¼ìš´ë“œ ìˆ˜")
    parser.add_argument("--feature-selection", action="store_true", help="í”¼ì²˜ ì„ íƒ í™œì„±í™”")
    parser.add_argument("--feature-selection-method", type=str, choices=['mutual_info', 'chi2', 'f_classif', 'recursive'], default=None, help="í”¼ì²˜ ì„ íƒ ë°©ë²•")
    parser.add_argument("--feature-selection-k", type=int, default=None, help="ì„ íƒí•  í”¼ì²˜ ìˆ˜")
    parser.add_argument("--experiment-name", type=str, default=None, help="MLflow ì‹¤í—˜ ì´ë¦„ (ê¸°ë³¸ê°’: model_type_experiment_type)")
    parser.add_argument("--save-model", action="store_true", help="ìµœì  ëª¨ë¸ ì €ì¥")
    parser.add_argument("--save-predictions", action="store_true", help="ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥")
    parser.add_argument("--verbose", type=int, choices=[0, 1, 2], default=1, help="ë¡œê·¸ ë ˆë²¨ (0: ìµœì†Œ, 1: ê¸°ë³¸, 2: ìƒì„¸)")
    parser.add_argument("--resampling-method", type=str, choices=['none', 'smote', 'borderline_smote', 'adasyn', 'under_sampling', 'hybrid'], default=None, help="íŠ¹ì • ë¦¬ìƒ˜í”Œë§ ë°©ë²• ì§€ì • (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ íŠœë‹ì—ì„œ ìë™ ì„ íƒ)")
    args = parser.parse_args()
    
    try:
        if args.model_type:
            # ConfigManager ê¸°ë°˜ ê³„ì¸µì  config ë³‘í•©
            config_manager = ConfigManager()
            config = config_manager.create_experiment_config(args.model_type, args.experiment_type or 'resampling_experiment')
            
            # ëª…ë ¹í–‰ ì¸ìë¥¼ configì— ì ìš©
            config = config_manager.apply_command_line_args(config, args)
            
            # ë¦¬ìƒ˜í”Œë§ ë°©ë²•ì´ ì§€ì •ëœ ê²½ìš° ì„¤ì •ì— ì ìš©
            if args.resampling_method:
                if 'imbalanced_data' not in config:
                    config['imbalanced_data'] = {}
                if 'resampling' not in config['imbalanced_data']:
                    config['imbalanced_data']['resampling'] = {}
                config['imbalanced_data']['resampling']['method'] = args.resampling_method
                logger.info(f"ì§€ì •ëœ ë¦¬ìƒ˜í”Œë§ ë°©ë²•: {args.resampling_method}")
            
            # ì„¤ì • ìš”ì•½ ì¶œë ¥
            config_manager.print_config_summary(config)
            
            result = run_resampling_experiment_with_config(
                config,
                data_path=args.data_path,
                nrows=args.nrows
            )
            # ì‹¤í—˜ ê²°ê³¼ ì €ì¥ì€ run_resampling_experiment_with_config ë‚´ë¶€ì—ì„œ ì²˜ë¦¬ë¨
        elif args.tuning_config and args.base_config:
            # legacy: ë‹¨ì¼ íŒŒì¼ ê¸°ë°˜
            raise NotImplementedError("Legacy ëª¨ë“œëŠ” ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ConfigManagerë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")
        else:
            raise ValueError("model-type ë˜ëŠ” tuning_config+base_config ì¤‘ í•˜ë‚˜ëŠ” ë°˜ë“œì‹œ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        best_params, best_score, _, _ = result
        print(f"\nğŸ‰ ë¦¬ìƒ˜í”Œë§ ì‹¤í—˜ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ğŸ“Š ìµœê³  ì„±ëŠ¥: {best_score:.4f}")
        print(f"âš™ï¸  ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")
        
        if args.mlflow_ui:
            print(f"\nğŸŒ MLflow UIë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤...")
            print(f"   ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:5000 ìœ¼ë¡œ ì ‘ì†í•˜ì„¸ìš”.")
            print(f"   ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”.")
            import subprocess
            try:
                subprocess.run(["mlflow", "ui"], check=True)
            except KeyboardInterrupt:
                print(f"\nğŸ‘‹ MLflow UIë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                
    except Exception as e:
        logger.error(f"ë¦¬ìƒ˜í”Œë§ ì‹¤í—˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


if __name__ == "__main__":
    main() 